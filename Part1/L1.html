<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.0.37">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mette Langaas">

<title>MA8701 Advanced methods in statistical inference and learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="L1_files/libs/clipboard/clipboard.min.js"></script>
<script src="L1_files/libs/quarto-html/quarto.js"></script>
<script src="L1_files/libs/quarto-html/popper.min.js"></script>
<script src="L1_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="L1_files/libs/quarto-html/anchor.min.js"></script>
<link href="L1_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="L1_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="L1_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="L1_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="L1_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#course-topics" id="toc-course-topics" class="nav-link active" data-scroll-target="#course-topics">Course topics</a>
  <ul class="collapse">
  <li><a href="#part-1-core-concepts-2-weeks" id="toc-part-1-core-concepts-2-weeks" class="nav-link" data-scroll-target="#part-1-core-concepts-2-weeks">Part 1: Core concepts [2 weeks]</a></li>
  <li><a href="#part-2-shrinkage-and-regularization-4-weeks" id="toc-part-2-shrinkage-and-regularization-4-weeks" class="nav-link" data-scroll-target="#part-2-shrinkage-and-regularization-4-weeks">Part 2: Shrinkage and regularization [4 weeks]</a></li>
  <li><a href="#part-3-ensembles-4-weeks" id="toc-part-3-ensembles-4-weeks" class="nav-link" data-scroll-target="#part-3-ensembles-4-weeks">Part 3: Ensembles [4 weeks]</a></li>
  <li><a href="#part-4-xai-2-weeks" id="toc-part-4-xai-2-weeks" class="nav-link" data-scroll-target="#part-4-xai-2-weeks">Part 4: XAI [2 weeks]</a></li>
  <li><a href="#part-5-closing-2-weekw" id="toc-part-5-closing-2-weekw" class="nav-link" data-scroll-target="#part-5-closing-2-weekw">Part 5: Closing [2 weekw]</a></li>
  <li><a href="#required-previous-knowledge" id="toc-required-previous-knowledge" class="nav-link" data-scroll-target="#required-previous-knowledge">Required previous knowledge</a></li>
  <li><a href="#some-observations-about-the-course" id="toc-some-observations-about-the-course" class="nav-link" data-scroll-target="#some-observations-about-the-course">Some observations about the course</a></li>
  </ul></li>
  <li><a href="#learning" id="toc-learning" class="nav-link" data-scroll-target="#learning">Learning</a>
  <ul class="collapse">
  <li><a href="#learning-outcome" id="toc-learning-outcome" class="nav-link" data-scroll-target="#learning-outcome">Learning outcome</a></li>
  <li><a href="#learning-methods-and-activities" id="toc-learning-methods-and-activities" class="nav-link" data-scroll-target="#learning-methods-and-activities">Learning methods and activities</a></li>
  <li><a href="#course-elements" id="toc-course-elements" class="nav-link" data-scroll-target="#course-elements">Course elements</a></li>
  <li><a href="#class-activity" id="toc-class-activity" class="nav-link" data-scroll-target="#class-activity">Class activity</a></li>
  </ul></li>
  <li><a href="#core-concepts" id="toc-core-concepts" class="nav-link" data-scroll-target="#core-concepts">Core concepts</a>
  <ul class="collapse">
  <li><a href="#plan" id="toc-plan" class="nav-link" data-scroll-target="#plan">Plan</a></li>
  <li><a href="#notation" id="toc-notation" class="nav-link" data-scroll-target="#notation">Notation</a></li>
  <li><a href="#training-set" id="toc-training-set" class="nav-link" data-scroll-target="#training-set">Training set</a></li>
  <li><a href="#group-discussion" id="toc-group-discussion" class="nav-link" data-scroll-target="#group-discussion">Group discussion</a></li>
  </ul></li>
  <li><a href="#statistical-decision-theoretic-framework" id="toc-statistical-decision-theoretic-framework" class="nav-link" data-scroll-target="#statistical-decision-theoretic-framework">Statistical decision theoretic framework</a>
  <ul class="collapse">
  <li><a href="#squared-error-loss" id="toc-squared-error-loss" class="nav-link" data-scroll-target="#squared-error-loss">Squared error loss</a></li>
  <li><a href="#absolute-loss" id="toc-absolute-loss" class="nav-link" data-scroll-target="#absolute-loss">Absolute loss</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a>
  <ul class="collapse">
  <li><a href="#quadratic-loss-and-decision-theoretic-framework" id="toc-quadratic-loss-and-decision-theoretic-framework" class="nav-link" data-scroll-target="#quadratic-loss-and-decision-theoretic-framework">Quadratic loss and decision theoretic framework</a></li>
  <li><a href="#curse-of-dimensionality" id="toc-curse-of-dimensionality" class="nav-link" data-scroll-target="#curse-of-dimensionality">Curse of dimensionality</a></li>
  <li><a href="#expected-training-and-test-mse-for-linear-regression" id="toc-expected-training-and-test-mse-for-linear-regression" class="nav-link" data-scroll-target="#expected-training-and-test-mse-for-linear-regression">Expected training and test MSE for linear regression</a></li>
  </ul></li>
  <li><a href="#solutions-to-exercises" id="toc-solutions-to-exercises" class="nav-link" data-scroll-target="#solutions-to-exercises">Solutions to exercises</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">MA8701 Advanced methods in statistical inference and learning</h1>
<p class="subtitle lead">L1: Introduction and core concepts</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mette Langaas </p>
          </div>
  </div>
    
    
  </div>
  

</header>

<div class="cell">

</div>
<div class="cell">

</div>
<hr>
<section id="course-topics" class="level1">
<h1>Course topics</h1>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="ELSbookcover.jpeg" class="img-fluid figure-img" style="width:20.0%"></p>
</figure>
</div>
</div>
</div>
<p>The starting point is that we cover important parts of</p>
<p>The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition (Springer Series in Statistics, 2009) by Trevor Hastie, Robert Tibshirani, and Jerome Friedman.</p>
<p>but, since the book is from 2009 (2nd edition, 12th corrected printing from 2017) this means that for many topic we need (to be up to date) additional selected material in the form of book chapters and research articles.</p>
<p>Download book at <a href="https://hastie.su.domains/ElemStatLearn/download.html" class="uri">https://hastie.su.domains/ElemStatLearn/download.html</a> (this is 12th corrected printing) or signed in at NTNU vpn at <a href="https://link.springer.com/book/10.1007/978-0-387-84858-7">Springer</a> (but I am not sure if this is an earlier print without the version 12 corrections).</p>
<hr>
<section id="part-1-core-concepts-2-weeks" class="level2">
<h2 class="anchored" data-anchor-id="part-1-core-concepts-2-weeks">Part 1: Core concepts [2 weeks]</h2>
<p>Sort out assumed background knowledge, and learn something new</p>
<ul>
<li>Notation</li>
<li>Repetition of core concepts (regression and classification)</li>
<li>Statistical decision theoretic framework (partly new) ELS 2.4</li>
<li>Model selection and model assessment - including bias-variance trade-off (mostly new) ELS 7</li>
<li>Missing data ELS 9.6</li>
</ul>
<hr>
</section>
<section id="part-2-shrinkage-and-regularization-4-weeks" class="level2">
<h2 class="anchored" data-anchor-id="part-2-shrinkage-and-regularization-4-weeks">Part 2: Shrinkage and regularization [4 weeks]</h2>
<p>or “Regularized linear and generalized linear models”, with focus on the ridge and lasso regression (in detail).</p>
<ul>
<li>ELS 3.2.3,3.4, 3.8, 4.4.4.</li>
<li>Hastie, Tibshirani, Wainwright (HTW): “Statistical Learning with Sparsity: The Lasso and Generalizations”. Selected sections from chapters 1,2,3,4,6.</li>
<li>Selective inference (articles)</li>
</ul>
<hr>
</section>
<section id="part-3-ensembles-4-weeks" class="level2">
<h2 class="anchored" data-anchor-id="part-3-ensembles-4-weeks">Part 3: Ensembles [4 weeks]</h2>
<ul>
<li>trees, bagging and random forests</li>
<li>xgboost</li>
<li>general ensembles (including super learner)</li>
<li>hyper-parameter tuning</li>
</ul>
<p>Selected chapters in ELS (8.7, 8.8, 9.2, parts of 10, 15, 16) and several articles.</p>
<hr>
</section>
<section id="part-4-xai-2-weeks" class="level2">
<h2 class="anchored" data-anchor-id="part-4-xai-2-weeks">Part 4: XAI [2 weeks]</h2>
<p>Lectured by Kjersti Aas <a href="https://www.nr.no/~kjersti/" class="uri">https://www.nr.no/~kjersti/</a>.</p>
<p>Articles on</p>
<ul>
<li>LIME,</li>
<li>partial dependence plots,</li>
<li>Shapley values,</li>
<li>relative weights and</li>
<li>counterfactuals.</li>
</ul>
</section>
<section id="part-5-closing-2-weekw" class="level2">
<h2 class="anchored" data-anchor-id="part-5-closing-2-weekw">Part 5: Closing [2 weekw]</h2>
<hr>
</section>
<section id="required-previous-knowledge" class="level2">
<h2 class="anchored" data-anchor-id="required-previous-knowledge">Required previous knowledge</h2>
<ul>
<li>TMA4267 Linear statistical methods</li>
<li>TMA4268 Statistical learning</li>
<li>TMA4295 Statistical inference</li>
<li>TMA4300 Computer intensive statistical methods</li>
<li>TMA4315 Generalized linear models</li>
<li>Good understanding and experience with R, or with Python, for statistical data analysis.</li>
<li>Knowledge of markdown for writing reports and presentations (Rmarkdown/Quarto, Jupyther)</li>
<li>Skills in group work - possibly using git</li>
</ul>
<hr>
</section>
<section id="some-observations-about-the-course" class="level2">
<h2 class="anchored" data-anchor-id="some-observations-about-the-course">Some observations about the course</h2>
<ul>
<li>Mainly a frequentist course, but some of the concepts and methods have a Bayesian version that might give insight into why and how the methods work. Then Bayesian methods will be used.</li>
<li>Focus is on regression and classification, and unsupervised learning is not planned to be part of the course.</li>
<li>The required previous knowlege is listed because this is a phd-course designed for statistics studnents. The background make the students go past an overview level of understanding of the course parts (move from algorithmic to deep understanding).</li>
</ul>
<hr>
</section>
</section>
<section id="learning" class="level1">
<h1>Learning</h1>
<section id="learning-outcome" class="level2">
<h2 class="anchored" data-anchor-id="learning-outcome">Learning outcome</h2>
<p><strong>1. Knowledge</strong></p>
<ul>
<li>Understand and explain the central theoretical aspects in statistical inference and learning.</li>
<li>Understand and explain how to use methods from statistical inference and learning to perform a sound data analysis.</li>
<li>Be able to evaluate strengths and weaknesses for the methods and choose between different methods in a given data analysis situation.</li>
</ul>
<hr>
<p><strong>2. Skills</strong></p>
<p>Be able to analyse a dataset using methods from statistical inference and learning in practice (using R or Python), and give a good presentation and discussion of the choices done and the results found.</p>
<p><strong>3. Competence</strong></p>
<ul>
<li>The students will be able to participate in scientific discussions, read research presented in statistical journals.</li>
<li>They will be able to participate in applied projects, and analyse data using methods from statistical inference and learning.</li>
</ul>
<hr>
</section>
<section id="learning-methods-and-activities" class="level2">
<h2 class="anchored" data-anchor-id="learning-methods-and-activities">Learning methods and activities</h2>
<p>Herbert A. Simon (Cognitive science, Nobel Laureate): <em>Learning results from what the student does and thinks and only from what the student does and thinks. The teacher can advance learning only by influencing what the student does to learn.</em></p>
<hr>
</section>
<section id="course-elements" class="level2">
<h2 class="anchored" data-anchor-id="course-elements">Course elements</h2>
<p>Course wiki at <a href="https://wiki.math.ntnu.no/ma8701/2023v/start" class="uri">https://wiki.math.ntnu.no/ma8701/2023v/start</a></p>
<ul>
<li><p>Lectures</p></li>
<li><p>Office hours (poll?)</p></li>
<li><p>Problem sets to work on between lectures.</p></li>
<li><p>Study techniques (share)</p></li>
<li><p>Ethical considerations</p></li>
<li><p>Compulsory work</p></li>
<li><p>Final individual oral exam in May</p></li>
</ul>
<hr>
<p><strong>Questions?</strong></p>
<hr>
</section>
<section id="class-activity" class="level2">
<h2 class="anchored" data-anchor-id="class-activity">Class activity</h2>
<p>Aim: get to know each other - to improve on subsequent group work!</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> (at least one student not presented) </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>   lecturer give two alternatives, you choose one. </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>   lecturer choose a few students to present their view </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>   together with giving their name and study programme </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>   (and say <span class="cf">if</span> they are looking <span class="cf">for</span> group members)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<hr>
<ul>
<li>Dog person or cat person?</li>
<li>When performing logistic regression - do you then say you do statistical learning or machine learning?</li>
<li>I will show you the result of a descriptive analysis: summary or graphical display?</li>
<li>Learning something new: read a book or watch a video?</li>
<li>Analysing data: R or python?</li>
<li>Analysing data: report p-values and or confidence intervals</li>
<li>In class: taking notes or not?</li>
<li>Use camel case or snake case for programming?</li>
</ul>
<p>camel: writing compound words such that each word in the middle of the phrase begins with a capital letter, with no intervening spaces or punctuation. “camelCase” or “CamelCase”.</p>
<p>snake: writing compound words where the elements are separated with one underscore character (_) and no spaces, with each element’s initial letter usually lower cased within the compound and the first letter either upper- or lower case as in “foo_bar”</p>
<hr>
</section>
</section>
<section id="core-concepts" class="level1">
<h1>Core concepts</h1>
<p>(finally - we start on the fun stuff!)</p>
<section id="plan" class="level2">
<h2 class="anchored" data-anchor-id="plan">Plan</h2>
<section id="l1" class="level3">
<h3 class="anchored" data-anchor-id="l1">L1:</h3>
<ul>
<li>Notation</li>
<li>Remind about assumed background knowledge (already known),
<ul>
<li>Regression (ELS ch 3, except 3.2.3, 3.2.4, 3.4, 3.7, 3.8)</li>
</ul></li>
<li>Statistical decision theoretic framework (partly new: ELS 2.4)</li>
</ul>
</section>
<section id="l2-continue-with-the-same-framework-but-for-classification" class="level3">
<h3 class="anchored" data-anchor-id="l2-continue-with-the-same-framework-but-for-classification">L2: continue with the same framework but for classification</h3>
<p>Read to remind yourself of previous knowledge</p>
<ul>
<li>Classification (ELS ch 4.1-4.5, except 4.4.4)</li>
</ul>
</section>
<section id="l2-l4-then-cover-new-aspects-for" class="level3">
<h3 class="anchored" data-anchor-id="l2-l4-then-cover-new-aspects-for">L2-L4: Then, cover new aspects for</h3>
<ul>
<li>Model selection and assessment (ELS Ch 7.1-7.6, 7.10-7.12), including statistical learning and the bias-variance trade-off (ELS ch 2)</li>
<li>How to handle missing data in data analyses (ELS 9.6)</li>
</ul>
<hr>
</section>
</section>
<section id="notation" class="level2">
<h2 class="anchored" data-anchor-id="notation">Notation</h2>
<p>(mainly from ELS)</p>
<p>We will only consider supervised methods.</p>
<ul>
<li>Response <span class="math inline">\(Y\)</span> (or <span class="math inline">\(G\)</span>): dependent variable, outcome, usually univariate (but may be multivariate)
<ul>
<li>quantitative <span class="math inline">\(Y\)</span>: for regression</li>
<li>qualitative, categorical <span class="math inline">\(G\)</span>: for classification, some times dummy variable coding used (named one-hot coding in machine learning)</li>
</ul></li>
<li>Covariates <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span>: “independent variables”, predictors, features
<ul>
<li>continuous, discrete: used directly</li>
<li>categorical, discrete: often dummy variable coding used</li>
</ul></li>
</ul>
<p>We aim to construct a rule, function, learner: <span class="math inline">\(f(X)\)</span>, to predict <span class="math inline">\(Y\)</span> (or <span class="math inline">\(G\)</span>).</p>
<hr>
<p>Random variables and (column) vectors are written as uppercase letters <span class="math inline">\(X\)</span>, and <span class="math inline">\(Y\)</span>, while observed values are written with lowercase <span class="math inline">\((x,y)\)</span>. (Dimensions specified if needed.)</p>
<p>Matrices are presented with bold face: <span class="math inline">\({\bf X}\)</span>, often <span class="math inline">\(N \times (p+1)\)</span>.</p>
<p>ELS uses boldface also for <span class="math inline">\({\bf x}_j\)</span> being a vector of all <span class="math inline">\(N\)</span> observations of variable <span class="math inline">\(j\)</span>, but the vector of observed variables for observation <span class="math inline">\(i\)</span> is just <span class="math inline">\(x_i\)</span>.</p>
<hr>
<p>Both the response <em>and covariates</em> will be considered to be random, and drawn from some joint distribution <span class="math inline">\(P(X_1,X_2,\ldots, X_p,Y)=P(X,Y)\)</span> or <span class="math inline">\(P(X,G)\)</span>.</p>
<p>Conditional distribution: <span class="math inline">\(P(X,Y)=P(Y \mid X)P(X)\)</span> or <span class="math inline">\(P(Y\mid X=x)P(X=x)\)</span></p>
<p>and double expectation is often used</p>
<p><span class="math display">\[\text{E}[L(Y,f(X))]=\text{E}_{X,Y}[L(Y,f(X))]=\text{E}_{X}\text{E}_{Y \mid X}[L(Y,f(X))]\]</span></p>
<p>where <span class="math inline">\(L\)</span> is a loss function (to be defined next) and <span class="math inline">\(f(X)\)</span> some function to predict <span class="math inline">\(Y\)</span> (or <span class="math inline">\(G\)</span>).</p>
<hr>
</section>
<section id="training-set" class="level2">
<h2 class="anchored" data-anchor-id="training-set">Training set</h2>
<p>(ELS 2.1)</p>
<p>A set of size <span class="math inline">\(N\)</span> of independent pairs <span class="math inline">\((x_i,y_i)\)</span> is called the <em>training set</em> and often denoted <span class="math inline">\({\cal T}\)</span>.</p>
<p>The training data is used to estimate the unknown function <span class="math inline">\(f\)</span>.</p>
<section id="test-data" class="level3">
<h3 class="anchored" data-anchor-id="test-data">Test data</h3>
<p>Test data is in general thought of as future data, and plays an important role in both</p>
<ul>
<li>model selection (finding the best model among a candidate set) and also for</li>
<li>model assessment (assess the performance of the fitted model on future data).</li>
</ul>
<p>We will consider theoretical results for future test data, and also look at different ways to split or resample available data.</p>
<hr>
</section>
</section>
<section id="group-discussion" class="level2">
<h2 class="anchored" data-anchor-id="group-discussion">Group discussion</h2>
<p>Two core regression methods are multiple linear regression (MLR) and <span class="math inline">\(k\)</span>-nearest neighbour (kNN).</p>
<p>For the two methods</p>
<ul>
<li>Set up the formal definition for <span class="math inline">\(f\)</span>, and model assumptions made</li>
<li>What top results do you remember? Write them down.</li>
<li>What are challenges?</li>
<li>If time: What changes need to be done to each of the two methods for classification?</li>
</ul>
<hr>
<section id="regression-and-mlr" class="level3">
<h3 class="anchored" data-anchor-id="regression-and-mlr">Regression and MLR</h3>
<p><strong>Resources</strong></p>
<p>(mostly what we learned in TMA4267, or ELS ch 3, except 3.2.3, 3.2.4, 3.4, 3.7, 3.8)</p>
<ul>
<li>From TMA4268: <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/TMA4268overview.html">Overview</a> and in particular <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/3LinReg/3LinReg.html">Module 3: Linear regression</a></li>
<li>From TMA4315: <a href="https://www.math.ntnu.no/emner/TMA4315/2018h/TMA4315overviewH2018.html">Overview</a> and in particular <a href="https://www.math.ntnu.no/emner/TMA4315/2018h/2MLR.html">Module 2: MLR</a></li>
</ul>
<p>For <span class="math inline">\(k\)</span>NN see also Problem 1 of the <a href="https://www.math.ntnu.no/emner/TMA4268/Exam/V2018e.pdf">TMA4268 2018 exam</a> with <a href="https://www.math.ntnu.no/emner/TMA4268/Exam/e2018sol.pdf">solutions</a></p>
<hr>
</section>
</section>
</section>
<section id="statistical-decision-theoretic-framework" class="level1">
<h1>Statistical decision theoretic framework</h1>
<p>(ELS ch 2.4)</p>
<p>is a mathematical framework for developing models <span class="math inline">\(f\)</span> - and assessing optimality.</p>
<p>First, regression:</p>
<ul>
<li><span class="math inline">\(X \in \Re^p\)</span></li>
<li><span class="math inline">\(Y \in \Re\)</span></li>
<li><span class="math inline">\(P(X,Y)\)</span> joint distribution of covariates and respons</li>
</ul>
<p>Aim: find a function <span class="math inline">\(f(X)\)</span> for predicting <span class="math inline">\(Y\)</span> from some inputs <span class="math inline">\(X\)</span>.</p>
<p>Ingredients: Loss function <span class="math inline">\(L(Y,f(X))\)</span> - for <em>penalizing errors in the prediction</em>.</p>
<p>Criterion for choosing <span class="math inline">\(f\)</span>: Expected prediction error (EPE)</p>
<hr>
<p><span class="math display">\[ \text{EPE}(f)=\text{E}_{X,Y}[L(Y,f(X))]=\int_{x,y}L(y,f(x))p(x,y)dxdy\]</span></p>
<p>Choose <span class="math inline">\(f\)</span> to minimize the <span class="math inline">\(\text{EPE}(f)\)</span>.</p>
<p>What is the most popular loss function for regression?</p>
<hr>
<section id="squared-error-loss" class="level2">
<h2 class="anchored" data-anchor-id="squared-error-loss">Squared error loss</h2>
<p><span class="math display">\[ \text{EPE}(f)=\text{E}_{X,Y}[L(Y,f(X))]=\text{E}_{X}\text{E}_{Y \mid X}[(Y-f(X))^2\mid X]\]</span></p>
<p>We want to minimize EPE, and see that it is sufficient to minimize <span class="math inline">\(\text{E}_{Y\mid X}[(Y-f(X))^2\mid X]\)</span> for each <span class="math inline">\(X=x\)</span> (pointwise):</p>
<p><span class="math display">\[ f(x)=\text{argmin}_c \text{E}_{Y \mid X}[(Y-c)^2 \mid X=x]\]</span></p>
<p>This gives as result the conditional expectation - the best prediction at any point <span class="math inline">\(X=x\)</span>:</p>
<p><span class="math display">\[ f(x)=\text{E}[Y \mid X=x]\]</span></p>
<p>Proof: by differentiating and setting equal 0.</p>
<p>In practice: need to estimate <span class="math inline">\(f\)</span>.</p>
<hr>
<section id="linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="linear-regression">Linear regression</h3>
<p>Conditionally (known from before): if we assume that <span class="math inline">\((X,Y) \sim N_{p+1}(\mu,\Sigma)\)</span> then we have seen (TMA4267) that <span class="math inline">\(\text{E}(Y\mid X)\)</span> is linear in <span class="math inline">\(X\)</span> and <span class="math inline">\(\text{Cov}(Y \mid X)\)</span> is independent of <span class="math inline">\(X\)</span>.</p>
<p>Then we know we get <span class="math inline">\(\hat{\beta}=(X^TX)^{-1}X^T Y\)</span> (with matrices) using OLS or MLE.</p>
<hr>
<p>But, also if we assume an approximate linear model: <span class="math inline">\(f(x)\approx x^T \beta\)</span></p>
<p>Marginally: <span class="math inline">\(\text{argmin}_{\beta} \text{E}[(Y-X^T\beta)^2]\)</span> gives <span class="math inline">\(\beta=\text{E}[X X^T]^{-1}\text{E}[XY]\)</span> (now random vectors).</p>
<p>We may replace expectations with averages in training data to estimate <span class="math inline">\(\beta\)</span>.</p>
<p>This is not conditional on <span class="math inline">\(X\)</span>, but we have assumed a linear relationship.</p>
<hr>
</section>
</section>
<section id="absolute-loss" class="level2">
<h2 class="anchored" data-anchor-id="absolute-loss">Absolute loss</h2>
<p>Regression with absolute (L1) loss: <span class="math inline">\(L(Y,f(X))=\lvert Y-f(X) \rvert\)</span> gives <span class="math inline">\(\hat{f}(x)=\text{median}(Y\mid X=x)\)</span>.</p>
<p>Proof: for example pages 8-11 of <a href="https://getd.libs.uga.edu/pdfs/ma_james_c_201412_ms.pdf" class="uri">https://getd.libs.uga.edu/pdfs/ma_james_c_201412_ms.pdf</a></p>
<hr>
</section>
</section>
<section id="exercises" class="level1">
<h1>Exercises</h1>
<section id="quadratic-loss-and-decision-theoretic-framework" class="level2">
<h2 class="anchored" data-anchor-id="quadratic-loss-and-decision-theoretic-framework">Quadratic loss and decision theoretic framework</h2>
<p>Prove that <span class="math inline">\(f(x)=\text{E}[Y \mid X=x]\)</span> for the quadratic loss.</p>
</section>
<section id="curse-of-dimensionality" class="level2">
<h2 class="anchored" data-anchor-id="curse-of-dimensionality">Curse of dimensionality</h2>
<p>Read pages 22-23 and then answer Exercise 2.3 - which is to “Derive equation (2.24).”</p>
<p>Important take home messages:</p>
<ul>
<li>All sample points are close to an edge of the sample.</li>
<li>If data are uniformly distributed in an hypercube in <span class="math inline">\(p\)</span> dimensions, we need to cover <span class="math inline">\(r^{1/p}\)</span> of the the range of each input variable to capture a fraction <span class="math inline">\(r\)</span> of the observations.</li>
</ul>
</section>
<section id="expected-training-and-test-mse-for-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="expected-training-and-test-mse-for-linear-regression">Expected training and test MSE for linear regression</h2>
<p>Do exercise 2.9.</p>
<p>Important take home message: We have proven (for MLR) that the expected test MSE is always at least as large as the expected training MSE.</p>
<hr>
</section>
</section>
<section id="solutions-to-exercises" class="level1">
<h1>Solutions to exercises</h1>
<p>Please try yourself first, or take a small peek - and try some more - before fully reading the solutions. Report errors or improvements to <a href="mailto:Mette.Langaas@ntnu.no" class="email">Mette.Langaas@ntnu.no</a>. (The solutions given here are very similar to the UiO STK-IN4300 solutions, see link under References.)</p>
<p>All except solution to 7.9 available.</p>
<ul>
<li>Quadratic loss: Page 8 of <a href="https://getd.libs.uga.edu/pdfs/ma_james_c_201412_ms.pdf" class="uri">https://getd.libs.uga.edu/pdfs/ma_james_c_201412_ms.pdf</a></li>
<li><a href="https://github.com/mettelang/MA8701V2021/blob/main/ELSe23.pdf">2.3</a></li>
<li><a href="https://github.com/mettelang/MA8701V2021/blob/main/ELSe29.pdf">2.9</a></li>
</ul>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ul>
<li><p><a href="https://hastie.su.domains/ElemStatLearn/">ELS official errata:</a> and choose “Errata” in the left menu</p></li>
<li><p><a href="https://waxworksmath.com/Authors/G_M/Hastie/hastie.html">ELS solutions to exercises</a></p></li>
<li><p><a href="https://www.uio.no/studier/emner/matnat/math/STK-IN4300/h20/exercises.html">ELS solutions from UiO</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Camel_case">Camel_case</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Snake_case">Snake_case</a></p></li>
</ul>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>