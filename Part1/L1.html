<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mette Langaas">
<meta name="dcterms.date" content="2023-01-02">

<title>MA8701 Advanced methods in statistical inference and learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="L1_files/libs/clipboard/clipboard.min.js"></script>
<script src="L1_files/libs/quarto-html/quarto.js"></script>
<script src="L1_files/libs/quarto-html/popper.min.js"></script>
<script src="L1_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="L1_files/libs/quarto-html/anchor.min.js"></script>
<link href="L1_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="L1_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="L1_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="L1_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="L1_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul class="collapse">
  <li><a href="#course-topics" id="toc-course-topics" class="nav-link active" data-scroll-target="#course-topics">Course topics</a>
  <ul class="collapse">
  <li><a href="#part-1-core-concepts-3-weeks" id="toc-part-1-core-concepts-3-weeks" class="nav-link" data-scroll-target="#part-1-core-concepts-3-weeks">Part 1: Core concepts [3 weeks]</a></li>
  <li><a href="#part-2-shrinkage-and-regularization-3-weeks" id="toc-part-2-shrinkage-and-regularization-3-weeks" class="nav-link" data-scroll-target="#part-2-shrinkage-and-regularization-3-weeks">Part 2: Shrinkage and regularization [3 weeks]</a></li>
  <li><a href="#part-3-ensembles-4-weeks" id="toc-part-3-ensembles-4-weeks" class="nav-link" data-scroll-target="#part-3-ensembles-4-weeks">Part 3: Ensembles [4 weeks]</a></li>
  <li><a href="#part-4-xai-2-weeks" id="toc-part-4-xai-2-weeks" class="nav-link" data-scroll-target="#part-4-xai-2-weeks">Part 4: XAI [2 weeks]</a></li>
  <li><a href="#part-5-closing-2-weeks" id="toc-part-5-closing-2-weeks" class="nav-link" data-scroll-target="#part-5-closing-2-weeks">Part 5: Closing [2 weeks]</a></li>
  <li><a href="#required-previous-knowledge" id="toc-required-previous-knowledge" class="nav-link" data-scroll-target="#required-previous-knowledge">“Required” previous knowledge</a></li>
  <li><a href="#some-observations-about-the-course" id="toc-some-observations-about-the-course" class="nav-link" data-scroll-target="#some-observations-about-the-course">Some observations about the course</a></li>
  </ul></li>
  <li><a href="#learning" id="toc-learning" class="nav-link" data-scroll-target="#learning">Learning</a>
  <ul class="collapse">
  <li><a href="#learning-outcome" id="toc-learning-outcome" class="nav-link" data-scroll-target="#learning-outcome">Learning outcome</a></li>
  <li><a href="#learning-methods-and-activities" id="toc-learning-methods-and-activities" class="nav-link" data-scroll-target="#learning-methods-and-activities">Learning methods and activities</a></li>
  <li><a href="#course-elements" id="toc-course-elements" class="nav-link" data-scroll-target="#course-elements">Course elements</a></li>
  <li><a href="#class-activity" id="toc-class-activity" class="nav-link" data-scroll-target="#class-activity">Class activity</a></li>
  </ul></li>
  <li><a href="#tentative-plan-for-part-1" id="toc-tentative-plan-for-part-1" class="nav-link" data-scroll-target="#tentative-plan-for-part-1">Tentative plan for part 1</a>
  <ul class="collapse">
  <li><a href="#l1" id="toc-l1" class="nav-link" data-scroll-target="#l1">L1</a></li>
  <li><a href="#l2" id="toc-l2" class="nav-link" data-scroll-target="#l2">L2</a></li>
  <li><a href="#w2" id="toc-w2" class="nav-link" data-scroll-target="#w2">W2</a></li>
  <li><a href="#w3" id="toc-w3" class="nav-link" data-scroll-target="#w3">W3</a></li>
  </ul></li>
  <li><a href="#core-concepts" id="toc-core-concepts" class="nav-link" data-scroll-target="#core-concepts">Core concepts</a>
  <ul class="collapse">
  <li><a href="#notation" id="toc-notation" class="nav-link" data-scroll-target="#notation">Notation</a></li>
  <li><a href="#random-variables-and-random-vectors" id="toc-random-variables-and-random-vectors" class="nav-link" data-scroll-target="#random-variables-and-random-vectors">Random variables and random vectors</a></li>
  <li><a href="#training-set" id="toc-training-set" class="nav-link" data-scroll-target="#training-set">Training set</a></li>
  <li><a href="#validation-and-test-data" id="toc-validation-and-test-data" class="nav-link" data-scroll-target="#validation-and-test-data">Validation and test data</a></li>
  <li><a href="#group-discussion" id="toc-group-discussion" class="nav-link" data-scroll-target="#group-discussion">Group discussion</a></li>
  <li><a href="#regression-and-mlr" id="toc-regression-and-mlr" class="nav-link" data-scroll-target="#regression-and-mlr">Regression and MLR</a></li>
  </ul></li>
  <li><a href="#statistical-decision-theoretic-framework" id="toc-statistical-decision-theoretic-framework" class="nav-link" data-scroll-target="#statistical-decision-theoretic-framework">Statistical decision theoretic framework</a>
  <ul class="collapse">
  <li><a href="#squared-error-loss" id="toc-squared-error-loss" class="nav-link" data-scroll-target="#squared-error-loss">Squared error loss</a></li>
  <li><a href="#absolute-loss" id="toc-absolute-loss" class="nav-link" data-scroll-target="#absolute-loss">Absolute loss</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  <li><a href="#solutions-to-exercises" id="toc-solutions-to-exercises" class="nav-link" data-scroll-target="#solutions-to-exercises">Solutions to exercises</a></li>
  <li><a href="#reference-links" id="toc-reference-links" class="nav-link" data-scroll-target="#reference-links">Reference links</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">Bibliography</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">MA8701 Advanced methods in statistical inference and learning</h1>
<p class="subtitle lead">L1: Introduction and core concepts</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mette Langaas </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 2, 2023</p>
    </div>
  </div>
    
  </div>
  

</header>

<p>Course homepage: <a href="https://wiki.math.ntnu.no/ma8701/2023v/start" class="uri">https://wiki.math.ntnu.no/ma8701/2023v/start</a></p>
<div class="cell">

</div>
<div class="cell">

</div>
<hr>
<!--- ![ELS](ELSbookcover.jpeg){fig-align="left"}--->
<section id="course-topics" class="level1">
<h1>Course topics</h1>
<p>The starting point is that we cover important parts of</p>
<p>The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition (Springer Series in Statistics, 2009) by Trevor Hastie, Robert Tibshirani, and Jerome Friedman.</p>
<p>but, since the book is from 2009 (2nd edition, 12th corrected printing from 2017) this means that for many topic we need (to be up to date) additional selected material in the form of book chapters and research articles.</p>
<p>Download book at <a href="https://hastie.su.domains/ElemStatLearn/download.html" class="uri">https://hastie.su.domains/ElemStatLearn/download.html</a> (this is 12th corrected printing) or sign in at NTNU vpn at <a href="https://link.springer.com/book/10.1007/978-0-387-84858-7">Springer</a> (but I am not sure if this is an earlier print without the version 12 corrections).</p>
<hr>
<div class="cell">
<div class="cell-output-display">
<p><img src="ELSbookcover.jpeg" class="img-fluid"></p>
</div>
</div>
<hr>
<section id="part-1-core-concepts-3-weeks" class="level2">
<h2 class="anchored" data-anchor-id="part-1-core-concepts-3-weeks">Part 1: Core concepts [3 weeks]</h2>
<p>Sort out assumed background knowledge, and learn something new</p>
<ul>
<li>Notation</li>
<li>Repetition of core concepts (regression and classification)</li>
<li>Statistical decision theoretic framework (partly new) ELS 2.4</li>
<li>Model selection and model assessment - including bias-variance trade-off (mostly new) ELS 7</li>
<li>Handbook of Missing Data Methology (parts of Chapters 11-12, partly 13) and Flexible Imputation of Missing Data (parts of Chapters 2-4)</li>
</ul>
<hr>
</section>
<section id="part-2-shrinkage-and-regularization-3-weeks" class="level2">
<h2 class="anchored" data-anchor-id="part-2-shrinkage-and-regularization-3-weeks">Part 2: Shrinkage and regularization [3 weeks]</h2>
<p>or “Regularized linear and generalized linear models”, with focus on the ridge and lasso regression (in detail).</p>
<ul>
<li>ELS 3.2.3,3.4, 3.8, 4.4.4.</li>
<li>Hastie, Tibshirani, Wainwright (HTW): “Statistical Learning with Sparsity: The Lasso and Generalizations”. Selected sections from Chapters 1,2,3,4,6.</li>
<li>Selective inference (articles)</li>
</ul>
<hr>
</section>
<section id="part-3-ensembles-4-weeks" class="level2">
<h2 class="anchored" data-anchor-id="part-3-ensembles-4-weeks">Part 3: Ensembles [4 weeks]</h2>
<ul>
<li>trees, bagging and random forests</li>
<li>xgboost</li>
<li>general ensembles (including super learner)</li>
<li>hyper-parameter tuning</li>
</ul>
<p>Selected Chapters in ELS (8.7, 8.8, 9.2, parts of 10, 15, 16) and several articles.</p>
<hr>
</section>
<section id="part-4-xai-2-weeks" class="level2">
<h2 class="anchored" data-anchor-id="part-4-xai-2-weeks">Part 4: XAI [2 weeks]</h2>
<p>Lectured by Kjersti Aas <a href="https://www.nr.no/~kjersti/" class="uri">https://www.nr.no/~kjersti/</a>.</p>
<p>Interpretable Machine Learning: A Guide for Making Black Box Models Explainable, <span class="citation" data-cites="molnar2019">Molnar (<a href="#ref-molnar2019" role="doc-biblioref">2019</a>)</span>, with the following topics:</p>
<ul>
<li>LIME,</li>
<li>partial dependence plots,</li>
<li>Shapley values,</li>
<li>relative weights and</li>
<li>counterfactuals.</li>
</ul>
</section>
<section id="part-5-closing-2-weeks" class="level2">
<h2 class="anchored" data-anchor-id="part-5-closing-2-weeks">Part 5: Closing [2 weeks]</h2>
<hr>
</section>
<section id="required-previous-knowledge" class="level2">
<h2 class="anchored" data-anchor-id="required-previous-knowledge">“Required” previous knowledge</h2>
<ul>
<li>TMA4267 Linear statistical methods</li>
<li>TMA4268 Statistical learning</li>
<li>TMA4295 Statistical inference</li>
<li>TMA4300 Computer intensive statistical methods</li>
<li>TMA4315 Generalized linear models</li>
<li>Good understanding and experience with R, or with Python, for statistical data analysis.</li>
<li>Knowledge of markdown for writing reports and presentations (Rmarkdown/Quarto, Jupyther).</li>
<li>Skills in group work - possibly using git or other collaborative tools.</li>
</ul>
<hr>
</section>
<section id="some-observations-about-the-course" class="level2">
<h2 class="anchored" data-anchor-id="some-observations-about-the-course">Some observations about the course</h2>
<ul>
<li>Mainly a frequentist course, but some of the concepts and methods have a Bayesian version that might give insight into why and how the methods work. Then Bayesian methods will be used.</li>
<li>Focus is on regression and classification, and unsupervised learning is not planned to be part of the course.</li>
<li>The required previous knowledge is listed because this is a phd-course designed for statistics studenets. The background make the students go past an overview level of understanding of the course parts (move from algorithmic to deep understanding).</li>
</ul>
<hr>
</section>
</section>
<section id="learning" class="level1">
<h1>Learning</h1>
<section id="learning-outcome" class="level2">
<h2 class="anchored" data-anchor-id="learning-outcome">Learning outcome</h2>
<p><strong>1. Knowledge</strong></p>
<ul>
<li>Understand and explain the central theoretical aspects in statistical inference and learning.</li>
<li>Understand and explain how to use methods from statistical inference and learning to perform a sound data analysis.</li>
<li>Be able to evaluate strengths and weaknesses for the methods and choose between different methods in a given data analysis situation.</li>
</ul>
<hr>
<p><strong>2. Skills</strong></p>
<p>Be able to analyse a dataset using methods from statistical inference and learning in practice (using R or Python), and give a good presentation and discussion of the choices done and the results found.</p>
<p><strong>3. Competence</strong></p>
<ul>
<li>The students will be able to participate in scientific discussions, read research presented in statistical journals.</li>
<li>They will be able to participate in applied projects, and analyse data using methods from statistical inference and learning.</li>
</ul>
<hr>
</section>
<section id="learning-methods-and-activities" class="level2">
<h2 class="anchored" data-anchor-id="learning-methods-and-activities">Learning methods and activities</h2>
<p>Herbert A. Simon (Cognitive science, Nobel Laureate): <em>Learning results from what the student does and thinks and only from what the student does and thinks. The teacher can advance learning only by influencing what the student does to learn.</em></p>
<hr>
</section>
<section id="course-elements" class="level2">
<h2 class="anchored" data-anchor-id="course-elements">Course elements</h2>
<p>Course wiki at <a href="https://wiki.math.ntnu.no/ma8701/2023v/start" class="uri">https://wiki.math.ntnu.no/ma8701/2023v/start</a></p>
<ul>
<li><p>Lectures</p></li>
<li><p>Office hours (poll?)</p></li>
<li><p>Problem sets to work on between lectures.</p></li>
<li><p>Study techniques (share)</p></li>
<li><p>Ethical considerations</p></li>
<li><p>Compulsory work</p></li>
<li><p>Final individual oral exam in May</p></li>
</ul>
<p>The learning material is also available at <a href="https://github.com/mettelang/MA8701V2023" class="uri">https://github.com/mettelang/MA8701V2023</a>.</p>
<hr>
<p><strong>Questions?</strong></p>
<hr>
</section>
<section id="class-activity" class="level2">
<h2 class="anchored" data-anchor-id="class-activity">Class activity</h2>
<p>Aim: get to know each other - to improve on subsequent group work!</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> (at least one student not presented) </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>   lecturer give two alternatives, you choose one. </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>   lecturer choose a few students to present their view </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>   together with giving their name and study programme </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>   (and say <span class="cf">if</span> they are looking <span class="cf">for</span> group members)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<hr>
<ul>
<li>Dog person or cat person?</li>
<li>When performing logistic regression - do you then say you do statistical learning or machine learning?</li>
<li>I will show you the result of a descriptive analysis: table summary or graphical display?</li>
<li>Learning something new: read a book or watch a video?</li>
<li>Analysing data: R or python?</li>
<li>Analysing data: report p-values and or confidence intervals</li>
<li>In class: taking notes or not?</li>
<li>Use camel case or snake case for programming?</li>
</ul>
<p>camel: writing compound words such that each word in the middle of the phrase begins with a capital letter, with no intervening spaces or punctuation. “camelCase” or “CamelCase”.</p>
<p>snake: writing compound words where the elements are separated with one underscore character (_) and no spaces, with each element’s initial letter usually lower cased within the compound and the first letter either upper- or lower case as in “foo_bar”</p>
<hr>
</section>
</section>
<section id="tentative-plan-for-part-1" class="level1">
<h1>Tentative plan for part 1</h1>
<p>(progress may be faster or slower than indicated)</p>
<section id="l1" class="level2">
<h2 class="anchored" data-anchor-id="l1">L1</h2>
<p>Notation, regression and statistical theoretic framework</p>
<ul>
<li>Notation (ELS Ch 2.2)</li>
<li>Regression - should not be new (ELS Ch 3, except 3.2.3, 3.2.4, 3.4, 3.7, 3.8)</li>
<li>Statistical decision theoretic framework for regression (ELS 2.4)</li>
</ul>
</section>
<section id="l2" class="level2">
<h2 class="anchored" data-anchor-id="l2">L2</h2>
<p>Continue with the same framework but for classification, if time also bias-variance trade-off</p>
<ul>
<li>Classification - should not be new (ELS Ch 4.1-4.5, except 4.4.4)</li>
<li>Statistical decision theoretic framework for classification (ELS 2.4)</li>
<li>and the bias-variance trade-off</li>
</ul>
<hr>
</section>
<section id="w2" class="level2">
<h2 class="anchored" data-anchor-id="w2">W2</h2>
<p>L3-4: Then, cover new aspects for</p>
<ul>
<li>Model selection and assessment (ELS Ch 7.1-7.6, 7.10-7.12)</li>
</ul>
</section>
<section id="w3" class="level2">
<h2 class="anchored" data-anchor-id="w3">W3</h2>
<p>L5-6</p>
<ul>
<li>How to handle missing data in data analyses</li>
</ul>
<hr>
</section>
</section>
<section id="core-concepts" class="level1">
<h1>Core concepts</h1>
<section id="notation" class="level2">
<h2 class="anchored" data-anchor-id="notation">Notation</h2>
<p>(mainly from ELS 2.2)</p>
<p>We will only consider supervised methods.</p>
<ul>
<li>Response <span class="math inline">\(Y\)</span> (or <span class="math inline">\(G\)</span>): dependent variable, outcome, usually univariate (but may be multivariate)
<ul>
<li>quantitative <span class="math inline">\(Y\)</span>: for regression</li>
<li>qualitative, categorical <span class="math inline">\(G\)</span>: for classification, some times dummy variable coding used (named one-hot coding in machine learning)</li>
</ul></li>
<li>Covariates <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span>: “independent variables”, predictors, features
<ul>
<li>continuous, discrete: used directly</li>
<li>categorical, discrete: often dummy variable coding used</li>
</ul></li>
</ul>
<p>We aim to construct a rule, function, learner: <span class="math inline">\(f(X)\)</span>, to predict <span class="math inline">\(Y\)</span> (or <span class="math inline">\(G\)</span>).</p>
<hr>
<p>Random variables and (column) vectors are written as uppercase letters <span class="math inline">\(X\)</span>, and <span class="math inline">\(Y\)</span>, while observed values are written with lowercase <span class="math inline">\((x,y)\)</span>. (Dimensions specified if needed.)</p>
<p>Matrices are presented with uppercase boldface: <span class="math inline">\(\boldsymbol{X}\)</span>, often <span class="math inline">\(N \times (p+1)\)</span>.</p>
<p>ELS uses boldface also for <span class="math inline">\(\boldsymbol{x}_j\)</span> being a vector of all <span class="math inline">\(N\)</span> observations of variable <span class="math inline">\(j\)</span>, but in general vectors are not boldface and the vector of observed variables for observation <span class="math inline">\(i\)</span> is just <span class="math inline">\(x_i\)</span>.</p>
<hr>
</section>
<section id="random-variables-and-random-vectors" class="level2">
<h2 class="anchored" data-anchor-id="random-variables-and-random-vectors">Random variables and random vectors</h2>
<p>Both the response <em>and covariates</em> will be considered to be random, and drawn from some joint distribution <span class="math inline">\(P(X_1,X_2,\ldots, X_p,Y)=P(X,Y)\)</span> or <span class="math inline">\(P(X,G)\)</span>.</p>
<p>Joint to conditional and marginal distribution: <span class="math inline">\(P(X,Y)=P(Y \mid X)P(X)\)</span> or <span class="math inline">\(P(Y\mid X=x)P(X=x)\)</span> or</p>
<p><span class="math display">\[P(Y=y ,X=x)=P(Y=y\mid X=x)P(X=x)\]</span></p>
<!-- To look ahead: we may then calculate the expected value in a sequential (iterated) manner: -->
<!-- $$\text{E}[L(Y,f(X))]=\text{E}_{X,Y}[L(Y,f(X))]=\text{E}_{X}\text{E}_{Y \mid X}[L(Y,f(X))]$$ -->
<!-- where $L$ is a loss function (to be defined next) and $f(X)$ some function to predict $Y$ (or $G$). (No, $f(X)$ is not the density pdf.) -->
<p>Maybe brush up on this?</p>
<p><strong>Resources</strong></p>
<ul>
<li>From TMA4268: <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/2StatLearn/2StatLearn.html#random_vector">Module 2 - Random vectors</a></li>
<li>From TMA4267: <a href="https://www.math.ntnu.no/emner/TMA4267/2017v/TMA4267V2017Part1.pdf">Part 1: Multivariate random variables and the multivariate normal distribution</a></li>
</ul>
<hr>
</section>
<section id="training-set" class="level2">
<h2 class="anchored" data-anchor-id="training-set">Training set</h2>
<p>(ELS 2.1)</p>
<p>A set of size <span class="math inline">\(N\)</span> of independent pairs ov observations <span class="math inline">\((x_i,y_i)\)</span> is called the <em>training set</em> and often denoted <span class="math inline">\(\mathcal{T}\)</span>. Here <span class="math inline">\(x_i\)</span> may be a vector.</p>
<p>The training data is used to estimate the unknown function <span class="math inline">\(f\)</span>.</p>
</section>
<section id="validation-and-test-data" class="level2">
<h2 class="anchored" data-anchor-id="validation-and-test-data">Validation and test data</h2>
<p>Validation data is used for <em>model selection</em> (finding the best model among a candidate set).</p>
<p>Test data is used for <em>model assessment</em> (assess the performance of the fitted model on future data).</p>
<p>We will consider theoretical results, and also look at different ways to split or resample available data.</p>
<p>More in ELS Chapter 7.</p>
<hr>
</section>
<section id="group-discussion" class="level2">
<h2 class="anchored" data-anchor-id="group-discussion">Group discussion</h2>
<p>Two core regression methods are multiple linear regression (MLR) and <span class="math inline">\(k\)</span>-nearest neighbour (kNN).</p>
<p>For the two methods</p>
<ul>
<li>Set up the formal definition for <span class="math inline">\(f\)</span>, and model assumptions made</li>
<li>What top results do you remember? Write them down.</li>
<li>What are challenges?</li>
</ul>
<hr>
</section>
<section id="regression-and-mlr" class="level2">
<h2 class="anchored" data-anchor-id="regression-and-mlr">Regression and MLR</h2>
<p>See also the exercises!</p>
<p><strong>Resources</strong></p>
<p>(mostly what we learned in TMA4267, or ELS Ch 3, except 3.2.3, 3.2.4, 3.4, 3.7, 3.8)</p>
<ul>
<li>From TMA4268: <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/TMA4268overview.html">Overview</a> and in particular <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/3LinReg/3LinReg.html">Module 3: Linear regression</a></li>
<li>From TMA4315: <a href="https://www.math.ntnu.no/emner/TMA4315/2018h/TMA4315overviewH2018.html">Overview</a> and in particular <a href="https://www.math.ntnu.no/emner/TMA4315/2018h/2MLR.html">Module 2: MLR</a></li>
</ul>
<p>For <span class="math inline">\(k\)</span>NN see also Problem 1 of the <a href="https://www.math.ntnu.no/emner/TMA4268/Exam/V2018e.pdf">TMA4268 2018 exam</a> with <a href="https://www.math.ntnu.no/emner/TMA4268/Exam/e2018sol.pdf">solutions</a></p>
<hr>
</section>
</section>
<section id="statistical-decision-theoretic-framework" class="level1">
<h1>Statistical decision theoretic framework</h1>
<p>(ELS Ch 2.4, regression part)</p>
<p>is a mathematical framework for developing models <span class="math inline">\(f\)</span> - and assessing optimality.</p>
<p>First, regression:</p>
<ul>
<li><span class="math inline">\(X \in \Re^p\)</span></li>
<li><span class="math inline">\(Y \in \Re\)</span></li>
<li><span class="math inline">\(P(X,Y)\)</span> joint distribution of covariates and respons</li>
</ul>
<p>Aim: find a function <span class="math inline">\(f(X)\)</span> for predicting <span class="math inline">\(Y\)</span> from some inputs <span class="math inline">\(X\)</span>.</p>
<p>Ingredients: Loss function <span class="math inline">\(L(Y,f(X))\)</span> - for <em>penalizing errors in the prediction</em>.</p>
<p>Criterion for choosing <span class="math inline">\(f\)</span>: Expected prediction error (EPE)</p>
<hr>
<p><span class="math display">\[ \text{EPE}(f)=\text{E}_{X,Y}[L(Y,f(X))]=\int_{x,y}L(y,f(x))p(x,y)dxdy\]</span> Choose <span class="math inline">\(f\)</span> to minimize the <span class="math inline">\(\text{EPE}(f)\)</span>.</p>
<p>Q: Why do we not involve the distribution of the random variable <span class="math inline">\(f(X)\)</span>, but instead the distribution of <span class="math inline">\(X\)</span>?</p>
<p>Law of the unconscious statistican (from our introductory course in statistics): <a href="https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician" class="uri">https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician</a> and <a href="https://tma4245.math.ntnu.no/forventing-og-varians/forventingsverdi/forventningsverdi-funksjoner-av-stokastiske-variabler-egx/">Thematics pages TMA4240/45</a>.</p>
<p>What is the most popular loss function for regression?</p>
<hr>
<section id="squared-error-loss" class="level2">
<h2 class="anchored" data-anchor-id="squared-error-loss">Squared error loss</h2>
<p><span class="math display">\[ \text{EPE}(f)=\text{E}_{X,Y}[L(Y,f(X))]=\text{E}_{X}\text{E}_{Y \mid X}[(Y-f(X))^2\mid X]\]</span></p>
<p>We want to minimize EPE, and see that it is sufficient to minimize <span class="math inline">\(\text{E}_{Y\mid X}[(Y-f(X))^2\mid X]\)</span> for each <span class="math inline">\(X=x\)</span> (pointwise):</p>
<p><span class="math display">\[ f(x)=\text{argmin}_c \text{E}_{Y \mid X}[(Y-c)^2 \mid X=x]\]</span> This gives as result the conditional expectation - the best prediction at any point <span class="math inline">\(X=x\)</span>:</p>
<p><span class="math display">\[ f(x)=\text{E}[Y \mid X=x]\]</span> Proof: by differentiating and setting equal 0.</p>
<p>But, do we know this conditional distribution? In practice: need to estimate <span class="math inline">\(f\)</span>.</p>
<hr>
<section id="what-is-the-joint-distribution-is-multivariate-normal" class="level3">
<h3 class="anchored" data-anchor-id="what-is-the-joint-distribution-is-multivariate-normal">What is the joint distribution is multivariate normal?</h3>
<p>Conditionally (known from before): if we assume that <span class="math inline">\((X,Y) \sim N_{p+1}(\mu,\Sigma)\)</span> then we have seen (TMA4267) that <span class="math inline">\(\text{E}(Y\mid X)\)</span> is linear in <span class="math inline">\(X\)</span> and <span class="math inline">\(\text{Cov}(Y \mid X)\)</span> is independent of <span class="math inline">\(X\)</span>.</p>
<p><a href="https://www.math.ntnu.no/emner/TMA4268/2019v/2StatLearn/2StatLearn.html#six_useful_properties_of_the_mvn">Properties of the mvN</a></p>
<!-- Then we know we get  -->
<!-- $\hat{\beta}=(X^TX)^{-1}X^T Y$ (with matrices) using OLS or MLE. -->
<hr>
</section>
<section id="approximate-linear-model" class="level3">
<h3 class="anchored" data-anchor-id="approximate-linear-model">Approximate linear model</h3>
<p>But, also if we assume an approximate linear model: <span class="math inline">\(f(x)\approx x^T \beta\)</span></p>
<p>Marginally: <span class="math inline">\(\text{argmin}_{\beta} \text{E}[(Y-X^T\beta)^2]\)</span> gives <span class="math inline">\(\beta=\text{E}[X X^T]^{-1}\text{E}[XY]\)</span> (now random vectors).</p>
<p>We may replace expectations with averages in training data to estimate <span class="math inline">\(\beta\)</span>.</p>
<p>This is not conditional on <span class="math inline">\(X\)</span>, but we have assumed a linear relationship.</p>
<hr>
</section>
<section id="knn-and-conditional-expectation" class="level3">
<h3 class="anchored" data-anchor-id="knn-and-conditional-expectation">kNN and conditional expectation</h3>
<p>Local conditional mean for observations in <span class="math inline">\(\cal T\)</span> close to <span class="math inline">\(\bf{x}_0\)</span>: <span class="math display">\[\hat{f}({\bf x}_0)=\frac{1}{k}\sum_{i \in \cal N_k({\bf x}_0)}Y_i\]</span></p>
<hr>
</section>
</section>
<section id="absolute-loss" class="level2">
<h2 class="anchored" data-anchor-id="absolute-loss">Absolute loss</h2>
<p>Regression with absolute (L1) loss: <span class="math inline">\(L(Y,f(X))=\lvert Y-f(X) \rvert\)</span> gives <span class="math inline">\(\hat{f}(x)=\text{median}(Y\mid X=x)\)</span>.</p>
<p>Proof: for example pages 8-11 of <a href="https://getd.libs.uga.edu/pdfs/ma_james_c_201412_ms.pdf" class="uri">https://getd.libs.uga.edu/pdfs/ma_james_c_201412_ms.pdf</a></p>
<hr>
</section>
</section>
<section id="exercises" class="level1">
<h1>Exercises</h1>
<section id="law-of-total-expectation-and-total-variance" class="level3">
<h3 class="anchored" data-anchor-id="law-of-total-expectation-and-total-variance">1: Law of total expectation and total variance</h3>
<p>This is to get a feeling of the joint and conditional distributions, so that we understand expected value notation with joint, conditional and marginal distributions.</p>
<p>Give a derivation of the law of total expectation:</p>
<p><span class="math display">\[\text{E}[X]=\text{E}[\text{E}(X\mid Y)]\]</span></p>
<p>and the law of total variance: <span class="math display">\[\text{Var}[X]=\text{E}\text{Var}[X \mid Y]+\text{Var}\text{E}[X\mid Y]\]</span> (There is also a law of total covariance.)</p>
</section>
<section id="quadratic-loss-and-decision-theoretic-framework" class="level3">
<h3 class="anchored" data-anchor-id="quadratic-loss-and-decision-theoretic-framework">2: Quadratic loss and decision theoretic framework</h3>
<p>Show that <span class="math inline">\(f(x)=\text{E}[Y \mid X=x]\)</span> for the quadratic loss.</p>
</section>
<section id="curse-of-dimensionality" class="level3">
<h3 class="anchored" data-anchor-id="curse-of-dimensionality">3: Curse of dimensionality</h3>
<p>Read pages 22-23 and then answer Exercise 2.3 - which is to “Derive equation (2.24).”</p>
<p>Important take home messages:</p>
<ul>
<li>All sample points are close to an edge of the sample.</li>
<li>If data are uniformly distributed in an hypercube in <span class="math inline">\(p\)</span> dimensions, we need to cover <span class="math inline">\(r^{1/p}\)</span> of the the range of each input variable to capture a fraction <span class="math inline">\(r\)</span> of the observations.</li>
</ul>
</section>
<section id="key-results-from-mlr" class="level3">
<h3 class="anchored" data-anchor-id="key-results-from-mlr">4: Key results from MLR</h3>
<p>(These results are known from TMA4267 and TMA4315, but useful to brush up on?)</p>
<p>Assume we have a data set with independent observation pairs <span class="math inline">\((y_i,{\bf x}_i)\)</span> for <span class="math inline">\(i=1,\ldots,N\)</span>.</p>
<p><span class="math display">\[{\bf Y=X \boldsymbol{\beta}}+{\bf\varepsilon}\]</span> where <span class="math inline">\({\bf Y}\)</span> is a <span class="math inline">\(N \times 1\)</span> random column vector, <span class="math inline">\({\bf X}\)</span> a <span class="math inline">\(N \times (p+1)\)</span> design matrix with row for observations (<span class="math inline">\({\bf x}^T_i\)</span>) and columns for covariates, and <span class="math inline">\({\bf{\varepsilon}}\)</span> <span class="math inline">\(N \times 1\)</span> random column vector</p>
<p>The assumptions for the classical linear model is:</p>
<ol type="1">
<li><p><span class="math inline">\(\text{E}(\bf{\varepsilon})=\bf{0}\)</span>.</p></li>
<li><p><span class="math inline">\(\text{Cov}(\varepsilon)=\text{E}(\varepsilon \varepsilon^T)=\sigma^2\bf{I}\)</span>.</p></li>
<li><p>The design matrix has full rank, <span class="math inline">\(\text{rank}({\bf X})=(p+1)\)</span>.</p></li>
</ol>
<p>The classical <em>normal</em> linear regression model is obtained if additionally</p>
<ol start="4" type="1">
<li><span class="math inline">\(\varepsilon\sim N_n(\bf{0},\sigma^2\bf{I})\)</span> holds.</li>
</ol>
<p>For random covariates these assumptions are to be understood conditionally on <span class="math inline">\(\bf{X}\)</span>.</p>
<section id="a-regression-parameter-estimator" class="level4">
<h4 class="anchored" data-anchor-id="a-regression-parameter-estimator">a) Regression parameter estimator</h4>
<p>Derive the least squares (or the maximum likelihood estimator) for <span class="math inline">\(\hat{\beta}\)</span>.</p>
</section>
<section id="b-properties-of-regression-parameter-estimator" class="level4">
<h4 class="anchored" data-anchor-id="b-properties-of-regression-parameter-estimator">b) Properties of regression parameter estimator</h4>
<p>Derive the distribution of <span class="math inline">\(\hat{\beta}\)</span> (when assumption 4 is true).</p>
</section>
<section id="c-estimator-for-variance" class="level4">
<h4 class="anchored" data-anchor-id="c-estimator-for-variance">c) Estimator for variance</h4>
<p>Explain how we may find the restricted maximum likelihood estimator for <span class="math inline">\(\sigma^2\)</span>. Which distribution is used for inference for <span class="math inline">\(\sigma^2\)</span>?</p>
</section>
<section id="d-hypothesis-test" class="level4">
<h4 class="anchored" data-anchor-id="d-hypothesis-test">d) Hypothesis test</h4>
<p>How would you test the hypothesis <span class="math inline">\(H_0: \beta_j=0\)</span> against <span class="math inline">\(H_1: \beta_j\neq 0\)</span>?</p>
</section>
<section id="e-explanability" class="level4">
<h4 class="anchored" data-anchor-id="e-explanability">e) Explanability</h4>
<p>Explanability is now very important - but then we usually talk about black box models. How would you explain the impact of each covariate in a multiple linear regression model? Can you give the proportion of the varibility that each variable is responsible for explaining?</p>
<hr>
</section>
</section>
</section>
<section id="solutions-to-exercises" class="level1">
<h1>Solutions to exercises</h1>
<p>Please try yourself first, or take a small peek - and try some more - before fully reading the solutions. Report errors or improvements to <a href="mailto:Mette.Langaas@ntnu.no" class="email">Mette.Langaas@ntnu.no</a>. (The solutions given here are very similar to the UiO STK-IN4300 solutions, see link under References.)</p>
<section id="law-of-total-e-and-var" class="level3">
<h3 class="anchored" data-anchor-id="law-of-total-e-and-var">1: Law of total E and Var</h3>
<p><a href="https://github.com/mettelang/MA8701V2023/blob/main/Part1/TotalEandTotalVar.pdf">Try first yourself</a></p>
</section>
<section id="quadratic-loss" class="level3">
<h3 class="anchored" data-anchor-id="quadratic-loss">2: Quadratic loss</h3>
<p>Page 8 of <a href="https://getd.libs.uga.edu/pdfs/ma_james_c_201412_ms.pdf" class="uri">https://getd.libs.uga.edu/pdfs/ma_james_c_201412_ms.pdf</a></p>
</section>
<section id="curse-of-dimensionality-1" class="level3">
<h3 class="anchored" data-anchor-id="curse-of-dimensionality-1">3: Curse of dimensionality</h3>
<p><a href="https://github.com/mettelang/MA8701V2023/blob/main/Part1/ELSe23.pdf">2.3</a></p>
</section>
<section id="key-results-from-mlr-1" class="level3">
<h3 class="anchored" data-anchor-id="key-results-from-mlr-1">4: Key results from MLR</h3>
<section id="a-regression-parameter-estimator-1" class="level4">
<h4 class="anchored" data-anchor-id="a-regression-parameter-estimator-1">a) Regression parameter estimator</h4>
<p>Both methods are written out in <a href="https://www.math.ntnu.no/emner/TMA4268/2018v/notes/LeastSquaresMLR.pdf">these class notes from TMA4267/8</a>. More on likelihood-version here: <a href="https://www.math.ntnu.no/emner/TMA4315/2018h/2MLR.html#likelihood_theory_(from_b4)">TMA4315 GLM Module 2</a>.</p>
</section>
<section id="b-properties-of-regression-estimator" class="level4">
<h4 class="anchored" data-anchor-id="b-properties-of-regression-estimator">b) Properties of regression estimator</h4>
<p><a href="https://www.math.ntnu.no/emner/TMA4268/2019v/notes/M3L2notes.pdf">Page 3 of classnotes from TMA4268</a></p>
</section>
<section id="c-estimator-for-variance-1" class="level4">
<h4 class="anchored" data-anchor-id="c-estimator-for-variance-1">c) Estimator for variance</h4>
<p><a href="https://www.math.ntnu.no/emner/TMA4315/2018h/2MLR.html#restricted_maximum_likelihood_estimator_for_(%7Bbf_sigma%7D%5E2)">TMA4315 GLM Module 2</a></p>
</section>
<section id="d-hypothesis-test-1" class="level4">
<h4 class="anchored" data-anchor-id="d-hypothesis-test-1">d) Hypothesis test</h4>
<p><a href="https://www.math.ntnu.no/emner/TMA4268/2019v/notes/M3L2notes.pdf">Page 4 of classnotes from TMA4268</a></p>
</section>
<section id="e" class="level4">
<h4 class="anchored" data-anchor-id="e">e)</h4>
<p>Standardized regression coefficients, estimated regression coefficients divided by their standard errors, is the most used solution. But, also popular is the decomposition of the <span class="math inline">\(R^2\)</span> - easy for orthogonal design matrix, but not easy for correlated covariates. The LMG-method of <span class="citation" data-cites="gromping2007">Grömping (<a href="#ref-gromping2007" role="doc-biblioref">2007</a>)</span> (decomposing <span class="math inline">\(R^2\)</span>) gives a solution that is also valid with correlated covariates, that is identical to the Shapley value of Part 4 (more later).</p>
</section>
</section>
</section>
<section id="reference-links" class="level1">
<h1>Reference links</h1>
<ul>
<li><p><a href="https://hastie.su.domains/ElemStatLearn/">ELS official errata:</a> and choose “Errata” in the left menu</p></li>
<li><p><a href="https://waxworksmath.com/Authors/G_M/Hastie/hastie.html">ELS notes and solutions to exercises</a></p></li>
<li><p><a href="https://www.uio.no/studier/emner/matnat/math/STK-IN4300/h20/exercises.html">ELS solutions from UiO</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Camel_case">Camel_case</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Snake_case">Snake_case</a></p></li>
</ul>
</section>
<section id="bibliography" class="level1">


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">Bibliography</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-gromping2007" class="csl-entry" role="doc-biblioentry">
Grömping, U. 2007. <span>“Estimators of Relative Importance in Linear Regression Based on Variance Decomposition.”</span> <em>The American Statistician</em> 61: 139–47.
</div>
<div id="ref-ESL" class="csl-entry" role="doc-biblioentry">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Vol. 2. Springer series in statistics New York. <a href="https://hastie.su.domains/ElemStatLearn">hastie.su.domains/ElemStatLearn</a>.
</div>
<div id="ref-molnar2019" class="csl-entry" role="doc-biblioentry">
Molnar, Christoph. 2019. <em>Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</em>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>