@Manual{R,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2013},
    url = {http://www.R-project.org/},
  }

@Article{mice,
    title = {{mice}: Multivariate Imputation by Chained Equations in
      R},
    author = {Stef {van Buuren} and Karin Groothuis-Oudshoorn},
    journal = {Journal of Statistical Software},
    year = {2011},
    volume = {45},
    number = {3},
    pages = {1-67},
    doi = {10.18637/jss.v045.i03},
  }
 @article{lydersen_multippel,
	title = {Multippel imputering av manglende data},
	issn = {0029-2001},
	doi = {10.4045/tidsskr.21.0772},
	journal = {Tidsskrift for Den norske legeforening},
	author = {Lydersen, Stian},
	year = {2022},
}

  @Book{mass,
    title = {Modern Applied Statistics with S},
    author = {W. N. Venables and B. D. Ripley},
    publisher = {Springer},
    edition = {Fourth},
    address = {New York},
    year = {2002},
    note = {ISBN 0-387-95457-0},
    url = {https://www.stats.ox.ac.uk/pub/MASS4/},
  }

 @Article{precrec,
    title = {Precrec: fast and accurate precision-recall and ROC curve
      calculations in R},
    author = {Takaya Saito and Marc Rehmsmeier},
    journal = {Bioinformatics},
    year = {2017},
    volume = {33 (1)},
    pages = {145-147},
    doi = {10.1093/bioinformatics/btw570},
  }

@book{VanBuuren2018,
 address = {Boca Raton, FL.},
 author = {{van Buuren}, Stef},
 publisher = {CRC Press},
 title = {Flexible Imputation of Missing Data. Second Edition.},
 year = {2018},
}


PHDThesis{brandphd,
 title                = {Development, Implementation and Evaluation of Multiple Imputation Strategies for the Statistical Analysis of Incomplete Data Sets},
 author               = {Brand, Jaap},
 school               = {E},
 year                 = 1999,
 month                = apr,
 url                  = {http://hdl.handle.net/1765/19790}
}

@misc{WNvW,
author = "Wessel N. van Wieringen",
year = "2021",
title = "Lecture notes on ridge regression",
url = "https://arxiv.org/pdf/1509.09169v7.pdf"
}

@misc{Dua:2017 ,
author = "Dheeru, Dua and Karra Taniskidou, Efi",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@book{ISL,
  title={An introduction to statistical learning},
  author={James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  volume={112},
  year={2013},
  publisher={Springer}
}

@book{ESL,
  title={The elements of statistical learning: Data Mining, Inference, and Prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  volume={2},
  year={2009},
  publisher={Springer series in statistics New York},
  url={hastie.su.domains/ElemStatLearn}
}

@article{Bagging,
author={Leo Breiman},
title={Bagging Predictors},
journal={Machine Learning},
volume={24},
pages={123-140},
year={1996}}

@article{RandomForest,
author={Leo Breiman},
title={Random Forest},
journal={Machine Learning},
volume={45},
pages={5-32},
year={2001}}

@article{SVMinR,
author={A. Karatzoglou and D. Meyer and K. Hornik},
title={Support Vector Machines in R},
journal={Journal of Statistical Software},
volume={15},
number={9},
year={2006}}

@book{Ripley,
title={Pattern Recognicion and Neural Networks},
author={Brian D. Ripley},
year={1996},
publisher={Cambridge University Press}}

@book{casi,
title={Computer age statistical inference - algorithms, evidence, and data science},
author={Bradley Efron and Trevor Hastie},
year={2016},
url={https://hastie.su.domains/CASI/},
publisher={Cambridge University Press}}

@book{MASS,
title={Modern Applied Statistics with S},
author={W. N. Venables and B. D. Ripley},
year={2002},
publisher={Springer}
}

@book{goodfellow,
title={Deep learning},
author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
year={2016},
publisher={MIT Press}}

@book{kerasR,
title={Deep learning with R},
author={François Chollet and J. J. Allaire},
year={2018},
publisher={Manning Press}}

@book{molnar2019,
  title      = {Interpretable Machine Learning},
  author     = {Christoph Molnar},
  note       = {\url{https://christophm.github.io/interpretable-ml-book/}},
  year       = {2019},
  subtitle   = {A Guide for Making Black Box Models Explainable}
}

@article{aleplot2020,
author = {Apley, Daniel W. and Zhu, Jingyu},
title = {Visualizing the effects of predictor variables in black box supervised learning models},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {82},
number = {4},
pages = {1059-1086},
keywords = {Functional analysis of variance, Marginal plots, Partial dependence plots, Supervised learning, Visualization},
doi = {https://doi.org/10.1111/rssb.12377},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12377},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12377},
abstract = {Summary In many supervised learning applications, understanding and visualizing the effects of the predictor variables on the predicted response is of paramount importance. A shortcoming of black box supervised learning models (e.g. complex trees, neural networks, boosted trees, random forests, nearest neighbours, local kernel-weighted methods and support vector regression) in this regard is their lack of interpretability or transparency. Partial dependence plots, which are the most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to partial dependence plots, we present a new visualization approach that we term accumulated local effects plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, accumulated local effects plots are far less computationally expensive than partial dependence plots. We also provide an R package ALEPlot as supplementary material to implement our proposed method.},
year = {2020}
}

@InProceedings{Dandletal2020,
author="Dandl, Susanne
and Molnar, Christoph
and Binder, Martin
and Bischl, Bernd",
editor="B{\"a}ck, Thomas
and Preuss, Mike
and Deutz, Andr{\'e}
and Wang, Hao
and Doerr, Carola
and Emmerich, Michael
and Trautmann, Heike",
title="Multi-Objective Counterfactual Explanations",
booktitle="Parallel Problem Solving from Nature -- PPSN XVI",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="448--469",
url={https://link.springer.com/content/pdf/10.1007%2F978-3-030-58112-1_31.pdf},
abstract="Counterfactual explanations are one of the most popular methods to make predictions of black box machine learning models interpretable by providing explanations in the form of `what-if scenarios'. Most current approaches optimize a collapsed, weighted sum of multiple objectives, which are naturally difficult to balance a-priori. We propose the Multi-Objective Counterfactuals (MOC) method, which translates the counterfactual search into a multi-objective optimization problem. Our approach not only returns a diverse set of counterfactuals with different trade-offs between the proposed objectives, but also maintains diversity in feature space. This enables a more detailed post-hoc analysis to facilitate better understanding and also more options for actionable user responses to change the predicted outcome. Our approach is also model-agnostic and works for numerical and categorical input features. We show the usefulness of MOC in concrete cases and compare our approach with state-of-the-art methods for counterfactual explanations.",
isbn="978-3-030-58112-1"
}

@article{gromping2007,
author = {Grömping, U.},
title = {Estimators of Relative Importance in Linear Regression Based on Variance Decomposition},
journal = {The American Statistician},
volume = {61},
year=2007,
pages = {139-147},
eprint = {https://prof.beuth-hochschule.de/fileadmin/prof/groemp/downloads/amstat07mayp139.pdf}}


@Article{ice2015,
    title = {Peeking Inside the Black Box: Visualizing Statistical
      Learning With Plots of Individual Conditional Expectation},
    author = {Alex Goldstein and Adam Kapelner and Justin Bleich and
      Emil Pitkin},
    journal = {Journal of Computational and Graphical Statistics},
    volume = {24},
    number = {1},
    pages = {44--65},
    doi = {10.1080/10618600.2014.907095},
    year = {2015},
  }

@inproceedings{LundbergLee2017,
 author = {Lundberg, Scott M and Lee, Su-In},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Unified Approach to Interpreting Model Predictions},
 url = {https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},
 volume = {30},
 year = {2017}
}

@Article{Wachter2018,
title={Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR},
author={Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
journal={Harvard Journal of Law \& Technology},
volume=31,
number=2,
year=2018,
url={http://dx.doi.org/10.2139/ssrn.3063289}} 

@inproceedings{lime2016,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1144},
numpages = {10},
keywords = {explaining machine learning, interpretable machine learning, interpretability, black box classifier},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@Article{Aas2021,
title={Explaining individual predictions when features are dependent: More accurate approximations to Shapley values},
author={Kjersti Aas and Martin Jullum and Anders Løland},
journal={Artificial Intelligence},
year=2021,
url={https://doi.org/10.1016/j.artint.2021.103502}} 

@Article{Johnson2000,
title= {Heuristic Method for Estimating the Relative Weight of Predictor Variables in Multiple Regression},
author={Johnson, J W.},
journal={Multivariate behavioral research},
year={2000},
url={doi:10.1207/S15327906MBR3501_1}}

@article{taylor2015,
	title = {Statistical learning and selective inference},
	volume = {112},
	issn = {1091-6490},
	doi = {10.1073/pnas.1507583112},
	abstract = {We describe the problem of "selective inference." This addresses the following challenge: Having mined a set of data to find potential associations, how do we properly assess the strength of these associations? The fact that we have "cherry-picked"--searched for the strongest associations--means that we must set a higher bar for declaring significant the associations that we see. This challenge becomes more important in the era of big data and complex statistical modeling. The cherry tree (dataset) can be very large and the tools for cherry picking (statistical learning methods) are now very sophisticated. We describe some recent new developments in selective inference and illustrate their use in forward stepwise regression, the lasso, and principal components analysis.},
	language = {eng},
	number = {25},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Taylor, Jonathan and Tibshirani, Robert J.},
	month = jun,
	year = {2015},
	pmid = {26100887},
	pmcid = {PMC4485109},
	keywords = {Datasets as Topic, Learning, Models, Statistical, P values, inference, lasso},
	pages = {7629--7634},
}

@book{missinghandbook2014,
	address = {Boca Raton},
	series = {Chapman \& {Hall}/{CRC} {Handbooks} of {Modern} {Statistical} {Methods}},
	title = {Handbook of {Missing} {Data} {Methodology}},
	isbn = {978-1-4398-5461-7},
	url = {https://search.ebscohost.com/login.aspx?direct=true&db=nlebk&AN=1499432&site=ehost-live&scope=site},
	abstract = {Missing data affect nearly every discipline by complicating the statistical analysis of collected data. But since the 1990s, there have been important developments in the statistical methodology for handling missing data. Written by renowned statisticians in this area, Handbook of Missing Data Methodology presents many methodological advances and t},
	language = {English},
	urldate = {2022-11-05},
	publisher = {Chapman and Hall/CRC},
	author = {Molenberghs, Geert and Fitzmaurice, Garrett and Kenward, Michael G. and Tsiatis, Anastasios and Verbeke, Geert},
	year = {2014},
	keywords = {MATHEMATICS / Probability \& Statistics / General, Missing observations (Statistics), Statistics--Methodology},
}

@book{dunnsmythGLM,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {Generalized {Linear} {Models} {With} {Examples} in {R}},
	isbn = {978-1-4419-0117-0 978-1-4419-0118-7},
	url = {http://link.springer.com/10.1007/978-1-4419-0118-7},
	language = {en},
	urldate = {2022-11-22},
	publisher = {Springer New York},
	author = {Dunn, Peter K. and Smyth, Gordon K.},
	year = {2018},
	doi = {10.1007/978-1-4419-0118-7},
}


@article{supermice2021,
    author = {Laqueur, Hannah S and Shev, Aaron B and Kagawa, Rose M C},
    title = "{SuperMICE: An Ensemble Machine Learning Approach to Multiple Imputation by Chained Equations}",
    journal = {American Journal of Epidemiology},
    volume = {191},
    number = {3},
    pages = {516-525},
    year = {2021},
    month = {11},
    abstract = "{Researchers often face the problem of how to address missing data. Multiple imputation is a popular approach, with multiple imputation by chained equations (MICE) being among the most common and flexible methods for execution. MICE iteratively fits a predictive model for each variable with missing values, conditional on other variables in the data. In theory, any imputation model can be used to predict the missing values. However, if the predictive models are incorrectly specified, they may produce biased estimates of the imputed data, yielding inconsistent parameter estimates and invalid inference. Given the set of modeling choices that must be made in conducting multiple imputation, in this paper we propose a data-adaptive approach to model selection. Specifically, we adapt MICE to incorporate an ensemble algorithm, Super Learner, to predict the conditional mean for each missing value, and we also incorporate a local kernel-based estimate of variance. We present a set of simulations indicating that this approach produces final parameter estimates with lower bias and better coverage than other commonly used imputation methods. These results suggest that using a flexible machine learning imputation approach can be useful in settings where data are missing at random, especially when the relationships among the variables are complex.}",
    issn = {0002-9262},
    doi = {10.1093/aje/kwab271},
    url = {https://doi.org/10.1093/aje/kwab271},
    eprint = {https://academic.oup.com/aje/article-pdf/191/3/516/42587241/kwab271.pdf},
}

@article{BootMIWahl2016,
	abstract = {Missing values are a frequent issue in human studies. In many situations, multiple imputation (MI) is an appropriate missing data handling strategy, whereby missing values are imputed multiple times, the analysis is performed in every imputed data set, and the obtained estimates are pooled. If the aim is to estimate (added) predictive performance measures, such as (change in) the area under the receiver-operating characteristic curve (AUC), internal validation strategies become desirable in order to correct for optimism. It is not fully understood how internal validation should be combined with multiple imputation.},
	author = {Wahl, Simone and Boulesteix, Anne-Laure and Zierer, Astrid and Thorand, Barbara and van de Wiel, Mark A.},
	date = {2016/10/26},
	date-added = {2023-01-24 12:56:28 +0100},
	date-modified = {2023-01-24 12:56:28 +0100},
	doi = {10.1186/s12874-016-0239-7},
	id = {Wahl2016},
	isbn = {1471-2288},
	journal = {BMC Medical Research Methodology},
	number = {1},
	pages = {144},
	title = {Assessment of predictive performance in incomplete data by combining internal validation and multiple imputation},
	url = {https://doi.org/10.1186/s12874-016-0239-7},
	volume = {16},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1186/s12874-016-0239-7}}

@article{moonsetal2006,
author= {Moons KG, Donders RA, Stijnen T, Harrell FE Jr},
title={Using the outcome for imputation of missing predictor values was preferred.},
journal={J Clin Epidemiol.},
year={2006},
volume={59(10)},
pages={1092-101},
doi={10.1016/j.jclinepi.2006.01.009}}


@book{Harville1997,
author={David A. Harville},
title={Matrix algebra from a statistician's perspective},
year={1997},
publisher={Springer}}

@article{Lasso1996,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 urldate = {2023-01-31},
 volume = {58},
 year = {1996}
}

@article{Zou2006,
author = {Hui Zou},
title = {The Adaptive Lasso and Its Oracle Properties},
journal = {Journal of the American Statistical Association},
volume = {101},
number = {476},
pages = {1418-1429},
year  = {2006},
publisher = {Taylor & Francis},
doi = {10.1198/016214506000000735},
URL = {https://doi.org/10.1198/016214506000000735},
eprint = {https://doi.org/10.1198/01621450600000073}
}

@book{HTW,
title={Statistical Learning with Sparsity: The Lasso and Generalizations},
author={Trevor Hastie and Robert J. Tibshirani and Martin Wainwright},
year={2015},
url={https://hastie.su.domains/StatLearnSparsity/},
publisher={CRC Press}}

@article{FrankFriedman1993,
author = {lldiko Frank and Jerome H. Friedman },
title = {A Statistical View of Some Chemometrics Regression Tools},
journal = {Technometrics},
volume = {35},
number = {2},
pages = {109-135},
year  = {1993},
publisher = {Taylor & Francis},
doi = {10.1080/00401706.1993.10485033},
URL = {  https://www.tandfonline.com/doi/abs/10.1080/00401706.1993.10485033},
eprint = { https://www.tandfonline.com/doi/pdf/10.1080/00401706.1993.10485033}
}

@article{JSSglmnet,
 title={Regularization Paths for Generalized Linear Models via Coordinate Descent},
 volume={33},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v033i01},
 doi={10.18637/jss.v033.i01},
 abstract={We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multi- nomial regression problems while the penalties include ℓ&amp;lt;sub&amp;gt;1&amp;lt;/sub&amp;gt; (the lasso), ℓ&amp;lt;sub&amp;gt;2&amp;lt;/sub&amp;gt; (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
 number={1},
 journal={Journal of Statistical Software},
 author={Friedman, Jerome H. and Hastie, Trevor and Tibshirani, Rob},
 year={2010},
 pages={1–22}}

@article{ZouHastie2005,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/3647580},
 abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p ≫ n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
 author = {Hui Zou and Trevor Hastie},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {2},
 pages = {301--320},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regularization and Variable Selection via the Elastic Net},
 urldate = {2023-02-04},
 volume = {67},
 year = {2005}
}

@article{ZhaoLong2017,
author = {Zhao, Yize and Long, Qi},
title = {Variable selection in the presence of missing data: imputation-based methods},
journal = {WIREs Computational Statistics},
volume = {9},
number = {5},
pages = {e1402},
keywords = {MAR, MCAR, MNAR, imputation, bootstrap, variable selection, missing data, resampling},
doi = {https://doi.org/10.1002/wics.1402},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1402},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1402},
abstract = {Variable selection plays an essential role in regression analysis as it identifies important variables that are associated with outcomes and is known to improve predictive accuracy of resulting models. Variable selection methods have been widely investigated for fully observed data. However, in the presence of missing data, methods for variable selection need to be carefully designed to account for missing data mechanisms and statistical techniques used for handling missing data. Since imputation is arguably the most popular method for handling missing data due to its ease of use, statistical methods for variable selection that are combined with imputation are of particular interest. These methods, valid and used under the assumptions of missing at random and missing completely at random, largely fall into three general strategies. The first strategy applies existing variable selection methods to each imputed dataset and then combines variable selection results across all imputed datasets. The second strategy applies existing variable selection methods to stacked imputed datasets. The third variable selection strategy combines resampling techniques such as bootstrap with imputation. Despite recent advances, this area remains under-developed and offers fertile ground for further research. WIREs Comput Stat 2017, 9:e1402. doi: 10.1002/wics.1402 This article is categorized under: Statistical and Graphical Methods of Data Analysis > Bootstrap and Resampling},
year = {2017}
}

@article{Lachenbruch2011,
author = {Peter A Lachenbruch},
title ={Variable selection when missing values are present: a case study},
journal = {Statistical Methods in Medical Research},
volume = {20},
number = {4},
pages = {429-444},
year = {2011},
doi = {10.1177/0962280209358003},
    note ={PMID: 20442196},

URL = { 
        https://doi.org/10.1177/0962280209358003
    
},
eprint = { 
        https://doi.org/10.1177/0962280209358003
    
}
,
    abstract = { We consider variable selection when missing values are present in the predictor variables. We compare using complete cases with multiple imputation using backward selection (backwards stepping) and least angle regression. These are studied using a data set from a rheumatological disease (myositis). We find that the coefficients are slightly different and the estimated standard errors are smaller in the complete cases (not a surprise). This seems to be due to the fact that because the estimated residual variance is small the complete cases are more homogeneous than the full data cases. }
}

 @Article{hdi2015,
    title = {High-Dimensional Inference: Confidence Intervals, p-values
      and {R}-Software {hdi}},
    author = {Ruben Dezeure and Peter B\"uhlmann and Lukas Meier and
      Nicolai Meinshausen},
    journal = {Statistical Science},
    year = {2015},
    volume = {30},
    number = {4},
    pages = {533--558}}
  
@article{TaylorTibshirani2015,
author = {Jonathan Taylor  and Robert J. Tibshirani },
title = {Statistical learning and selective inference},
journal = {Proceedings of the National Academy of Sciences},
volume = {112},
number = {25},
pages = {7629-7634},
year = {2015},
doi = {10.1073/pnas.1507583112},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1507583112},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1507583112}}

@article{Berketal2013,
author = {Richard Berk and Lawrence Brown and Andreas Buja and Kai Zhang and Linda Zhao},
title = {{Valid post-selection inference}},
volume = {41},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {802 -- 837},
keywords = {Family-wise error, high-dimensional inference, Linear regression, Model selection, multiple comparison, sphere packing},
year = {2013},
doi = {10.1214/12-AOS1077},
URL = {https://doi.org/10.1214/12-AOS1077}
}

@article{BootLasso2011,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/41416396},
 abstract = {In this article, we consider bootstrapping the Lasso estimator of the regression parameter in a multiple linear regression model. It is known that the standard bootstrap method fails to be consistent. Here, we propose a modified bootstrap method, and show that it provides valid approximation to the distribution of the Lasso estimator, for all possible values of the unknown regression parameter vector, including the case where some of the components are zero. Further, we establish consistency of the modified bootstrap method for estimating the asymptotic bias and variance of the Lasso estimator. We also show that the residual bootstrap can be used to consistently estimate the distribution and variance of the adaptive Lasso estimator. Using the former result, we formulate a novel data-based method for choosing the optimal penalizing parameter for the Lasso using the modified bootstrap. A numerical study is performed to investigate the finite sample performance of the modified bootstrap. The methodology proposed in the article is illustrated with a real data example.},
 author = {A. Chatterjee and S. N. Lahiri},
 journal = {Journal of the American Statistical Association},
 number = {494},
 pages = {608--625},
 publisher = {Taylor & Francis, Ltd.},
 title = {Bootstrapping Lasso Estimators},
 urldate = {2023-02-19},
 volume = {106},
 year = {2011}
}


@article{Breiman1996Stacking,
	abstract = {Stacking regressions is a method for forming linear combinations of different predictors to give improved prediction accuracy. The idea is to use cross-validation data and least squares under non-negativity constraints to determine the coefficients in the combination. Its effectiveness is demonstrated in stacking regression trees of different sizes and in a simulation stacking linear subset and ridge regressions. Reasons why this method works are explored. The idea of stacking originated with Wolpert (1992).},
	author = {Breiman, Leo},
	date = {1996/07/01},
	date-added = {2023-03-04 17:35:19 +0100},
	date-modified = {2023-03-04 17:35:19 +0100},
	doi = {10.1007/BF00117832},
	id = {Breiman1996},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {1},
	pages = {49--64},
	title = {Stacked regressions},
	url = {https://doi.org/10.1007/BF00117832},
	volume = {24},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1007/BF00117832}}

@article{vanderLaanPolleyHubbard2007,
url = {https://doi.org/10.2202/1544-6115.1309},
title = {Super Learner},
title = {},
author = {Mark J. van der Laan and Eric C Polley and Alan E. Hubbard},
volume = {6},
number = {1},
journal = {Statistical Applications in Genetics and Molecular Biology},
doi = {doi:10.2202/1544-6115.1309},
year = {2007},
lastchecked = {2023-03-04}
}

@Inbook{Polley2011,
author="Polley, Eric C.
and Rose, Sherri
and van der Laan, Mark J.",
title="Super Learning",
bookTitle="Targeted Learning: Causal Inference for Observational and Experimental Data",
year="2011",
publisher="Springer New York",
address="New York, NY",
pages="43--66",
abstract="This is the first chapter in our text focused on estimation within the road map for targeted learning. Now that we've defined the research question, including our data, the model, and the target parameter, we are ready to begin. For the estimation of a target parameter of the probability distribution of the data, such as target parameters that can be interpreted as causal effects, we implement TMLE. The first step in this estimation procedure is an initial estimate of the data-generating distribution P0, or the relevant part Q0 of P0 that is needed to evaluate the target parameter. This is the step presented in Chap. 3, and TMLE will be presented in Chaps. 4 and 5.",
isbn="978-1-4419-9782-1",
doi="10.1007/978-1-4419-9782-1_3",
url="https://doi.org/10.1007/978-1-4419-9782-1_3"
}

@thesis{LeDell2015,
  title={Scalable Ensemble Learning and Computationally Efficient Variance Estimation},
  author={Erin LeDell},
  year={2015}
}

	


