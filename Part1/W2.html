<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mette Langaas">
<meta name="dcterms.date" content="2023-01-13">

<title>MA8701 Advanced methods in statistical inference and learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="W2_files/libs/clipboard/clipboard.min.js"></script>
<script src="W2_files/libs/quarto-html/quarto.js"></script>
<script src="W2_files/libs/quarto-html/popper.min.js"></script>
<script src="W2_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="W2_files/libs/quarto-html/anchor.min.js"></script>
<link href="W2_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="W2_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="W2_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="W2_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="W2_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul class="collapse">
  <li><a href="#model-assessment-and-selection" id="toc-model-assessment-and-selection" class="nav-link active" data-scroll-target="#model-assessment-and-selection">Model assessment and selection</a>
  <ul class="collapse">
  <li><a href="#plan" id="toc-plan" class="nav-link" data-scroll-target="#plan">Plan</a></li>
  </ul></li>
  <li><a href="#optimism-of-the-training-error-rate" id="toc-optimism-of-the-training-error-rate" class="nav-link" data-scroll-target="#optimism-of-the-training-error-rate">Optimism of the training error rate</a>
  <ul class="collapse">
  <li><a href="#in-sample-error" id="toc-in-sample-error" class="nav-link" data-scroll-target="#in-sample-error">In-sample error</a></li>
  <li><a href="#optimism" id="toc-optimism" class="nav-link" data-scroll-target="#optimism">Optimism</a></li>
  <li><a href="#average-optimism" id="toc-average-optimism" class="nav-link" data-scroll-target="#average-optimism">Average optimism</a></li>
  <li><a href="#covariance-result" id="toc-covariance-result" class="nav-link" data-scroll-target="#covariance-result">Covariance result</a></li>
  </ul></li>
  <li><a href="#expected-in-sample-prediction-error" id="toc-expected-in-sample-prediction-error" class="nav-link" data-scroll-target="#expected-in-sample-prediction-error">Expected in-sample prediction error</a>
  <ul class="collapse">
  <li><a href="#result-for-omega" id="toc-result-for-omega" class="nav-link" data-scroll-target="#result-for-omega">Result for <span class="math inline">\(\omega\)</span></a></li>
  </ul></li>
  <li><a href="#three-ways-to-perform-model-selection" id="toc-three-ways-to-perform-model-selection" class="nav-link" data-scroll-target="#three-ways-to-perform-model-selection">Three ways to perform model selection</a></li>
  <li><a href="#estimates-of-expected-in-sample-prediction-error" id="toc-estimates-of-expected-in-sample-prediction-error" class="nav-link" data-scroll-target="#estimates-of-expected-in-sample-prediction-error">Estimates of (expected) in-sample prediction error</a>
  <ul class="collapse">
  <li><a href="#c_p-statistics" id="toc-c_p-statistics" class="nav-link" data-scroll-target="#c_p-statistics"><span class="math inline">\(C_p\)</span> statistics</a></li>
  <li><a href="#akaike-information-criterion-aic" id="toc-akaike-information-criterion-aic" class="nav-link" data-scroll-target="#akaike-information-criterion-aic">Akaike information criterion (AIC)</a></li>
  </ul></li>
  <li><a href="#the-effective-number-of-parameters" id="toc-the-effective-number-of-parameters" class="nav-link" data-scroll-target="#the-effective-number-of-parameters">The effective number of parameters</a></li>
  <li><a href="#cross-validation-cv" id="toc-cross-validation-cv" class="nav-link" data-scroll-target="#cross-validation-cv">Cross-validation (CV)</a>
  <ul class="collapse">
  <li><a href="#formal-set-up-for-model-assessment" id="toc-formal-set-up-for-model-assessment" class="nav-link" data-scroll-target="#formal-set-up-for-model-assessment">Formal set-up for model assessment</a></li>
  <li><a href="#formal-set-up-for-model-selection" id="toc-formal-set-up-for-model-selection" class="nav-link" data-scroll-target="#formal-set-up-for-model-selection">Formal set-up for model selection</a></li>
  <li><a href="#choice-of-k" id="toc-choice-of-k" class="nav-link" data-scroll-target="#choice-of-k">Choice of <span class="math inline">\(K\)</span></a></li>
  <li><a href="#generalized-cross-validation-gcv" id="toc-generalized-cross-validation-gcv" class="nav-link" data-scroll-target="#generalized-cross-validation-gcv">Generalized cross-validation (GCV)</a></li>
  <li><a href="#the-wrong-and-the-right-way-to-do-cross-validation" id="toc-the-wrong-and-the-right-way-to-do-cross-validation" class="nav-link" data-scroll-target="#the-wrong-and-the-right-way-to-do-cross-validation">The wrong and the right way to do cross-validation</a></li>
  </ul></li>
  <li><a href="#bootstrap-methods" id="toc-bootstrap-methods" class="nav-link" data-scroll-target="#bootstrap-methods">Bootstrap methods</a></li>
  <li><a href="#conclusions-model-selection-and-assessment-and-future-use" id="toc-conclusions-model-selection-and-assessment-and-future-use" class="nav-link" data-scroll-target="#conclusions-model-selection-and-assessment-and-future-use">Conclusions: Model selection and assessment and future use</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a>
  <ul class="collapse">
  <li><a href="#expected-training-and-test-mse-for-linear-regression" id="toc-expected-training-and-test-mse-for-linear-regression" class="nav-link" data-scroll-target="#expected-training-and-test-mse-for-linear-regression">Expected training and test MSE for linear regression</a></li>
  <li><a href="#establish-the-average-optimism-in-the-training-error" id="toc-establish-the-average-optimism-in-the-training-error" class="nav-link" data-scroll-target="#establish-the-average-optimism-in-the-training-error">Establish the average optimism in the training error</a></li>
  <li><a href="#relate-the-covariance-to-the-trace-of-a-linear-smoother" id="toc-relate-the-covariance-to-the-trace-of-a-linear-smoother" class="nav-link" data-scroll-target="#relate-the-covariance-to-the-trace-of-a-linear-smoother">Relate the covariance to the trace of a linear smoother</a></li>
  <li><a href="#derive-the-estimate-of-in-sample-error" id="toc-derive-the-estimate-of-in-sample-error" class="nav-link" data-scroll-target="#derive-the-estimate-of-in-sample-error">Derive the estimate of in-sample error</a></li>
  <li><a href="#additive-error-model-effective-degrees-of-freedom" id="toc-additive-error-model-effective-degrees-of-freedom" class="nav-link" data-scroll-target="#additive-error-model-effective-degrees-of-freedom">Additive error model effective degrees of freedom</a></li>
  <li><a href="#comparing-methods" id="toc-comparing-methods" class="nav-link" data-scroll-target="#comparing-methods">Comparing methods</a></li>
  </ul></li>
  <li><a href="#solutions-to-exercises" id="toc-solutions-to-exercises" class="nav-link" data-scroll-target="#solutions-to-exercises">Solutions to exercises</a></li>
  <li><a href="#reference-links" id="toc-reference-links" class="nav-link" data-scroll-target="#reference-links">Reference links</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">Bibliography</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">MA8701 Advanced methods in statistical inference and learning</h1>
<p class="subtitle lead">L3-4: Model selection and assessment</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mette Langaas </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 13, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>Course homepage: <a href="https://wiki.math.ntnu.no/ma8701/2023v/start" class="uri">https://wiki.math.ntnu.no/ma8701/2023v/start</a></p>
<div class="cell">

</div>
<section id="model-assessment-and-selection" class="level1">
<h1>Model assessment and selection</h1>
<p>(ESL Ch 7.1-7.6,7.10-7.12)</p>
<p>The generalization performance of <span class="math inline">\(\hat{f}\)</span> can be evaluated from the EPE (expected prediction error) on an independent test set.</p>
<p>We use this for</p>
<ul>
<li>Model assessment: evaluate the performance of a selected model</li>
<li>Model selection: select the best model for a specific task - among a set of models</li>
</ul>
<hr>
<section id="plan" class="level2">
<h2 class="anchored" data-anchor-id="plan">Plan</h2>
<ol type="1">
<li>Look at <span class="math inline">\(\text{EPE}(x_0)\)</span> (now called Err(<span class="math inline">\(x_0\)</span>)) and how model complexity can be broken down into irreducible error, squared bias and variance (should be known from before)</li>
<li>Study EPE (Err) unconditional and conditional on the training set</li>
<li>Study optimism of the training error rate, and how in-sample error may shed light</li>
<li>Cross-validation and .632 bootstrap estimates of EPE</li>
<li>How will we build on this in the rest of the course?</li>
</ol>
<p>We finished 1) and 2) in L2, now we continue!</p>
<hr>
</section>
</section>
<section id="optimism-of-the-training-error-rate" class="level1">
<h1>Optimism of the training error rate</h1>
<p>(again - focus is on regression)</p>
<p>First, nothing new, but new notation <span class="math inline">\((X^0,Y^0)\)</span> to specify that a new test observation is drawn from the joint distribution <span class="math inline">\(F\)</span> (both over new <span class="math inline">\(X\)</span> and new <span class="math inline">\(Y\)</span>):</p>
<p><span class="math display">\[\text{Err}_{\cal T}=\text{E}_{X^0,Y^0}[L(Y^0,\hat{f}(X^0))\mid {\cal T}]\]</span></p>
<p>and then the averaging over the training set (both <span class="math inline">\(X\)</span>s and <span class="math inline">\(Y\)</span>s in the training set): <span class="math display">\[\text{Err}=\text{E}_{\cal T} \text{E}_{X^0,Y^0}[L(Y^0,\hat{f}(X^0))\mid {\cal T}]\]</span></p>
<p>This is also called <em>extra-sample error</em> (in contrast to what we now will define to be in-sample).</p>
<hr>
<p>We saw before - from the ESL Figure 7.1, the training error <span class="math inline">\(\overline{\text{err}}\)</span> is (in general) less than (or equal to) the true test error, so not a good estimator for the test error.</p>
<p><span class="math display">\[\overline{\text{err}}=\frac{1}{N} \sum_{i=1}^N L(y_i,\hat{f}(x_i))\]</span></p>
<p>[In Exercise 2.9 we prove that the expected training error is smaller or equal the expected error of a testset - for MLR. Important to work on this exercise!]</p>
<p>Part of this is due to where the <span class="math inline">\(X\)</span> values are “placed”. The test input vectors need not be “in the same positions” as in the training <span class="math inline">\(X\)</span> values (when the mean is taken over the full distribution of <span class="math inline">\(X\)</span>).</p>
<p>To eliminate this “confusing fact”, calculations can be made be assuming the <span class="math inline">\(X\)</span>-values in the training data are kept fixed - and this is called the <em>in-sample error</em>. (We did the same in TMA4267 using the Fahrmeir et al book, Chapter 3.4.)</p>
<hr>
<section id="in-sample-error" class="level2">
<h2 class="anchored" data-anchor-id="in-sample-error">In-sample error</h2>
<p><span class="math display">\[\text{Err}_{\text{in}}=\frac{1}{N}\sum_{i=1}^N \text{E}_{Y^0}[L(Y_i^0,\hat{f}(x_i))\mid {\cal T}]\]</span></p>
<p>Observe that we now take the expected value over distribution of the response - but that the (new) responses are found at the original training points. The training predictor positions <span class="math inline">\(x_i\)</span>, <span class="math inline">\(i=1,\ldots, N\)</span> are fixed. In addition the responses in the training data are also kept fixed, so the only random quantity here is the new responses at the fixed predictors.</p>
<hr>
</section>
<section id="optimism" class="level2">
<h2 class="anchored" data-anchor-id="optimism">Optimism</h2>
<p>Optimism is defined as the difference between the in-sample error and the training error:</p>
<p><span class="math display">\[ \text{op}=\text{Err}_{\text{in}}-\overline{\text{err}}\]</span></p>
</section>
<section id="average-optimism" class="level2">
<h2 class="anchored" data-anchor-id="average-optimism">Average optimism</h2>
<p>is defined as the expected value of the optimism, where the expectation is taken over the distribution of the training responses - denoted <span class="math inline">\({\mathbf y}\)</span> (training predictors still kept fixed):</p>
<p><span class="math display">\[ \omega=\text{E}_{\mathbf y}(op)=\text{E}_{\mathbf y}(\text{Err}_{\text{in}})-\text{E}_{\mathbf y}(\overline{\text{err}})\]</span></p>
<p>Observe that if we write <span class="math inline">\({\cal T}\)</span> then the expectation is taken over the distribution of both the predictors and responses in the training set, and we here write <span class="math inline">\({\mathbf y}\)</span> for taking the distribution only over the reponses in the training set (not the predictors in the training set).</p>
<p>So: we will focus on “modelling” <span class="math inline">\(\omega\)</span>, “instead of” <span class="math inline">\(\text{Err}\)</span>.</p>
<hr>
</section>
<section id="covariance-result" class="level2">
<h2 class="anchored" data-anchor-id="covariance-result">Covariance result</h2>
<p>For squared error (see ESL Exercise 7.4), 0-1 loss, and “other loss functions” it can be shown</p>
<p><span class="math display">\[ \omega=\frac{2}{N} \sum_{i=1}^N \text{Cov}(\hat{y}_i,y_i)\]</span> Interpretation:</p>
<ul>
<li>how much the training error <em>underestimates</em> the true error depends on how strongly the observed response <span class="math inline">\(y_i\)</span> affects its own prediction <span class="math inline">\(\hat{y}_i\)</span>.</li>
<li>the <em>harder</em> we fit the data the greater the covariance - which increases the expected (averaged) optimism.</li>
</ul>
<hr>
</section>
</section>
<section id="expected-in-sample-prediction-error" class="level1">
<h1>Expected in-sample prediction error</h1>
<p><span class="math display">\[ \text{E}_{\mathbf y}(\text{Err}_{\text{in}})=\text{E}_{\mathbf y}(\overline{\text{err}})+\frac{2}{N} \sum_{i=1}^N \text{Cov}(\hat{y}_i,y_i)\]</span> This is the starting point for several methods to “penalize” fitting complex models!</p>
<hr>
<section id="result-for-omega" class="level2">
<h2 class="anchored" data-anchor-id="result-for-omega">Result for <span class="math inline">\(\omega\)</span></h2>
<p>Additive error model and squared loss: <span class="math inline">\(Y=f(X)+\varepsilon\)</span>, with <span class="math inline">\(\hat{y}_i\)</span> obtained by a linear fit with <span class="math inline">\(d\)</span> inputs (or basis functions) <span class="math display">\[\omega=2 \frac{d}{N}\sigma_{\varepsilon}^2\]</span></p>
<p>Proof? We look at a generalization in ESL exercise 7.5.</p>
<p>Observe that the optimism increases with <span class="math inline">\(d\)</span> and decreases with <span class="math inline">\(N\)</span>.</p>
<p>Comment: versions of the formula hold approximately for other error models than linear with squared loss (ESL mention binary data and entropy loss), but not in general for 0-1 loss (page 231, bottom, with reference to Efron 1986 - consult the ESL book).</p>
<hr>
</section>
</section>
<section id="three-ways-to-perform-model-selection" class="level1">
<h1>Three ways to perform model selection</h1>
<ul>
<li><p>Estimate of expected in-sample prediction error (ESL Ch 7.5-7.6): We may develop the average optimism for a class of models that are linear in the parameters (Mallows Cp, AIC, BIC, …) - and compare models of different complexity using <span class="math inline">\(\text{E}_{\mathbf y}(\text{Err}_{\text{in}})\)</span>. Remark: in-sample error is not of interest, but used to choose between models effectively.</p></li>
<li><p>Estimate <span class="math inline">\(\text{Err}\)</span> (ESL Ch 7.10-7.11): We may instead use resampling methods (cross-validation and bootstrapping) to estimate <span class="math inline">\(\text{Err}\)</span> directly (and use that for model selection and assessment).</p></li>
<li><p>In the data rich approach: we have so much data that we use a separate validation set for model selection (and a separate test set for model assessment). That is not the focus of ESL Ch 7.</p></li>
</ul>
<hr>
</section>
<section id="estimates-of-expected-in-sample-prediction-error" class="level1">
<h1>Estimates of (expected) in-sample prediction error</h1>
<p>We have the following result:</p>
<p><span class="math display">\[ \text{E}_{\mathbf y}(\text{Err}_{\text{in}})=\text{E}_{\mathbf y}(\overline{\text{err}})+\frac{2}{N} \sum_{i=1}^N \text{Cov}(\hat{y}_i,y_i)\]</span> where now <span class="math display">\[ \omega=\frac{2}{N} \sum_{i=1}^N \text{Cov}(\hat{y}_i,y_i)\]</span> We now want to get an estimate of the average optimism, to get an estimate of the in-sample prediction error:</p>
<p><span class="math display">\[ \widehat{\text{Err}_{\text{in}}}=\overline{\text{err}}+\hat{\omega}\]</span></p>
<p>Comment: observe that <span class="math inline">\(\overline{\text{err}}\)</span> is now an estimate of <span class="math inline">\(\text{E}_{\mathbf y}(\overline{\text{err}})\)</span> and even though we write <span class="math inline">\(\widehat{\text{Err}_{\text{in}}}\)</span> we are aiming to estimate <span class="math inline">\(\text{E}_{\mathbf y}(\text{Err}_{\text{in}})\)</span>. Focus now is on <span class="math inline">\(\hat{\omega}\)</span>!</p>
<hr>
<section id="c_p-statistics" class="level2">
<h2 class="anchored" data-anchor-id="c_p-statistics"><span class="math inline">\(C_p\)</span> statistics</h2>
<p>for squared error loss (follows directly from the <span class="math inline">\(\omega\)</span>-result for additive error model)</p>
<p><span class="math display">\[C_p=\overline{\text{err}}+2\frac{d}{N}\hat{\sigma}_{\varepsilon}^2\]</span> where <span class="math inline">\(\hat{\sigma}_{\varepsilon}^2\)</span> is estimated from a “low-bias model” (in MLR we use a “full model”).</p>
<p>(This method is presented both in TMA4267 and TMA4268, see also exam question <a href="https://www.math.ntnu.no/emner/TMA4267/2017v/Exam/eV2015.pdf">Problem 3 in TMA4267 in 2015</a> and <a href="https://www.math.ntnu.no/emner/TMA4267/2017v/Exam/lV2015.pdf">solutions</a>.)</p>
<hr>
</section>
<section id="akaike-information-criterion-aic" class="level2">
<h2 class="anchored" data-anchor-id="akaike-information-criterion-aic">Akaike information criterion (AIC)</h2>
<p>Based on different asymptotic (<span class="math inline">\(N \rightarrow \infty\)</span>) relationship for log-likelihood loss functions</p>
<p><span class="math display">\[ -2 \text{E}[\log P_{\hat{\theta}}(Y)]\approx - \frac{2}{N} \text{E}[\text{loglik}]+2 \frac{d}{N} \]</span></p>
<ul>
<li><span class="math inline">\(P_{\hat{\theta}}(Y)\)</span>: family of density for <span class="math inline">\(Y\)</span> where the true density is included</li>
<li><span class="math inline">\(\hat{\theta}\)</span>: MLE of <span class="math inline">\(\theta\)</span></li>
<li><span class="math inline">\(\text{loglik}\)</span>: maximized log-likelihood <span class="math inline">\(\sum_{i=1}^N \log P_{\hat{\theta}}(y_i)\)</span></li>
</ul>
<p><strong>Logistic regression with binomial loglikelihood</strong></p>
<p><span class="math display">\[ \text{AIC}=- \frac{2}{N} \text{loglik}+2 \frac{d}{N}\]</span> <strong>Multiple linear regression</strong> if variance <span class="math inline">\(\sigma_{\varepsilon}^2=\hat{\sigma}_{\varepsilon}^2\)</span> assumed known then AIC is equivalent to <span class="math inline">\(C_p\)</span>.</p>
<p>For nonlinear or similar models then <span class="math inline">\(d\)</span> is replaced by some measure of model complexity.</p>
<hr>
<p><strong>AIC as function of tuning parameter</strong> (back to squared error loss)</p>
<p>We have a set of models <span class="math inline">\(f_{\alpha}(x)\)</span> indexed by some tuning parameter <span class="math inline">\(\alpha\)</span>.</p>
<p><span class="math display">\[\text{AIC}(\alpha)=\overline{\text{err}}(\alpha)+2 \frac{d(\alpha)}{N}\hat{\sigma}_{\varepsilon}^2\]</span></p>
<ul>
<li><span class="math inline">\(\overline{\text{err}}(\alpha)\)</span>: training error</li>
<li><span class="math inline">\(d(\alpha)\)</span> number of parameters</li>
<li><span class="math inline">\(\hat{\sigma}_{\varepsilon}^2\)</span> estimated variance of large model</li>
</ul>
<p>The model complexity <span class="math inline">\(\alpha\)</span> is chosen to minimize <span class="math inline">\(\text{AIC}(\alpha)\)</span>.</p>
<p>This is not true if the models are chosen adaptively (for example basis functions) this formula underestimates the optimism - and we may regard this as the <em>effective number of parameters</em> is larger than <span class="math inline">\(d\)</span>.</p>
<hr>
</section>
</section>
<section id="the-effective-number-of-parameters" class="level1">
<h1>The effective number of parameters</h1>
<p>(ESL 7.6)</p>
<p>The number of parameters <span class="math inline">\(d\)</span> can be generalized into an <em>effective number of parameters</em>. We will look at linear fitting method:</p>
<p><span class="math display">\[ \hat{\mathbf y}={\mathbf Sy}\]</span> where <span class="math inline">\({\mathbf S}\)</span> as a <span class="math inline">\(n \times n\)</span> matrix depending on covariates <span class="math inline">\(x_i\)</span> but not responses <span class="math inline">\(y_i\)</span>.</p>
<ul>
<li>MLR <span class="math inline">\({\mathbf H}={\mathbf X}({\mathbf X}^T{\mathbf X})^{-1}{\mathbf X}^T\)</span></li>
<li>cubic smoothing splines</li>
<li>ridge regression</li>
</ul>
<p>The effective number of parameters is</p>
<p><span class="math display">\[\text{df}({\mathbf S})=\text{trace}({\mathbf S})\]</span></p>
<hr>
<p>Remember that the trace of a square matrix is the sum of the diagonal elements, and trace is often denoted tr.</p>
<p>What is the trace (tr) for MLR?</p>
<p><span class="math inline">\(\text{tr}({\mathbf H})=\text{tr}({\mathbf X}({\mathbf X}^T{\mathbf X})^{-1}{\mathbf X}^T)=\text{tr}(({\mathbf X}^T{\mathbf X})^{-1}{\mathbf X}^T{\mathbf X})=\text{tr}({\mathbf I})_{p+1}=(p+1)\)</span> if intercept model with <span class="math inline">\(p\)</span> covariates.</p>
<hr>
<p><strong>Additive error model and squared loss:</strong> <span class="math inline">\(Y=f(X)+\varepsilon\)</span> with <span class="math inline">\(\text{Var}(\varepsilon)=\sigma_{\varepsilon}^2\)</span> then <span class="math display">\[ \sum_{i=1}^N \text{Cov}(\hat{y}_i,y_i)=\text{trace}({\mathbf S})\sigma_{\varepsilon}^2\]</span> leading to a generalization <span class="math display">\[\text{df}(\hat{{\mathbf y}})=\frac{\sum_{i=1}^N \text{Cov}(\hat{y}_i,y_i)}{\sigma_{\varepsilon}^2}\]</span> See exercise 7.5 to prove this.</p>
<!-- We return to this formula when we look at neural networks with quadratic penalization (weigth decay, ridge regularization) in Part 2.  -->
<hr>
</section>
<section id="cross-validation-cv" class="level1">
<h1>Cross-validation (CV)</h1>
<p>(ESL Ch 7.10, 7.12 - most should be known from TMA4268)</p>
<p>The aim is to estimate <span class="math inline">\(\text{Err}_{\cal T}\)</span>, but from simulation analyses (ESL Ch 7.12) it turns out that cross-validation estimates <span class="math inline">\(\text{Err}\)</span> “the best”.</p>
<p>The starting point for the method is that we only have one training set - and try to use that for either model selection or model assessment (not both).</p>
<p>What to do when both is needed, is not covered in this chapter. Nested cross-validations aka two-layers of cross-validation is one possibility. Another is to set aside data for a test set for model assessment, but use the training set in cross-validation for model selection.</p>
<hr>
<section id="formal-set-up-for-model-assessment" class="level2">
<h2 class="anchored" data-anchor-id="formal-set-up-for-model-assessment">Formal set-up for model assessment</h2>
<ul>
<li><p>The allocation of observation <span class="math inline">\(\{1,\ldots,N\}\)</span> to folds <span class="math inline">\(\{1,\ldots,K\}\)</span> is done using an indexing function <span class="math inline">\(\kappa: \{1,\ldots,N\} \rightarrow \{1,\ldots,K\}\)</span>, that for each observation allocate the observation to one of <span class="math inline">\(K\)</span> folds.</p></li>
<li><p>Further, <span class="math inline">\(\hat{f}^{-k}(x)\)</span> is the fitted function, computed on the observations except the <span class="math inline">\(k\)</span>th fold (the observations from the <span class="math inline">\(k\)</span>th fold is removed).</p></li>
<li><p>The CV estimate of the expected prediction error <span class="math inline">\(\text{Err}=\text{Err}=\text{E}_{\cal T} \text{E}_{X^0,Y^0}[L(Y^0,\hat{f}(X^0))\mid {\cal T}]\)</span> is then <span class="math display">\[ \text{CV}(\hat{f})=\frac{1}{N}\sum_{i=1}^N L(y_i,\hat{f}^{-k(i)}(x_i))\]</span></p></li>
</ul>
<hr>
</section>
<section id="formal-set-up-for-model-selection" class="level2">
<h2 class="anchored" data-anchor-id="formal-set-up-for-model-selection">Formal set-up for model selection</h2>
<ul>
<li><p>The indexing function <span class="math inline">\(\kappa\)</span> is unchanged, and for the fitting function we add a tuning parameter <span class="math inline">\(\alpha\)</span>: <span class="math inline">\(f(x,\alpha)\)</span> such that <span class="math inline">\(\hat{f}^{-k}(x,\alpha)\)</span> is the fitted function using tuning parameter <span class="math inline">\(\alpha\)</span>, with the <span class="math inline">\(k\)</span>th fold removed from the model fitting.</p></li>
<li><p>The expected prediction error is estimated by</p></li>
</ul>
<p><span class="math display">\[ \text{CV}(\hat{f},\alpha)=\frac{1}{N}\sum_{i=1}^N L(y_i,\hat{f}^{-k(i)}(x_i,\alpha))\]</span></p>
<ul>
<li><p>We find the best tuning parameter <span class="math inline">\(\hat{\alpha}\)</span> that minimize the <span class="math inline">\(\text{CV}(\hat{f},\alpha)\)</span>. Alternatively the <em>one-standard error rule</em> can be used: choose the most parsimonious (“smallest”) model whose error is no more than one standard error above the error of the best model.</p></li>
<li><p>This best chosen model is then fit to all the data. (ESL page 242).</p></li>
</ul>
<hr>
</section>
<section id="choice-of-k" class="level2">
<h2 class="anchored" data-anchor-id="choice-of-k">Choice of <span class="math inline">\(K\)</span></h2>
<ul>
<li>Popular choices are 5 and 10 based on observations in simulation studies- and arguments similar to a bias-variance trace off.</li>
<li><span class="math inline">\(K=N\)</span> is called <em>leave-one-out</em> cross-validation LOOCV, and gives the lowest bias for estimating the <span class="math inline">\(\text{Err}\)</span>.</li>
</ul>
<hr>
</section>
<section id="generalized-cross-validation-gcv" class="level2">
<h2 class="anchored" data-anchor-id="generalized-cross-validation-gcv">Generalized cross-validation (GCV)</h2>
<p>For LOOCV with squared loss and linear fitting. Remember <span class="math display">\[ \hat{\mathbf y}={\mathbf Sy}\]</span> For many fitting methods (including MLR)</p>
<p><span class="math display">\[ \frac{1}{N}\sum_{i=1}^N [y_i-\hat{f}^{-i}(x_i)]^2=\frac{1}{N}\sum_{i=1}^N [\frac{y_i-\hat{f}(x_i)}{1-S_{ii}}]^2\]</span> where <span class="math inline">\(S_{ii}\)</span> is the <span class="math inline">\(i\)</span>th diagonal element of <span class="math inline">\({\mathbf S}\)</span>. This leads to the GCV approximation:</p>
<p><span class="math display">\[ \text{GCV}(\hat{f})=\frac{1}{N}\sum_{i=1}^N [\frac{y_i-\hat{f}(x_i)}{1-\text{tr}({\mathbf S})/N}]^2\]</span> where we recognise the effective number of parameters <span class="math inline">\(\text{trace}({\mathbf S})\)</span>. In some settings the <span class="math inline">\(\text{trace}({\mathbf S})\)</span> is computed more easily than the individual elements <span class="math inline">\(S_{ii}\)</span>.</p>
<hr>
</section>
<section id="the-wrong-and-the-right-way-to-do-cross-validation" class="level2">
<h2 class="anchored" data-anchor-id="the-wrong-and-the-right-way-to-do-cross-validation">The wrong and the right way to do cross-validation</h2>
<p>In short: make sure that all part of the model fit process is “inside” the CV.</p>
<p>See learning material from TMA4268: <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/5Resample/5Resample.html#the_right_and_the_wrong_way_to_do_cross-validation">Module 5: Resampling</a>, and I also recommend to work on <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/5Resample/5Resample.html#problem_3:_selection_bias_and_the_%E2%80%9Cwrong_way_to_do_cv%E2%80%9D">Problem 3</a> with <a href="">solutions</a></p>
<hr>
</section>
</section>
<section id="bootstrap-methods" class="level1">
<h1>Bootstrap methods</h1>
<p>(ESL Ch 7.11 - bootstrapping is known from TMA4268 and TMA4300, but not the special case of estimating <span class="math inline">\(\text{Err}\)</span>). <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/5Resample/5Resample.html#the_bootstrap">Bootstrap in TMA4268: Module 5</a></p>
<p><strong>Notation:</strong> <span class="math inline">\({\mathbf Z}=(z_1,\ldots,z_N)\)</span> is the training set with <span class="math inline">\(z_i=(x_i,y_i)\)</span>.</p>
<p><strong>Aim:</strong> Of interest is some quantity calculated from the data <span class="math inline">\({\mathbf Z}\)</span>, denoted <span class="math inline">\(S({\mathbf Z})\)</span>. We will have focus on the expected prediction error.</p>
<p><strong>Resampling:</strong> We draw with replacement from <span class="math inline">\({\mathbf Z}\)</span> a total of <span class="math inline">\(N\)</span> observastions into <span class="math inline">\({\mathbf Z}^{*b}\)</span>. We repeat this <span class="math inline">\(B\)</span> times.</p>
<p><strong>Estimator for expected predicted error <span class="math inline">\(\text{Err}\)</span>:</strong></p>
<p><span class="math display">\[\widehat{\text{Err}}_{\text{boot}}=\frac{1}{B}\frac{1}{N}\sum_{b=1}^B \sum_{i=1}^N L(y_i,\hat{f}^{*b}(x_i))\]</span></p>
<hr>
<p>However - <span class="math inline">\(\widehat{\text{Err}}_{\text{boot}}\)</span> is not a good estimator: bootstrap datasets are acting as training data and the original data as a test sample - and the two samples have observations in common.</p>
<p>This overlap can make predictions too good. Remeber, in CV we have no overlap.</p>
<p><strong>Q:</strong> What is the probability that observation <span class="math inline">\(i\)</span> is included in bootstrap sample <span class="math inline">\(b\)</span>?</p>
<hr>
<p>The problem is given in TMA4268 Module 5 as <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/5Resample/5Resample.html#recexboot">Problem 1</a> with (handwritten) <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/5Resample/5Resample-sol.pdf">solutions</a>.</p>
<p>The answer is <span class="math inline">\(1-(1-\frac{1}{N})^N\approx 1-e^{-1}=0.632\)</span>.</p>
<p>Why is this relevant?</p>
<p>What if we try to change the bootstrap <span class="math inline">\(\text{Err}\)</span> estimator - so that we for each observation <span class="math inline">\(i\)</span> only keep predictions from bootstrap samples this observation is not present? Then we would mimick the CV-estimator.</p>
<hr>
<p>The <em>leave-one-out</em> bootstrap estimate:</p>
<p><span class="math display">\[\widehat{\text{Err}}^{(1)}=\frac{1}{N} \sum_{i=1}^N \frac{1}{\lvert C^{-i} \rvert} \sum_{b \in C^{-i}} L(y_i,\hat{f}^{*b}(x_i))\]</span> where <span class="math inline">\(C^{-i}\)</span> are the indices in the bootstrap sample <span class="math inline">\(b\)</span> that do not contain observation <span class="math inline">\(i\)</span>, and <span class="math inline">\(\lvert C^{-i} \rvert\)</span> is the number of samples. (<span class="math inline">\(B\)</span> must be large enough that we do not get any <span class="math inline">\(C^{-i}\)</span>s that are empty, or leave out these zero sets in the formula.)</p>
<p>Comment: this is also called out-of-bootstrap, and is closely connected to the popular out-of-bag estimate for random forests.</p>
<hr>
<p>There is an addition fix to make the estimate even better.</p>
<p>Since the average number of distinct observations in each bootstrap sample is approximately <span class="math inline">\(0.632 N\)</span> - and the bootstrap sample behaves like a training set - this gives a socalled training-set-size bias (similar to C with <span class="math inline">\(K=2\)</span>), meaning that the leave-one-out bootstrap estimator will be <em>biased upwards</em>. This can be fixed by weighing together the leave-one-out boostrap estimator with the training error.</p>
<p>The “.632” estimator:</p>
<p><span class="math display">\[\widehat{\text{Err}}^{(.632)}=0.368 \overline{\text{err}}+0.632 \widehat{\text{Err}}^{(1)}\]</span></p>
<hr>
<p>According to ESL (page 251): the derivation of the .632 estimator is complex, and the estimator is expected to work well in situation where the data is not overfitted, but may break down in overfit situations.</p>
<p>According to CASI (page 323) the .632 rule is less variable than the leave-one-out CV.</p>
<p>Example of this on page 251-252: two equal size classes where predictors independent of class, classification with <span class="math inline">\(1\)</span>NN gives <span class="math inline">\(\overline{\text{err}}=0\)</span>, <span class="math inline">\(\widehat{\text{Err}}^{(1)}=0.5\)</span> and thus <span class="math inline">\(\widehat{\text{Err}}^{(.632)}=0.632\cdot 0.5=0.316\)</span>, where here the true error rate is <span class="math inline">\(0.5\)</span>.</p>
<hr>
<p>There is an improved version of the estimator - taking into account the amount of overfitting, leading to an adjustment to the weight <span class="math inline">\(w=0.632\)</span> (and <span class="math inline">\(1-w=0.368\)</span>) dependent on a socalled <em>no-information error rate</em>=<span class="math inline">\(\gamma\)</span>=the error rate of the prediction rule when predictors and class labels are independent.</p>
<p><span class="math display">\[\hat{\gamma}=\frac{1}{N^2}\sum_{i=1}^{N}\sum_{i´=1}^N L(y_i,\hat{f}(x_{i´}))\]</span> Further the <em>relative overfitting rate</em> is defined to be</p>
<p><span class="math display">\[ \hat{R}=\frac{\widehat{\text{Err}}^{(1)}-\overline{\text{err}}}{\hat{\gamma}-\overline{\text{err}}}\]</span></p>
<hr>
<p>Finally, the “.632+”-estimator is</p>
<p><span class="math display">\[\widehat{\text{Err}}^{(.632+)}=(1-\hat{w}) \overline{\text{err}}+ \hat{w} \widehat{\text{Err}}^{(1)}\]</span> where <span class="math inline">\(\hat{w}=\frac{0.632}{1-0.368 \hat{R}}\)</span>.</p>
<p>For details on this approach consult ESL page 252-253.</p>
<hr>
</section>
<section id="conclusions-model-selection-and-assessment-and-future-use" class="level1">
<h1>Conclusions: Model selection and assessment and future use</h1>
<p><strong>Group discussion:</strong> Construct a “mind map”/“overview sheet”/“concept map” for the “Model assessement and selection” topics, and write down important take home messages!</p>
<hr>
<!-- * in a perfect world we would be rich on data and can divide available data into sets for training, validation and testing -->
<!-- * cool covariance-result on expected optimism for training error related to in-sample prediction error (the covariance) - that is used for finding model selection criteria (but not for model assessment) -->
<!-- * estimating expected prediction (test) error for a particular training set is not easy in general (if we only have this one training set), but cross-validation and bootstrapping may provide reasonable estimates of the expected test error $\text{Err}$. -->
</section>
<section id="exercises" class="level1">
<h1>Exercises</h1>
<section id="expected-training-and-test-mse-for-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="expected-training-and-test-mse-for-linear-regression">Expected training and test MSE for linear regression</h2>
<p>Do exercise 2.9.</p>
<p>Important take home message: We have proven (for MLR) that the expected test MSE is always at least as large as the expected training MSE.</p>
</section>
<section id="establish-the-average-optimism-in-the-training-error" class="level2">
<h2 class="anchored" data-anchor-id="establish-the-average-optimism-in-the-training-error">Establish the average optimism in the training error</h2>
<p>Exercise 7.4</p>
<hr>
</section>
<section id="relate-the-covariance-to-the-trace-of-a-linear-smoother" class="level2">
<h2 class="anchored" data-anchor-id="relate-the-covariance-to-the-trace-of-a-linear-smoother">Relate the covariance to the trace of a linear smoother</h2>
<p>Exercise 7.5</p>
<p>Need to know about covariance and variance of linear combinations. The reading list in TMA4267 included Härdle and Simar (2015): Applied Multivariate Statistical Analysis (fourth edition) - <a href="https://www.springer.com/gp/book/9783662451717">ebook from Springer available at NTNU</a>. Alternatively [classnotes from TMA4267 (page 58-59)] (https://www.math.ntnu.no/emner/TMA4267/2017v/TMA4267V2017Part1.pdf)</p>
<hr>
</section>
<section id="derive-the-estimate-of-in-sample-error" class="level2">
<h2 class="anchored" data-anchor-id="derive-the-estimate-of-in-sample-error">Derive the estimate of in-sample error</h2>
<p>for the additive error model <span class="math display">\[Y=f(X)+\varepsilon\]</span> of Eq (7.24):</p>
<p><span class="math display">\[ \text{E}_{y}(\text{Err}_{\text{in}})=\text{E}_{y}(\bar{\text{err}})+2 \cdot \frac{d}{N} \sigma^2_{\varepsilon}\]</span></p>
<p>Exercise 7.1</p>
<hr>
</section>
<section id="additive-error-model-effective-degrees-of-freedom" class="level2">
<h2 class="anchored" data-anchor-id="additive-error-model-effective-degrees-of-freedom">Additive error model effective degrees of freedom</h2>
<p>Exercise 7.6</p>
<p>Show that for an additive-error model, the eﬀective degrees-of freedom for the <span class="math inline">\(k\)</span>-nearest-neighbors regression ﬁt is <span class="math inline">\(N/k\)</span>.</p>
</section>
<section id="comparing-methods" class="level2">
<h2 class="anchored" data-anchor-id="comparing-methods">Comparing methods</h2>
<p>Exercise 7.9 (minus BIC).</p>
<p>For the prostate data of ESL Chapter 3, carry out a best-subset linear regression analysis, as in Table 3.3 (third column from left). Compute the AIC, ﬁve- and tenfold cross-validation, and bootstrap .632 estimates of prediction error. Discuss the results.</p>
</section>
</section>
<section id="solutions-to-exercises" class="level1">
<h1>Solutions to exercises</h1>
<p>Please try yourself first, or take a small peek - and try some more - before fully reading the solutions. Report errors or improvements to <a href="mailto:Mette.Langaas@ntnu.no" class="email">Mette.Langaas@ntnu.no</a>. (The solutions given here are very similar to the UiO STK-IN4300 solutions, see link under References.)</p>
<ul>
<li><a href="https://github.com/mettelang/MA8701V2023/blob/main/Part1/ELSe29.pdf">2.9</a></li>
<li><a href="https://github.com/mettelang/MA8701V2023/blob/main/Part1/ELSe74.pdf">7.4</a></li>
<li><a href="https://github.com/mettelang/MA8701V2023/blob/main/Part1/ELSe75.pdf">7.5</a></li>
<li>7.1</li>
<li>7.6</li>
<li>Solution from Hastie: <a href="https://waxworksmath.com/Authors/G_M/Hastie/Code/Chapter7/Exericse_7_9.R">7.9</a> also using <a href="https://waxworksmath.com/Authors/G_M/Hastie/Code/Chapter7/utils.R">utils.R</a></li>
</ul>
</section>
<section id="reference-links" class="level1">
<h1>Reference links</h1>
<ul>
<li><p><a href="https://hastie.su.domains/ElemStatLearn/">ESL official errata:</a> and choose “Errata” in the left menu</p></li>
<li><p><a href="https://waxworksmath.com/Authors/G_M/Hastie/hastie.html">ESL solutions to exercises</a></p></li>
<li><p><a href="https://www.uio.no/studier/emner/matnat/math/STK-IN4300/h20/exercises.html">ESL solutions from UiO</a></p></li>
<li><p><a href="https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf">CASI Computer Age Statistical Inference, Efron and Hastie (2017). Chapter 12: Cross-Validation and <span class="math inline">\(C_p\)</span> Estimates of Prediction Error</a></p></li>
<li><p><a href="https://link.springer.com/chapter/10.1007/978-0-387-22456-5_7">Burnham and Andersen (2002): Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach. Springer. Chapter 7: Statistical Theory and Numerical Results</a></p></li>
</ul>
</section>
<section id="bibliography" class="level1">
<h1>Bibliography</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-casi" class="csl-entry" role="doc-biblioentry">
Efron, Bradley, and Trevor Hastie. 2016. <em>Computer Age Statistical Inference - Algorithms, Evidence, and Data Science</em>. Cambridge University Press. <a href="https://hastie.su.domains/CASI/">https://hastie.su.domains/CASI/</a>.
</div>
</div>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>