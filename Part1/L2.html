<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mette Langaas">
<meta name="dcterms.date" content="2023-01-08">

<title>MA8701 Advanced methods in statistical inference and learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="L2_files/libs/clipboard/clipboard.min.js"></script>
<script src="L2_files/libs/quarto-html/quarto.js"></script>
<script src="L2_files/libs/quarto-html/popper.min.js"></script>
<script src="L2_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="L2_files/libs/quarto-html/anchor.min.js"></script>
<link href="L2_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="L2_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="L2_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="L2_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="L2_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul class="collapse">
  <li><a href="#plan" id="toc-plan" class="nav-link active" data-scroll-target="#plan">Plan</a></li>
  <li><a href="#statistical-decision-theoretic-framework" id="toc-statistical-decision-theoretic-framework" class="nav-link" data-scroll-target="#statistical-decision-theoretic-framework">Statistical decision theoretic framework</a>
  <ul class="collapse">
  <li><a href="#classification-set-up" id="toc-classification-set-up" class="nav-link" data-scroll-target="#classification-set-up">Classification set-up</a></li>
  <li><a href="#group-discussion" id="toc-group-discussion" class="nav-link" data-scroll-target="#group-discussion">Group discussion</a></li>
  </ul></li>
  <li><a href="#model-assessment-and-selection" id="toc-model-assessment-and-selection" class="nav-link" data-scroll-target="#model-assessment-and-selection">Model assessment and selection</a>
  <ul class="collapse">
  <li><a href="#plan-1" id="toc-plan-1" class="nav-link" data-scroll-target="#plan-1">Plan</a></li>
  <li><a href="#the-bias-variance-trade-off" id="toc-the-bias-variance-trade-off" class="nav-link" data-scroll-target="#the-bias-variance-trade-off">The bias-variance trade-off</a></li>
  <li><a href="#expected-prediction-error" id="toc-expected-prediction-error" class="nav-link" data-scroll-target="#expected-prediction-error">Expected prediction error</a></li>
  <li><a href="#group-discussion-1" id="toc-group-discussion-1" class="nav-link" data-scroll-target="#group-discussion-1">Group discussion</a></li>
  <li><a href="#loss-function-and-training-error-for-classification" id="toc-loss-function-and-training-error-for-classification" class="nav-link" data-scroll-target="#loss-function-and-training-error-for-classification">Loss function and training error for classification</a></li>
  </ul></li>
  <li><a href="#discussion-and-conclusions" id="toc-discussion-and-conclusions" class="nav-link" data-scroll-target="#discussion-and-conclusions">Discussion and conclusions</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a>
  <ul class="collapse">
  <li><a href="#what-are-the-most-important-results-from-the-statistical-decision-theoretic-framework" id="toc-what-are-the-most-important-results-from-the-statistical-decision-theoretic-framework" class="nav-link" data-scroll-target="#what-are-the-most-important-results-from-the-statistical-decision-theoretic-framework">What are the most important results from the “Statistical decision theoretic framework”?</a></li>
  <li><a href="#look-into-the-derivation-for-the-bias-and-variance" id="toc-look-into-the-derivation-for-the-bias-and-variance" class="nav-link" data-scroll-target="#look-into-the-derivation-for-the-bias-and-variance">Look into the derivation for the bias and variance</a></li>
  <li><a href="#key-results-from-logistic-regression" id="toc-key-results-from-logistic-regression" class="nav-link" data-scroll-target="#key-results-from-logistic-regression">Key results from logistic regression</a></li>
  </ul></li>
  <li><a href="#solutions-to-exercises" id="toc-solutions-to-exercises" class="nav-link" data-scroll-target="#solutions-to-exercises">Solutions to exercises</a>
  <ul class="collapse">
  <li><a href="#key-results-from-logistic-regression-1" id="toc-key-results-from-logistic-regression-1" class="nav-link" data-scroll-target="#key-results-from-logistic-regression-1">Key results from logistic regression</a></li>
  </ul></li>
  <li><a href="#reference-links" id="toc-reference-links" class="nav-link" data-scroll-target="#reference-links">Reference links</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">Bibliography</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">MA8701 Advanced methods in statistical inference and learning</h1>
<p class="subtitle lead">L2: Classification and statistical decision theory, model selection and assessment</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mette Langaas </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 8, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>Course homepage: <a href="https://wiki.math.ntnu.no/ma8701/2023v/start" class="uri">https://wiki.math.ntnu.no/ma8701/2023v/start</a></p>
<div class="cell">

</div>
<section id="plan" class="level1">
<h1>Plan</h1>
<p>Continue with the decision theoretic framework from L1, but now for classification. Bias-variance trade-off.</p>
<ul>
<li>Classification - should not be new (ESL Ch 4.1-4.5, except 4.4.4)</li>
<li>Statistical decision theoretic framework for classification (ESL 2.4)</li>
<li>and the bias-variance trade-off (ESL 2.9 and 7.2-7.3)</li>
</ul>
</section>
<section id="statistical-decision-theoretic-framework" class="level1">
<h1>Statistical decision theoretic framework</h1>
<p>(ESL Ch 2.4)</p>
<p>is a mathematical framework for developing models <span class="math inline">\(f\)</span> - and assessing optimality.</p>
<p>Last time, regression:</p>
<ul>
<li><span class="math inline">\(X \in \Re^p\)</span></li>
<li><span class="math inline">\(Y \in \Re\)</span></li>
<li><span class="math inline">\(P(X,Y)\)</span> joint distribution of covariates and respons</li>
</ul>
<p>Aim: find a function <span class="math inline">\(f(X)\)</span> for predicting <span class="math inline">\(Y\)</span> from some inputs <span class="math inline">\(X\)</span>.</p>
<p>Ingredients: Loss function <span class="math inline">\(L(Y,f(X))\)</span> - for <em>penalizing errors in the prediction</em>.</p>
<p>Criterion for choosing <span class="math inline">\(f\)</span>: Expected prediction error (EPE)</p>
<hr>
<p><span class="math display">\[ \text{EPE}(f)=\text{E}_{X,Y}[L(Y,f(X))]=\int_{x,y}L(y,f(x))p(x,y)dxdy\]</span> Choose <span class="math inline">\(f\)</span> to minimize the <span class="math inline">\(\text{EPE}(f)\)</span>.</p>
<p>What changes do we need to do for classifiation?</p>
<hr>
<section id="classification-set-up" class="level2">
<h2 class="anchored" data-anchor-id="classification-set-up">Classification set-up</h2>
<ul>
<li><span class="math inline">\(X \in \Re^p\)</span></li>
<li><span class="math inline">\(G \in {\cal G}=\{1,\ldots,K\}\)</span></li>
<li><span class="math inline">\(\hat{G}(X) \in {\cal G}=\{1,\ldots,K\}\)</span> (why <span class="math inline">\(f\)</span> for regression and <span class="math inline">\(\hat{G}\)</span> for classification? Why not <span class="math inline">\(g\)</span> or <span class="math inline">\(h\)</span>?)</li>
<li><span class="math inline">\(L(G,\hat{G}(X))\)</span> is a <span class="math inline">\(K\times K\)</span> matrix where <span class="math inline">\(K=\lvert G \rvert\)</span>, with elements <span class="math inline">\(l_{jk}\)</span> giving the price to pay to misclassify an observation with true class <span class="math inline">\(g_j\)</span> to class <span class="math inline">\(g_k\)</span>.</li>
<li>Elements on the diagonal of <span class="math inline">\(L\)</span> is 0, and off-diagonal elements are often <span class="math inline">\(1\)</span>.</li>
</ul>
<hr>
<p>We would like to find <span class="math inline">\(\hat{G}\)</span> to minimize the EPE:</p>
<p><span class="math display">\[\text{EPE}=\text{E}_{G,X}[L(G,\hat{G}(X))]=\text{E}_X \text{E}_{G\mid X}[L(G,\hat{G}(X))]\]</span> <span class="math display">\[=\text{E}_X \{ \sum_{k=1}^K L(g_k,\hat{G}(X))P(G=g_k \mid X=x) \} \]</span></p>
<hr>
<p>Also here it is sufficient to minimize the loss for each value of <span class="math inline">\(x\)</span> (pointwise)</p>
<p><span class="math display">\[ \hat{G}=\text{argmin}_{g \in {\cal G}}\sum_{k=1}^K L(g_k,\hat{G}(X))P(G=g_k \mid X=x) \]</span></p>
<p>In the special case of 0-1 loss (off-diagonal elements in <span class="math inline">\(L\)</span> equal to 1) then all <span class="math inline">\(k\)</span> except the correct class gives loss <span class="math inline">\(1\)</span> with probability <span class="math inline">\(P(G=g_k \mid X=x)\)</span>. Summing over the wrong classes gives the same as taking <span class="math inline">\(1\)</span> minus the conditional probability of the correct class <span class="math inline">\(g\)</span>.</p>
<hr>
<p><span class="math display">\[\hat{G}=\text{argmin}_{g \in {\cal G}} [1-P(G=g \mid X=x)]\]</span></p>
<p><span class="math display">\[=\text{argmax}_{g \in {\cal G}}P(G=g \mid X=x)\]</span></p>
<p>The <em>Bayes classifier</em> classifies to the most probable class using the conditional distribution <span class="math inline">\(P(G \mid X)\)</span>. The class boundaries are the <em>Bayes decision boundaries</em> and the error rate is the <em>Bayes rate</em>.</p>
<!-- Note: can also achieve the same result with dummy variable coding for classes and squared error. -->
<hr>
</section>
<section id="group-discussion" class="level2">
<h2 class="anchored" data-anchor-id="group-discussion">Group discussion</h2>
<ol type="1">
<li>What do we know about classification? (TMA4268 and TMA4315 mainly, or ESL ch 4.1-4.5, except 4.4.4)</li>
</ol>
<p>Some possible variants: * What is the difference between discrimination and classification? * What are the sampling vs diagnostic paradigm? Which paradigm for <span class="math inline">\(k\)</span>NN and LDA? * Parametric vs non-parametric methods?</p>
<ol start="2" type="1">
<li>Logistic regression is by many seen as the “most important method in machine learning”. What do we remember about logistic regression?</li>
</ol>
<hr>
<p><strong>Resources</strong></p>
<p>(mostly what we learned in TMA4267, or ESL ch 4.1-4.5, except 4.4.4)</p>
<ul>
<li>From TMA4268: <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/TMA4268overview.html">Overview</a> and in particular <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/4Classif/4Classif.html">Module 4: Classification</a> and <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/2StatLearn/2StatLearn.html#k-nearest_neighbour_classifier">Module 2: Statistical learning</a></li>
<li>From TMA4315: <a href="https://www.math.ntnu.no/emner/TMA4315/2018h/TMA4315overviewH2018.html">Overview</a> and in particular <a href="https://www.math.ntnu.no/emner/TMA4315/2018h/3BinReg.html">Module 3: Binary regression</a> and for more than two classes: <a href="https://www.math.ntnu.no/emner/TMA4315/2018h/6Categorical.html">Module 6: Categorical regression</a>.</li>
</ul>
<hr>
</section>
</section>
<section id="model-assessment-and-selection" class="level1">
<h1>Model assessment and selection</h1>
<p>(ESL Ch 7.1-7.6,7.10-7.12)</p>
<p>We use a training set to estimate <span class="math inline">\(\hat{f}\)</span>.</p>
<p>The generalization performance of <span class="math inline">\(\hat{f}\)</span> can be evaluated from the EPE (expected prediction error) on an independent data set</p>
<p>We use this for</p>
<ul>
<li>Model assessment: evaluate the performance of a selected model</li>
<li>Model selection: select the best model for a specific task - among a set of models</li>
</ul>
<hr>
<p>If we are in a <em>data rich situation</em> we “just” divide our data into three parts, and use</p>
<ul>
<li>one for training</li>
<li>one for validation (model selection)</li>
<li>one for testing (model assessment)</li>
</ul>
<p>A typical split might be 50-60% training and 20-25% validation and test, but this depends on the complexity of the model to be fitted and the signal-to-noise ratio in the data.</p>
<p>The focus in Ch 7 of ESL is to present methods to be used in the situations where we <em>do not have enough data</em> to rely on the training-validation-testing split.</p>
<p>And, even if we have enough data - what we now will learn will give us insight into much used methods for model assessment and model selection!</p>
<hr>
<section id="plan-1" class="level2">
<h2 class="anchored" data-anchor-id="plan-1">Plan</h2>
<ol type="1">
<li><p>Look at <span class="math inline">\(\text{EPE}(x_0)\)</span> (now called Err(<span class="math inline">\(x_0\)</span>) after we have estimated <span class="math inline">\(f\)</span>) and how model complexity can be broken down into irreducible error, squared bias and variance (should be known from before)</p></li>
<li><p>Study EPE (Err) unconditional and conditional on the training set</p></li>
<li><p>Study optimism of the training error rate, and how in-sample error may shed light</p></li>
<li><p>Cross-validation and .632 bootstrap estimates of EPE</p></li>
<li><p>How will we build on this in Parts 2-4?</p></li>
</ol>
<hr>
</section>
<section id="the-bias-variance-trade-off" class="level2">
<h2 class="anchored" data-anchor-id="the-bias-variance-trade-off">The bias-variance trade-off</h2>
<p>(ESL p26 and 7.3)</p>
<p>Assume: <span class="math display">\[ Y=f(X)+\varepsilon\]</span> where <span class="math inline">\(\text{E}(\varepsilon)=0\)</span> and <span class="math inline">\(\text{Var}(\varepsilon)=\sigma_{\varepsilon}^2\)</span>.</p>
<p>For the bias-variance decomposition we only consider the squared loss. Why?</p>
<p>In Ch 7 we use the notation Err instead of EPE (expected prediction error) that we used in Ch 2, and now we have estimated <span class="math inline">\(f\)</span> by <span class="math inline">\(\hat{f}\)</span>.</p>
<p>Let <span class="math inline">\(\text{Err}(x_0)\)</span> be the expected prediction error of a regression fit <span class="math inline">\(\hat{f}(X)\)</span> at a (new) input value <span class="math inline">\(X=x_0\)</span>. As in Ch 2 the expected value is over <span class="math inline">\((X,Y)\)</span> for Err, and we may look at <span class="math display">\[ \text{Err}=E_{x_0} \text{Err}(x_0)\]</span> How can we partition <span class="math inline">\(\text{Err}(x_0)\)</span> into different sources?</p>
<hr>
<p><span class="math display">\[ \text{Err}(x_0)=\text{E}[(Y-\hat{f}(x_0))^2 \mid X=x_0]=\sigma_{\varepsilon}^2 +  \text{Var}[\hat{f}(x_0)]+[\text{Bias}(\hat{f}(x_0))]^2\]</span></p>
<ul>
<li>First term: irreducible error, <span class="math inline">\(\text{Var}(\varepsilon)=\sigma^2\)</span> and is always present unless we have measurements without error. This term cannot be reduced regardless how well our statistical model fits the data.</li>
<li>Second term: variance of the prediction at <span class="math inline">\(x_0\)</span> or the expected deviation around the mean at <span class="math inline">\(x_0\)</span>. If the variance is high, there is large uncertainty associated with the prediction.</li>
<li>Third term: squared bias. The bias gives an estimate of how much the prediction differs from the true mean. If the bias is low the model gives a prediction which is close to the true value.</li>
</ul>
<hr>
<section id="derivation" class="level3">
<h3 class="anchored" data-anchor-id="derivation">Derivation</h3>
<p>If you need to refresh your memory of the bias-variance trade-off, you might also look at the exam Problem 2 <a href="https://www.math.ntnu.no/emner/TMA4268/Exam/V2018e.pdf">TMA4268 2018 exam</a> with <a href="https://www.math.ntnu.no/emner/TMA4268/Exam/e2018sol.pdf">solutions</a></p>
<p>Also: <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/TMA4268overview.html">TMA4268</a> and in particular <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/2StatLearn/2StatLearn.html">Module 2</a></p>
<hr>
<p>The following is a derivation:</p>
<p><span class="math display">\[\begin{align*} \text{Err}(x_0)&amp;=\text{E}[(Y-\hat{f}(x_0))^2 \mid X=x_0]\\
&amp;=\text{E}[Y^2 + \hat{f}(x_0)^2 - 2 Y \hat{f}(x_0)\mid X=x_0] \\
&amp;= \text{E}[Y^2\mid X=x_0] + \text{E}[\hat{f}(x_0)^2\mid X=x_0] - \text{E}[2Y \hat{f}(x_0)\mid X=x_0]\\
&amp;= \text{Var}[Y\mid X=x_0] + \text{E}[Y\mid X=x_0]^2 + \text{Var}[\hat{f}(x_0)\mid X=x_0] + \text{E}[\hat{f}(x_0)\mid X=x_0]^2 - 2 \text{E}[Y\mid X=x_0]\text{E}[\hat{f}(x_0)\mid X=x_0] \\
&amp;= \text{Var}[Y\mid X=x_0]+f(x_0)^2+\text{Var}[\hat{f}(x_0)\mid X=x_0]+\text{E}[\hat{f}(x_0)\mid X=x_0]^2-2f(x_0)\text{E}[\hat{f}(x_0)\mid X=x_0]\\
&amp;= \text{Var}[Y\mid X=x_0]+\text{Var}[\hat{f}(x_0)\mid X=x_0]+(f(x_0)-\text{E}[\hat{f}(x_0)\mid X=x_0])^2\\
&amp;= \text{Var}(\varepsilon\mid X=x_0) +  \text{Var}[\hat{f}(x_0)\mid X=x_0]+[\text{Bias}(\hat{f}(x_0))\mid X=x_0]^2
\end{align*}\]</span> </p>
<p>(For some applications also the training Xs are fixed.) See the exercises below to study the results for <span class="math inline">\(k\)</span>NN and OLS.</p>
<hr>
</section>
</section>
<section id="expected-prediction-error" class="level2">
<h2 class="anchored" data-anchor-id="expected-prediction-error">Expected prediction error</h2>
<p>(ESL 7.2 and 7.4, and we are now back to a general loss function - but first have regression in mind)</p>
<p>If we now keep the training set fixed (we would do that in practice - since we often only have one training set):</p>
<p><span class="math display">\[ \text{Err}_{\cal T}=\text{E}[L(Y,\hat{f}(X))\mid {\cal T}]\]</span></p>
<p>as before the expected value is with respect to <span class="math inline">\((X,Y)\)</span>, but the training set is fixed - so that this is the test set error is for this specific training set <span class="math inline">\({\cal T}\)</span>.</p>
<p>Getting back to the unconditional version, we take expected value over ALL that is random - including the training set <span class="math display">\[ \text{Err}=\text{E}(\text{E}[L(Y,\hat{f}(X))\mid {\cal T}])=\text{E}_{\cal T} [\text{Err}_{\cal T}]\]</span></p>
<p>We want to estimate <span class="math inline">\(\text{Err}_{\cal T}\)</span>, but we will soon see that it turns out that most methods estimate <span class="math inline">\(\text{Err}\)</span>.</p>
<hr>
<section id="training-error" class="level3">
<h3 class="anchored" data-anchor-id="training-error">Training error</h3>
<p>(also referred to as apparent error)</p>
<p>For a regression problem: The training error is the average loss over the training sample: <span class="math display">\[\overline{\text{err}}=\frac{1}{N} \sum_{i=1}^N L(y_i,\hat{f}(x_i))\]</span></p>
<!--Unfortuneately the training error is  not a good estimate of the test error. -->
<hr>
</section>
</section>
<section id="group-discussion-1" class="level2">
<h2 class="anchored" data-anchor-id="group-discussion-1">Group discussion</h2>
<p>Look at Figure 7.1 (with figure caption) on page 220 in the ESL book. The text reads that “100 simulated training sets of size 50” and that “lasso produced sequence of fits”.</p>
<p>Explain what you see - in particular what are the red and blue lines and the bold lines. What can you conclude from the figure?</p>
<ul>
<li>Red lines</li>
<li>Bold red line</li>
<li>Blue lines</li>
<li>Bold blue line</li>
</ul>
<hr>
<div class="cell">
<div class="cell-output-display">
<p><img src="ELSfig71.png" class="img-fluid" width="452"></p>
</div>
</div>
<hr>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>(from Figure 7.1)</p>
<p>The training error <span class="math inline">\(\overline{\text{err}}\)</span> is not a good estimate for the <span class="math inline">\(\text{Err}_{\cal T}\)</span> nor the <span class="math inline">\(\text{Err}\)</span>.</p>
<hr>
</section>
</section>
<section id="loss-function-and-training-error-for-classification" class="level2">
<h2 class="anchored" data-anchor-id="loss-function-and-training-error-for-classification">Loss function and training error for classification</h2>
<ul>
<li><span class="math inline">\(X \in \Re^p\)</span></li>
<li><span class="math inline">\(G \in {\cal G}=\{1,\ldots,K\}\)</span></li>
<li><span class="math inline">\(\hat{G}(X) \in {\cal G}=\{1,\ldots,K\}\)</span></li>
</ul>
<p>0-1 loss with <span class="math inline">\(\hat{G}(X)=\text{argmax}_k \hat{p}_k(X)\)</span> <span class="math display">\[L(G,\hat{G}(X))=I(G\neq \hat{G}(X))\]</span> <span class="math inline">\(-2\)</span>-loglikelihood loss (why <span class="math inline">\(-2\)</span>?): <span class="math display">\[ L(G,\hat{p}(X))=-2 \text{log} \hat{p}_G(X)\]</span></p>
<hr>
<p>Test error (only replace <span class="math inline">\(\hat{f}\)</span> with <span class="math inline">\(\hat{G}\)</span>): <span class="math display">\[ \text{Err}_{\cal T}=\text{E}[L(Y,\hat{G}(X))\mid {\cal T}]\]</span> <span class="math display">\[ \text{Err}=\text{E}[\text{E}[L(Y,\hat{G}(X))\mid {\cal T}]]=\text{E} [\text{Err}_{\cal T}]\]</span></p>
<p>Training error (for 0-1 loss) <span class="math display">\[\overline{\text{err}}=\frac{1}{N}\sum_{i=1}^N I(g_i\neq \hat{g}(x_i))\]</span> Training error (for <span class="math inline">\(-2\)</span>loglikelihood loss) <span class="math display">\[\overline{\text{err}}=-\frac{2}{N}\sum_{i=1}^N \text{log}\hat{p}_{g_i}(x_i)\]</span></p>
</section>
</section>
<section id="discussion-and-conclusions" class="level1">
<h1>Discussion and conclusions</h1>
<ul>
<li>What are key take home messages from today´s teaching session?</li>
<li>What do you plan to do before the next teaching session?</li>
<li>Feedback on today´s teaching session?</li>
</ul>
</section>
<section id="exercises" class="level1">
<h1>Exercises</h1>
<section id="what-are-the-most-important-results-from-the-statistical-decision-theoretic-framework" class="level2">
<h2 class="anchored" data-anchor-id="what-are-the-most-important-results-from-the-statistical-decision-theoretic-framework">What are the most important results from the “Statistical decision theoretic framework”?</h2>
<ul>
<li>What are results to remember for regression and for classification?</li>
<li>How would you use these results?</li>
</ul>
</section>
<section id="look-into-the-derivation-for-the-bias-and-variance" class="level2">
<h2 class="anchored" data-anchor-id="look-into-the-derivation-for-the-bias-and-variance">Look into the derivation for the bias and variance</h2>
<p>(no solutions posted)</p>
<ul>
<li>for <span class="math inline">\(k\)</span>NN in Equation 7.10 and</li>
<li>OLS in Equation 7.11 on pages 222-223 of ESL.</li>
</ul>
</section>
<section id="key-results-from-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="key-results-from-logistic-regression">Key results from logistic regression</h2>
<section id="a-what-are-the-three-components-of-a-generalized-linear-model" class="level4">
<h4 class="anchored" data-anchor-id="a-what-are-the-three-components-of-a-generalized-linear-model">a) What are the three components of a generalized linear model?</h4>
</section>
<section id="b-what-are-these-three-for-a-logistic-regression" class="level4">
<h4 class="anchored" data-anchor-id="b-what-are-these-three-for-a-logistic-regression">b) What are these three for a logistic regression?</h4>
</section>
<section id="c-parameter-estimation" class="level4">
<h4 class="anchored" data-anchor-id="c-parameter-estimation">c) Parameter estimation</h4>
<p>How are regression parameters estimated for the GLM, and for logistic regression in particular?</p>
<p>Does it matter if you use the observed or expected information matrix for logistic regression?</p>
</section>
<section id="d-asymptotic-distribution" class="level4">
<h4 class="anchored" data-anchor-id="d-asymptotic-distribution">d) Asymptotic distribution</h4>
<p>What is the asymptotic distribution of the estimator for the regression parameter <span class="math inline">\(\hat{\beta}\)</span>? How can that be used to construct confidence intervals or perform hypothesis tests?</p>
</section>
<section id="e-deviance" class="level4">
<h4 class="anchored" data-anchor-id="e-deviance">e) Deviance</h4>
<p>How is the deviance defined in general, and how is this done for the logistic regression?</p>
</section>
</section>
</section>
<section id="solutions-to-exercises" class="level1">
<h1>Solutions to exercises</h1>
<p>Please try yourself first, or take a small peek - and try some more - before fully reading the solutions. Report errors or improvements to <a href="mailto:Mette.Langaas@ntnu.no" class="email">Mette.Langaas@ntnu.no</a>.</p>
<section id="key-results-from-logistic-regression-1" class="level2">
<h2 class="anchored" data-anchor-id="key-results-from-logistic-regression-1">Key results from logistic regression</h2>
<section id="a-what-are-the-three-components-of-a-generalized-linear-model-1" class="level4">
<h4 class="anchored" data-anchor-id="a-what-are-the-three-components-of-a-generalized-linear-model-1">a) What are the three components of a generalized linear model?</h4>
<p><a href="https://www.math.ntnu.no/emner/TMA4315/2018h/5GLM.html">TMA4315 GLM, module 5</a></p>
</section>
<section id="b-what-are-these-three-for-a-logistic-regression-1" class="level4">
<h4 class="anchored" data-anchor-id="b-what-are-these-three-for-a-logistic-regression-1">b) What are these three for a logistic regression?</h4>
<p><a href="https://www.math.ntnu.no/emner/TMA4315/2018h/3BinReg.html#how_to_model_a_binary_response">Binary response from TMA4315 Module 3</a></p>
</section>
<section id="c-parameter-estimation-1" class="level4">
<h4 class="anchored" data-anchor-id="c-parameter-estimation-1">c) Parameter estimation</h4>
<p><a href="https://www.math.ntnu.no/emner/TMA4315/2018h/2MLR.html#restricted_maximum_likelihood_estimator_for_(%7Bbf_sigma%7D%5E2)">TMA4315 GLM Module 2</a></p>
</section>
<section id="d-asymptotic-distribution-1" class="level4">
<h4 class="anchored" data-anchor-id="d-asymptotic-distribution-1">d) Asymptotic distribution</h4>
</section>
<section id="e-deviance-1" class="level4">
<h4 class="anchored" data-anchor-id="e-deviance-1">e) Deviance</h4>
<p><a href="https://www.math.ntnu.no/emner/TMA4315/2018h/3BinReg.html#grouped_vs_individual_data">Grouped vs individual data - and deviance</a></p>
</section>
</section>
</section>
<section id="reference-links" class="level1">
<h1>Reference links</h1>
<ul>
<li><p><a href="https://hastie.su.domains/ElemStatLearn/">ESL official errata:</a> and choose “Errata” in the left menu</p></li>
<li><p><a href="https://waxworksmath.com/Authors/G_M/Hastie/hastie.html">ESL solutions to exercises</a></p></li>
<li><p><a href="https://www.uio.no/studier/emner/matnat/math/STK-IN4300/h20/exercises.html">ESL solutions from UiO</a></p></li>
<li><p><a href="https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf">CASI Computer Age Statistical Inference, Efron and Hastie (2017). Chapter 12: Cross-Validation and <span class="math inline">\(C_p\)</span> Estimates of Prediction Error</a></p></li>
<li><p><a href="https://link.springer.com/chapter/10.1007/978-0-387-22456-5_7">Burnham and Andersen (2002): Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach. Springer. Chapter 7: Statistical Theory and Numerical Results</a></p></li>
</ul>
</section>
<section id="bibliography" class="level1">
<h1>Bibliography</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-casi" class="csl-entry" role="doc-biblioentry">
Efron, Bradley, and Trevor Hastie. 2016. <em>Computer Age Statistical Inference - Algorithms, Evidence, and Data Science</em>. Cambridge University Press. <a href="https://hastie.su.domains/CASI/">https://hastie.su.domains/CASI/</a>.
</div>
</div>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>