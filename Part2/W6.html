<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mette Langaas">
<meta name="dcterms.date" content="2023-02-12">

<title>MA8701 Advanced methods in statistical inference and learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="W6_files/libs/clipboard/clipboard.min.js"></script>
<script src="W6_files/libs/quarto-html/quarto.js"></script>
<script src="W6_files/libs/quarto-html/popper.min.js"></script>
<script src="W6_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="W6_files/libs/quarto-html/anchor.min.js"></script>
<link href="W6_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="W6_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="W6_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="W6_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="W6_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#before-we-begin" id="toc-before-we-begin" class="nav-link active" data-scroll-target="#before-we-begin">Before we begin</a>
  <ul class="collapse">
  <li><a href="#outline" id="toc-outline" class="nav-link" data-scroll-target="#outline">Outline</a></li>
  <li><a href="#literature" id="toc-literature" class="nav-link" data-scroll-target="#literature">Literature</a></li>
  </ul></li>
  <li><a href="#statistical-inference" id="toc-statistical-inference" class="nav-link" data-scroll-target="#statistical-inference">Statistical inference</a>
  <ul class="collapse">
  <li><a href="#prediction-vs-statistical-inference" id="toc-prediction-vs-statistical-inference" class="nav-link" data-scroll-target="#prediction-vs-statistical-inference">Prediction vs statistical inference</a>
  <ul class="collapse">
  <li><a href="#prediction" id="toc-prediction" class="nav-link" data-scroll-target="#prediction">Prediction</a></li>
  <li><a href="#inference" id="toc-inference" class="nav-link" data-scroll-target="#inference">Inference</a></li>
  </ul></li>
  <li><a href="#statistics-vs-machine-learning" id="toc-statistics-vs-machine-learning" class="nav-link" data-scroll-target="#statistics-vs-machine-learning">Statistics vs Machine learning</a></li>
  <li><a href="#known-sampling-distributions" id="toc-known-sampling-distributions" class="nav-link" data-scroll-target="#known-sampling-distributions">Known sampling distributions</a>
  <ul class="collapse">
  <li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link" data-scroll-target="#multiple-linear-regression">Multiple linear regression</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression">Logistic regression</a></li>
  </ul></li>
  <li><a href="#confidence-interval-generic-set-up" id="toc-confidence-interval-generic-set-up" class="nav-link" data-scroll-target="#confidence-interval-generic-set-up">Confidence interval — generic set-up</a></li>
  <li><a href="#sampling-distribution-for-ridge-and-lasso" id="toc-sampling-distribution-for-ridge-and-lasso" class="nav-link" data-scroll-target="#sampling-distribution-for-ridge-and-lasso">Sampling distribution for ridge and lasso?</a>
  <ul class="collapse">
  <li><a href="#multippel-linear-ridge-regression" id="toc-multippel-linear-ridge-regression" class="nav-link" data-scroll-target="#multippel-linear-ridge-regression">Multippel linear ridge regression</a></li>
  <li><a href="#logistic-ridge" id="toc-logistic-ridge" class="nav-link" data-scroll-target="#logistic-ridge">Logistic ridge</a></li>
  <li><a href="#lasso" id="toc-lasso" class="nav-link" data-scroll-target="#lasso">Lasso</a></li>
  <li><a href="#what-is-our-aim" id="toc-what-is-our-aim" class="nav-link" data-scroll-target="#what-is-our-aim">What is our aim?</a></li>
  <li><a href="#debiased-desparsified-lasso" id="toc-debiased-desparsified-lasso" class="nav-link" data-scroll-target="#debiased-desparsified-lasso">Debiased (desparsified) lasso</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#diabetes-data" id="toc-diabetes-data" class="nav-link" data-scroll-target="#diabetes-data">Diabetes data</a></li>
  </ul></li>
  <li><a href="#bayesian-ridge-and-lasso" id="toc-bayesian-ridge-and-lasso" class="nav-link" data-scroll-target="#bayesian-ridge-and-lasso">Bayesian ridge and lasso</a>
  <ul class="collapse">
  <li><a href="#bayesian-set-up" id="toc-bayesian-set-up" class="nav-link" data-scroll-target="#bayesian-set-up">Bayesian set-up</a></li>
  <li><a href="#prior-for-regression-parameters-ridge" id="toc-prior-for-regression-parameters-ridge" class="nav-link" data-scroll-target="#prior-for-regression-parameters-ridge">Prior for regression parameters (ridge)</a></li>
  <li><a href="#posterior-for-regression-parameters-ridge" id="toc-posterior-for-regression-parameters-ridge" class="nav-link" data-scroll-target="#posterior-for-regression-parameters-ridge">Posterior for regression parameters (ridge)</a></li>
  <li><a href="#prior-for-regression-parameters-lasso" id="toc-prior-for-regression-parameters-lasso" class="nav-link" data-scroll-target="#prior-for-regression-parameters-lasso">Prior for regression parameters (lasso)</a></li>
  <li><a href="#posterior-for-regression-parameters-lasso" id="toc-posterior-for-regression-parameters-lasso" class="nav-link" data-scroll-target="#posterior-for-regression-parameters-lasso">Posterior for regression parameters (lasso)</a></li>
  <li><a href="#not-only-the-point-estimate" id="toc-not-only-the-point-estimate" class="nav-link" data-scroll-target="#not-only-the-point-estimate">Not only the point estimate</a>
  <ul class="collapse">
  <li><a href="#diabetes-example-with-blasso" id="toc-diabetes-example-with-blasso" class="nav-link" data-scroll-target="#diabetes-example-with-blasso">Diabetes example with <code>blasso</code></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#bootstrap" id="toc-bootstrap" class="nav-link" data-scroll-target="#bootstrap">Bootstrap</a>
  <ul class="collapse">
  <li><a href="#procedure-to-find-lasso-estimate-hatbetahatlambda_cv" id="toc-procedure-to-find-lasso-estimate-hatbetahatlambda_cv" class="nav-link" data-scroll-target="#procedure-to-find-lasso-estimate-hatbetahatlambda_cv">Procedure to find lasso estimate <span class="math inline">\(\hat{\beta}(\hat{\lambda}_{CV})\)</span></a></li>
  <li><a href="#non-parametric-paired-bootstrap" id="toc-non-parametric-paired-bootstrap" class="nav-link" data-scroll-target="#non-parametric-paired-bootstrap">Non-parametric (paired) bootstrap</a>
  <ul class="collapse">
  <li><a href="#diabetes-example" id="toc-diabetes-example" class="nav-link" data-scroll-target="#diabetes-example">Diabetes example</a></li>
  </ul></li>
  <li><a href="#bootstrapping-vs-bayesian-lasso" id="toc-bootstrapping-vs-bayesian-lasso" class="nav-link" data-scroll-target="#bootstrapping-vs-bayesian-lasso">Bootstrapping vs Bayesian lasso</a></li>
  <li><a href="#bootstrapt-percentile-ci" id="toc-bootstrapt-percentile-ci" class="nav-link" data-scroll-target="#bootstrapt-percentile-ci">Bootstrapt percentile CI</a></li>
  <li><a href="#bootstrap-bca-ci" id="toc-bootstrap-bca-ci" class="nav-link" data-scroll-target="#bootstrap-bca-ci">Bootstrap BCa CI</a>
  <ul class="collapse">
  <li><a href="#diabetes-example-1" id="toc-diabetes-example-1" class="nav-link" data-scroll-target="#diabetes-example-1">Diabetes example</a></li>
  <li><a href="#medical-example" id="toc-medical-example" class="nav-link" data-scroll-target="#medical-example">Medical example</a></li>
  </ul></li>
  <li><a href="#bootstrap-cis-for-beta_j" id="toc-bootstrap-cis-for-beta_j" class="nav-link" data-scroll-target="#bootstrap-cis-for-beta_j">Bootstrap CIs for <span class="math inline">\(\beta_j\)</span></a></li>
  <li><a href="#outline-1" id="toc-outline-1" class="nav-link" data-scroll-target="#outline-1">Outline</a></li>
  </ul></li>
  <li><a href="#sample-splitting" id="toc-sample-splitting" class="nav-link" data-scroll-target="#sample-splitting">Sample splitting</a>
  <ul class="collapse">
  <li><a href="#what-if-we-just-split-the-data-in-two" id="toc-what-if-we-just-split-the-data-in-two" class="nav-link" data-scroll-target="#what-if-we-just-split-the-data-in-two">What if we just split the data in two?</a></li>
  <li><a href="#from-single-to-multiple-hypotheses" id="toc-from-single-to-multiple-hypotheses" class="nav-link" data-scroll-target="#from-single-to-multiple-hypotheses">From single to multiple hypotheses</a></li>
  <li><a href="#familywise-error-rate" id="toc-familywise-error-rate" class="nav-link" data-scroll-target="#familywise-error-rate">Familywise error rate</a>
  <ul class="collapse">
  <li><a href="#the-bonferroni-method-controls-the-fwer" id="toc-the-bonferroni-method-controls-the-fwer" class="nav-link" data-scroll-target="#the-bonferroni-method-controls-the-fwer">The Bonferroni method controls the FWER</a></li>
  </ul></li>
  <li><a href="#high-dimensional-inference" id="toc-high-dimensional-inference" class="nav-link" data-scroll-target="#high-dimensional-inference">High-dimensional inference</a></li>
  <li><a href="#single-sample-splitting" id="toc-single-sample-splitting" class="nav-link" data-scroll-target="#single-sample-splitting">Single-sample splitting</a></li>
  <li><a href="#multiple-sample-splitting" id="toc-multiple-sample-splitting" class="nav-link" data-scroll-target="#multiple-sample-splitting">Multiple-sample splitting</a>
  <ul class="collapse">
  <li><a href="#hdi-with-logistic-regression" id="toc-hdi-with-logistic-regression" class="nav-link" data-scroll-target="#hdi-with-logistic-regression">hdi with logistic regression</a></li>
  <li><a href="#hdi---also-with-other-solutions" id="toc-hdi---also-with-other-solutions" class="nav-link" data-scroll-target="#hdi---also-with-other-solutions">hdi - also with other solutions</a></li>
  <li><a href="#summing-up" id="toc-summing-up" class="nav-link" data-scroll-target="#summing-up">Summing up</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#inference-after-selection" id="toc-inference-after-selection" class="nav-link" data-scroll-target="#inference-after-selection">Inference after selection</a>
  <ul class="collapse">
  <li><a href="#the-plot" id="toc-the-plot" class="nav-link" data-scroll-target="#the-plot">The plot</a></li>
  <li><a href="#forward-stepwise-regression" id="toc-forward-stepwise-regression" class="nav-link" data-scroll-target="#forward-stepwise-regression">Forward stepwise regression</a></li>
  <li><a href="#the-polyhedral-result" id="toc-the-polyhedral-result" class="nav-link" data-scroll-target="#the-polyhedral-result">The polyhedral result</a></li>
  <li><a href="#polyhedral-lasso-result" id="toc-polyhedral-lasso-result" class="nav-link" data-scroll-target="#polyhedral-lasso-result">Polyhedral lasso result</a>
  <ul class="collapse">
  <li><a href="#further-improvements" id="toc-further-improvements" class="nav-link" data-scroll-target="#further-improvements">Further improvements</a></li>
  </ul></li>
  <li><a href="#posi" id="toc-posi" class="nav-link" data-scroll-target="#posi">PoSI</a>
  <ul class="collapse">
  <li><a href="#posi-r-package" id="toc-posi-r-package" class="nav-link" data-scroll-target="#posi-r-package">PoSI R-package</a></li>
  </ul></li>
  <li><a href="#post-selection-inference-and-the-reproducibility-crisis" id="toc-post-selection-inference-and-the-reproducibility-crisis" class="nav-link" data-scroll-target="#post-selection-inference-and-the-reproducibility-crisis">Post selection inference and the reproducibility crisis</a></li>
  </ul></li>
  <li><a href="#conclusion-1" id="toc-conclusion-1" class="nav-link" data-scroll-target="#conclusion-1">Conclusion</a>
  <ul class="collapse">
  <li><a href="#how-will-you-perform-inference" id="toc-how-will-you-perform-inference" class="nav-link" data-scroll-target="#how-will-you-perform-inference">How will you perform inference</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  <li><a href="#left-overs" id="toc-left-overs" class="nav-link" data-scroll-target="#left-overs">Left overs</a>
  <ul class="collapse">
  <li><a href="#bootstrap-confidence-intervals" id="toc-bootstrap-confidence-intervals" class="nav-link" data-scroll-target="#bootstrap-confidence-intervals">Bootstrap confidence intervals</a></li>
  <li><a href="#percentile-interval" id="toc-percentile-interval" class="nav-link" data-scroll-target="#percentile-interval">Percentile interval</a></li>
  <li><a href="#bias-corrected-accelerated-interval" id="toc-bias-corrected-accelerated-interval" class="nav-link" data-scroll-target="#bias-corrected-accelerated-interval">Bias corrected accelerated interval</a></li>
  <li><a href="#single-hypothesis-test" id="toc-single-hypothesis-test" class="nav-link" data-scroll-target="#single-hypothesis-test">Single hypothesis test</a></li>
  <li><a href="#p-value" id="toc-p-value" class="nav-link" data-scroll-target="#p-value"><span class="math inline">\(p\)</span>-value</a></li>
  <li><a href="#false-discovery-rate" id="toc-false-discovery-rate" class="nav-link" data-scroll-target="#false-discovery-rate">False discovery rate</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">MA8701 Advanced methods in statistical inference and learning</h1>
<p class="subtitle lead">W6: Statistical inference for penalized GLM methods</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mette Langaas </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 12, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<div class="cell">

</div>
<section id="before-we-begin" class="level1">
<h1>Before we begin</h1>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Prediction vs statistics inference: what are the aims?</li>
<li>Sampling distributions</li>
<li>Debiased lasso</li>
<li>Bayesian lasso</li>
<li>Boostrapping</li>
<li>Sample splitting</li>
<li>Inference after selection (forward regression example, polyhedral result, PoSI)</li>
<li>Reproducibility crisis and selective inference</li>
<li>Conclusions</li>
</ul>
<hr>
</section>
<section id="literature" class="level2">
<h2 class="anchored" data-anchor-id="literature">Literature</h2>
<p><strong>Main source:</strong></p>
<ul>
<li>[HTW] Hastie, Tibshirani, Wainwright: “Statistical Learning with Sparsity: The Lasso and Generalizations”. CRC press. <a href="https://trevorhastie.github.io/">Ebook</a>. Chapter 6.0, 6.1, 6.2, 6.4, 6.5. (Results from 6.3 through Taylor and Tibshirani (2015))</li>
</ul>
<p>Also: brush up on <strong>bootstrap intervals from TMA4300</strong>, where <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9781118555552">Givens and Hoeting (2013)</a> is on the reading list. See specifically chapter 9 (9.2.1 and 9.3 will be used here). NTNU-access to the full book if you are on vpn.</p>
<hr>
<p><strong>Secondary sources:</strong></p>
<ul>
<li><p><a href="https://www.math.ntnu.no/emner/TMA4267/2017v/multtest.pdf">Short note on multiple hypothesis testing</a> in TMA4267 Linear Statistical Models, Kari K. Halle, Øyvind Bakke and Mette Langaas, March 15, 2017.</p></li>
<li><p>Single/multi-sampling splitting part of <a href="https://projecteuclid.org/download/pdfview_1/euclid.ss/1449670857">Dezeure, Bühlmann, Meier, Meinshausen (2015)</a>. “High-Dimensional Inference: Confidence Intervals, p-Values and R-Software hdi”. Statistical Science, 2015, Vol. 30, No.&nbsp;4, 533–558 DOI: 10.1214/15-STS527 (only the single/multiple sample splitting part in 2.1.1 and 2.2 for linear regression, and using the method in practice). Or Chapter 11 of <a href="https://link.springer.com/book/10.1007/978-3-642-20192-9">Bühlmann and de Geer (2011)</a></p></li>
<li><p><a href="https://www.pnas.org/content/112/25/7629">Taylor and Tibshirani (2015)</a>: Statistical learning and selective inference, PNAS, vol 112, no 25, pages 7629-7634. (Soft version of HTW 6.3.2)</p></li>
</ul>
<hr>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./P2overview.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Overview of Part 2</figcaption><p></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
</section>
<section id="statistical-inference" class="level1">
<h1>Statistical inference</h1>
<p>We have now heard about the South African heart disease data, and we will also look at at a regression problem with prediction of disease progression in diabetes.</p>
<hr>
<section id="prediction-vs-statistical-inference" class="level2">
<h2 class="anchored" data-anchor-id="prediction-vs-statistical-inference">Prediction vs statistical inference</h2>
<section id="prediction" class="level3">
<h3 class="anchored" data-anchor-id="prediction">Prediction</h3>
<ul>
<li>Predict the value of the progression variable for a person with diabetes.</li>
<li>Predict the probability of heart disease for a person from the population in the South African heart disease example.</li>
</ul>
</section>
<section id="inference" class="level3">
<h3 class="anchored" data-anchor-id="inference">Inference</h3>
<ul>
<li>Assess the goodness of the prediction (MSE, error rate, ROC-AUC) - with uncertainty.</li>
<li>Interpret the GLM-model - which covariates are included?</li>
<li>Confidence interval for the model regression parameters.</li>
<li>Testing hypotheses about the model regression parameters.</li>
</ul>
<hr>
</section>
</section>
<section id="statistics-vs-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="statistics-vs-machine-learning">Statistics vs Machine learning</h2>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./StatsvsMLPredInf.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figures redrawn from Robert Tibshiran´s Breiman lecture at the NIPS 2015 <a href="https://www.youtube.com/watch?v=RKQJEvc02hc&amp;t=81s" class="uri">https://www.youtube.com/watch?v=RKQJEvc02hc&amp;t=81s</a>. (Conference on Neural Information Processing System)</figcaption><p></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
<section id="known-sampling-distributions" class="level2">
<h2 class="anchored" data-anchor-id="known-sampling-distributions">Known sampling distributions</h2>
<p>For the linear regression and logistic regression we know the sampling distribution of the regression coefficient estimators.</p>
<p>Then it is easy to construct confidence intervals and perform hypothesis tests.</p>
<p>What are the known results?</p>
<hr>
<section id="multiple-linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="multiple-linear-regression">Multiple linear regression</h3>
<p><span class="math display">\[{\boldsymbol Y=X \boldsymbol{\beta}}+{\boldsymbol \varepsilon}\]</span> where <span class="math inline">\(\varepsilon\sim N_N({\boldsymbol 0},\sigma^2{\boldsymbol I})\)</span> (independent observation pairs).</p>
<p><span class="math display">\[\hat{\beta}_{\text{LS}}=({\boldsymbol X}^T{\boldsymbol X})^{-1} {\boldsymbol X}^T {\boldsymbol Y}\]</span> with <span class="math inline">\(\hat{\beta}_{\text LS}\sim N_{p}(\beta,\sigma^2({\boldsymbol X}^T{\boldsymbol X})^{-1})\)</span>.</p>
<p>Restricted maximum likelihood estimator for <span class="math inline">\(\sigma^2\)</span>: <span class="math display">\[\hat{\sigma}^2=\frac{1}{N-p}({\boldsymbol Y}-{\boldsymbol X}\hat{\beta}_{\text LS})^T({\boldsymbol Y}-{\boldsymbol X}\hat{\beta}_{\text LS})=\frac{\text{SSE}}{N-p}\]</span> with <span class="math inline">\(\frac{(N-p)\hat{\sigma}^2}{\sigma^2} \sim \chi^2_{N-p}\)</span>.</p>
<hr>
<p>Statistic for inference about <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(c_{jj}\)</span> is diagonal element <span class="math inline">\(j\)</span> of <span class="math inline">\(({\boldsymbol X}^T{\boldsymbol X})^{-1}\)</span>. <span class="math display">\[T_j=\frac{\hat{\beta}_{\text LS,j}-\beta_j}{\sqrt{c_{jj}}\hat{\sigma}}\sim t_{N-p}\]</span></p>
<p>or, inference can be done asymptotically and then replace the <span class="math inline">\(t\)</span> with the normal distribution.</p>
<p><span class="math inline">\(T_j\)</span> is the starting point for constructing CIs for <span class="math inline">\(\beta_j\)</span> and testing hypotheses about <span class="math inline">\(\beta_j\)</span>.</p>
<p>Observe: the least squares estimator is <em>unbiased</em>!</p>
<hr>
</section>
<section id="logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression">Logistic regression</h3>
<p><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is now not on closed form, but asymptotically when <span class="math inline">\(N \rightarrow \infty\)</span> <span class="math display">\[\hat{\boldsymbol\beta} \approx N_{p}(\boldsymbol\beta,(\boldsymbol X^T\hat{\boldsymbol{W}}\boldsymbol X)^{-1})\]</span> where <span class="math inline">\(\boldsymbol W=diag(\hat{\pi}_i(1-\hat{\pi}_i))\)</span>, so that inference can be based on the asymptotic normality of each element of the regression estimate vector.</p>
<p>Observe: the logistic regression parameter estimator is <em>unbiased</em>.</p>
<hr>
</section>
</section>
<section id="confidence-interval-generic-set-up" class="level2">
<h2 class="anchored" data-anchor-id="confidence-interval-generic-set-up">Confidence interval — generic set-up</h2>
<p><strong>Set-up</strong></p>
<ul>
<li>We have a random sample <span class="math inline">\(Y_1,Y_2,\ldots,Y_N\)</span> from</li>
<li>some distribution <span class="math inline">\(F\)</span> with some (unkonwn) parameter <span class="math inline">\(\theta\)</span>.</li>
<li>Let <span class="math inline">\(y_1,y_2,\ldots,y_N\)</span> be the observed values for the random sample.</li>
</ul>
<p><strong>Statistics</strong></p>
<ul>
<li>We have two statistics <span class="math inline">\(\hat{\theta}_L(Y_1,Y_2,\ldots,Y_N)\)</span> and <span class="math inline">\(\hat{\theta}_U(Y_1,Y_2,\ldots,Y_N)\)</span> so that</li>
</ul>
<p><span class="math display">\[P(\hat{\theta}_L(Y_1,Y_2,\ldots,Y_N)\le \theta \le \hat{\theta}_U(Y_1,Y_2,\ldots,Y_N))=1-\alpha\]</span> where <span class="math inline">\(\alpha\in [0,1]\)</span></p>
<p><strong>Confidence interval</strong></p>
<p>The numerical interval <span class="math display">\[[\hat{\theta}_L(y_1,y_2,\ldots,y_N),\hat{\theta}_U(y_1,y_2,\ldots,y_N)]\]</span> is called a <span class="math inline">\((1-\alpha)\)</span> 100% confidence interval.</p>
<hr>
</section>
<section id="sampling-distribution-for-ridge-and-lasso" class="level2">
<h2 class="anchored" data-anchor-id="sampling-distribution-for-ridge-and-lasso">Sampling distribution for ridge and lasso?</h2>
<section id="multippel-linear-ridge-regression" class="level3">
<h3 class="anchored" data-anchor-id="multippel-linear-ridge-regression">Multippel linear ridge regression</h3>
<p><span class="math display">\[ \hat{\beta}_{\text{ridge}}=({\boldsymbol X}^T{\boldsymbol X}+\lambda {\boldsymbol I})^{-1} {\boldsymbol X}^T {\boldsymbol Y}\]</span></p>
<p><span class="math display">\[\hat{\beta}(\lambda)_{\text{ridge}} \sim N \{ (\boldsymbol{X}^T \boldsymbol{X} + \lambda \boldsymbol{I}_{p})^{-1} \boldsymbol{X}^T \boldsymbol{X} \, \beta,\]</span></p>
<p><span class="math display">\[\sigma^2 ( \boldsymbol{X}^T \boldsymbol{X} + \lambda \boldsymbol{I}_{p} )^{-1}  \boldsymbol{X}^T \boldsymbol{X} ( \boldsymbol{X}^T \boldsymbol{X} + \lambda \boldsymbol{I}_{p} )^{-1}  \}.\]</span></p>
<p><span class="math display">\[\text{df}(\lambda)=\text{tr}({\boldsymbol H}_{\lambda})=\text{tr}({\boldsymbol X}({\boldsymbol X}^T{\boldsymbol X}+ \lambda {\boldsymbol I})^{-1}{\boldsymbol X}^T)=\cdots=\sum_{j=1}^p \frac{d_j^2}{d_j^2+\lambda}\]</span></p>
<ul>
<li>What can we do with that?</li>
<li>What if the design matrix is orthogonal, does that help?</li>
</ul>
<hr>
</section>
<section id="logistic-ridge" class="level3">
<h3 class="anchored" data-anchor-id="logistic-ridge">Logistic ridge</h3>
<p>For large sample sizes the ridge logistic regression estimator is approximately multivariate normal (<span class="citation" data-cites="WNvW">Wieringen (<a href="#ref-WNvW" role="doc-biblioref">2021</a>)</span> Section 5.3).</p>
<p><span class="math display">\[\hat{\boldsymbol\beta}(\lambda) \approx N_{p}(\boldsymbol\beta-\lambda (\boldsymbol X^T\hat{\boldsymbol{W}}\boldsymbol X+\lambda \boldsymbol I)^{-1}\boldsymbol \beta),\]</span> <span class="math display">\[(\boldsymbol X^T\hat{\boldsymbol{W}}\boldsymbol X+\lambda \boldsymbol I)^{-1}-(\boldsymbol X^T\hat{\boldsymbol{W}}\boldsymbol X+\lambda \boldsymbol I)^{-2})\]</span> This is based on the asymptotic normality of the score function (gradient of the loglikelihood - here the penalized loglikelihood).</p>
<ul>
<li>What can we do with that?</li>
<li>What if the design matrix is orthogonal, does that help?</li>
</ul>
<hr>
</section>
<section id="lasso" class="level3">
<h3 class="anchored" data-anchor-id="lasso">Lasso</h3>
<p>Some results using approximations to ridge (for mean and variance, see <span class="citation" data-cites="WNvW">Wieringen (<a href="#ref-WNvW" role="doc-biblioref">2021</a>)</span> p 97), but else <em>no parametric version of sampling distribution</em> known.</p>
<hr>
<!-- ## Confidence interval — generic set-up -->
<!-- **Set-up** -->
<!-- * We have a random sample $Y_1,Y_2,\ldots,Y_N$ from  -->
<!-- * some distribution $F$ with some (unkonwn) parameter $\theta$.  -->
<!-- * Let $y_1,y_2,\ldots,y_N$ be the observed values for the random sample. -->
<!-- **Statistics** -->
<!-- * We have two statistics $\hat{\theta}_L(Y_1,Y_2,\ldots,Y_N)$ and $\hat{\theta}_U(Y_1,Y_2,\ldots,Y_N)$ so that  -->
<!-- $$P(\hat{\theta}_L(Y_1,Y_2,\ldots,Y_N)\le \theta \le \hat{\theta}_U(Y_1,Y_2,\ldots,Y_N))=1-\alpha$$ -->
<!-- where $\alpha\in [0,1]$ -->
<!-- **Confidence interval** -->
<!-- The numerical interval  -->
<!-- $$[\hat{\theta}_L(y_1,y_2,\ldots,y_N),\hat{\theta}_U(y_1,y_2,\ldots,y_N)]$$ -->
<!-- is called a $(1-\alpha)$ 100% confidence interval. -->
<hr>
</section>
<section id="what-is-our-aim" class="level3">
<h3 class="anchored" data-anchor-id="what-is-our-aim">What is our aim?</h3>
<p>Penalized estimation: reduce variance by introducing (strong) bias.</p>
<p>The squared bias then is a major part of the mean squared error, and the variance is thus a minor part.</p>
<p>But, do we need to use the ridge or lasso estimator to construct a confidence interval for <span class="math inline">\(\boldsymbol \beta\)</span> or test if <span class="math inline">\(\beta_j=0\)</span>?</p>
<ul>
<li><span class="math inline">\(p&gt;N\)</span></li>
<li><span class="math inline">\(p&lt;N\)</span>?</li>
</ul>
<hr>
</section>
<section id="debiased-desparsified-lasso" class="level3">
<h3 class="anchored" data-anchor-id="debiased-desparsified-lasso">Debiased (desparsified) lasso</h3>
<p>(HTW Section 6.4)</p>
<p><span class="math display">\[\hat{\beta}^d=\hat{\beta}_{\lambda}+\frac{1}{N}{\boldsymbol M}{\boldsymbol X}^T(\boldsymbol Y-\boldsymbol X \hat{\beta}_{\lambda})\]</span></p>
<p>the matrix <span class="math inline">\({\boldsymbol M}\)</span> is some approximation to the inverse of <span class="math display">\[\hat{\boldsymbol \Sigma}=\frac{1}{N}{\boldsymbol X}^T{\boldsymbol X}\]</span> Use the debiased estimator to form CI from:</p>
<p><span class="math display">\[\hat{\beta}^d \sim N(\beta,\frac{\sigma^2}{N}{\boldsymbol M}\hat{\boldsymbol \Sigma}{\boldsymbol M}^T)\]</span></p>
<p>Interpretation of debiasing: assume we want to minimize the residual sum of squares using an approximate Newton step starting at the lasso estimator.</p>
<hr>
<p>Two solutions are based on</p>
<ul>
<li>neighbour-based methods to impose sparsity</li>
<li>optimization problem to get <span class="math inline">\(\hat{\boldsymbol \Sigma}\hat{\boldsymbol M}\approx \boldsymbol I\)</span> while the variance of the debiased estimator is small.</li>
</ul>
<p>See <span class="citation" data-cites="HTW">Hastie, Tibshirani, and Wainwright (<a href="#ref-HTW" role="doc-biblioref">2015</a>)</span> page 159 for references to these solutions.</p>
<hr>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./HTWFig613.jpg" class="img-fluid figure-img" style="width:90.0%"></p>
<p></p><figcaption class="figure-caption">Figure 6.13 in <span class="citation" data-cites="HTW">Hastie, Tibshirani, and Wainwright (<a href="#ref-HTW" role="doc-biblioref">2015</a>)</span></figcaption><p></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This is absolutely not straightforward.</p>
<p>The <em>adaptive and biased nature</em> of the estimation procedures makes it challenging to perform inference.</p>
<ul>
<li>We will <em>discuss</em> other (in addition to the debiasing) solutions to finding confidence intervals for regression parameters for lasso, and for constructing <span class="math inline">\(p\)</span>-values for testing hypotheses about the regression parameters.</li>
<li>We will address some <em>philosophical principles behind inference</em></li>
<li>and mention topics that can be studied further for the interested student!</li>
</ul>
<p>Warning: there seems not to be consensus, but many interesting approaches and ideas that we may consider.</p>
<hr>
</section>
<section id="diabetes-data" class="level2">
<h2 class="anchored" data-anchor-id="diabetes-data">Diabetes data</h2>
<p>In a medical study the aim was to explain the ethiology of diabetes progression. Data was collected from <span class="math inline">\(n=442\)</span> diabetes patients, and from each patient the following measurements are available:</p>
<ul>
<li><code>age</code> (in years) at baseline</li>
<li><code>sex</code> (0=female and 1=male) at baseline</li>
<li>body mass index (<code>bmi</code>) at baseline</li>
<li>mean arterial blood pressure (<code>map</code>) at baseline</li>
<li>six blood serum measurements: total cholesterol (<code>tc</code>), ldl cholesterol (<code>ldl</code>), hdl cholesterol (<code>hdl</code>), <code>tch</code>, <code>ltg</code>, glucose <code>glu</code>, all at baseline,</li>
<li>a quantitative measurement of disease progression one year after baseline (<code>prog</code>)</li>
</ul>
<p>All measurements except <code>sex</code> are continuous. There are 10 covariates.</p>
<p>The response is the disease progression <code>prog</code> - thus a regression problem.</p>
<hr>
<p>Data can be</p>
<ul>
<li>downloaded from <a href="https://web.stanford.edu/~hastie/StatLearnSparsity_files/DATA/diabetes.html" class="uri">https://web.stanford.edu/~hastie/StatLearnSparsity_files/DATA/diabetes.html</a> in three variants: raw, standardized and 442 <span class="math inline">\(\times\)</span> 64 matrix with quadratic terms (not used here).</li>
<li>Or, loaded from the <code>lars</code> package, that is automatically loaded in the <code>monomvn</code> package (where <code>blasso</code> is found).</li>
</ul>
<hr>
<div class="cell">
<div class="cell-output-display">
<p><img src="W6_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="1152"></p>
</div>
</div>
</section>
</section>
<section id="bayesian-ridge-and-lasso" class="level1">
<h1>Bayesian ridge and lasso</h1>
<p>(HTW 6.1 for lasso, WNvW Section 5.5 and 6.6)</p>
<p>For penalized models there exists Bayesian equivalents. We will here focus on the multiple linear regression model.</p>
<hr>
<section id="bayesian-set-up" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-set-up">Bayesian set-up</h2>
<p>In the Bayesian statistics the regression parameters <span class="math inline">\(\beta\)</span> are random quantities, and in addition to the likelihood also a prior for the regression parameters (and other parameters) are needed. When a <em>conjugate prior</em> the posterior distribution may be derived analytically.</p>
<p>Multiple linear regression: distribution of response - where we for simplicity assume that we have centred covariates and centred response (so no intercept term)</p>
<p><span class="math display">\[ {\boldsymbol y}\mid \beta, \sigma \sim N({\boldsymbol X}\beta,\sigma^2 {\boldsymbol I})\]</span> This gives the likelihood:</p>
<p><span class="math display">\[ L(\boldsymbol \beta \mid \boldsymbol y,\boldsymbol X, \sigma) \propto
(\sigma^{-N/2}) \exp[- \frac{1}{2 \sigma^2}(\boldsymbol y -\boldsymbol X \boldsymbol \beta)^T(\boldsymbol y -\boldsymbol X \boldsymbol \beta)]\]</span></p>
<hr>
</section>
<section id="prior-for-regression-parameters-ridge" class="level2">
<h2 class="anchored" data-anchor-id="prior-for-regression-parameters-ridge">Prior for regression parameters (ridge)</h2>
<p>In Part 1 we worked with multiple imputation and one method for drawing observations was the Bayesian linear regression.</p>
<p>We will use the same priors here, for the <span class="math inline">\(\sigma^2\)</span> we use a inverse Gamma prior. For the regression coefficents a normal prior is used.</p>
<p><span class="math display">\[\beta \mid \sigma \sim \prod_{j=1}^p \sqrt{\frac{\lambda}{2 \sigma^2}}\exp(-\frac{\lambda}{2 \sigma^2}\beta_j^2)\]</span></p>
<hr>
<div class="cell">
<div class="cell-output-display">
<p><img src="W6_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="1152"></p>
</div>
</div>
<hr>
</section>
<section id="posterior-for-regression-parameters-ridge" class="level2">
<h2 class="anchored" data-anchor-id="posterior-for-regression-parameters-ridge">Posterior for regression parameters (ridge)</h2>
<p><span class="math display">\[ \beta, \mid \boldsymbol X, \boldsymbol Y \sigma^2 \propto \exp[-\frac{1}{2\sigma^2}](\boldsymbol \beta-\hat{\boldsymbol \beta}({\lambda}))^T ({\boldsymbol X}^T \boldsymbol X+\lambda \boldsymbol I)(\boldsymbol \beta-\hat{\boldsymbol \beta}({\lambda}))]\]</span> The posterior mean is the ridge estimator <span class="math inline">\(\hat{\boldsymbol \beta}(\lambda)\)</span>.</p>
<hr>
</section>
<section id="prior-for-regression-parameters-lasso" class="level2">
<h2 class="anchored" data-anchor-id="prior-for-regression-parameters-lasso">Prior for regression parameters (lasso)</h2>
<p><span class="math display">\[\beta \mid \lambda, \sigma \sim \prod_{j=1}^p \frac{\lambda}{2 \sigma}\exp(-\frac{\lambda}{\sigma}\lvert \beta_j \rvert)\]</span></p>
<p>This prior is called an i.i.d. <em>Laplacian</em> (or double exponential) prior.</p>
<hr>
<div class="cell">
<div class="cell-output-display">
<p><img src="W6_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="1152"></p>
</div>
</div>
<hr>
</section>
<section id="posterior-for-regression-parameters-lasso" class="level2">
<h2 class="anchored" data-anchor-id="posterior-for-regression-parameters-lasso">Posterior for regression parameters (lasso)</h2>
<p>It can be shown that the negative log of the posterior density for <span class="math inline">\(\beta \mid {\boldsymbol y}, \lambda, \sigma\)</span> is (up to an additive constant)</p>
<p><span class="math display">\[\frac{1}{2\sigma^2} \Vert {\boldsymbol y}-{\boldsymbol X}\beta\Vert_2^2 +\frac{\lambda}{\sigma} \Vert \beta \Vert_1\]</span></p>
<p>Does this look familiar?</p>
<hr>
<p>For a fixed value of <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\lambda\)</span> - the <span class="math inline">\(\beta\)</span> giving the minimum of the negative log posterior is the <em>lasso</em> estimate where the regularization parameter is <span class="math inline">\(\sigma \lambda\)</span>.</p>
<p>The minimum negative log posterior will then be the same as the maximum log posterior - and the maximum of a distribution is called the <em>mode</em> of the distribution.</p>
<p>The lasso estimate is the <em>posterior mode</em> in the Bayesian model.</p>
<hr>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./HTWFig61.jpg" class="img-fluid figure-img" style="width:90.0%"></p>
<p></p><figcaption class="figure-caption">Figure 6.1 in <span class="citation" data-cites="HTW">Hastie, Tibshirani, and Wainwright (<a href="#ref-HTW" role="doc-biblioref">2015</a>)</span></figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>From <span class="citation" data-cites="HTW">Hastie, Tibshirani, and Wainwright (<a href="#ref-HTW" role="doc-biblioref">2015</a>)</span>: a 95% posterior credibility interval covers zero.</p>
<!-- (Study Figure 6.1 in HTW: describe what we see.) -->
<hr>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./HTWFig62.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figure 6.2 in <span class="citation" data-cites="HTW">Hastie, Tibshirani, and Wainwright (<a href="#ref-HTW" role="doc-biblioref">2015</a>)</span></figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><span class="citation" data-cites="HTW">Hastie, Tibshirani, and Wainwright (<a href="#ref-HTW" role="doc-biblioref">2015</a>)</span>: MCMC with 10 000 samples.</p>
<!-- (Study Figure 6.2 in HTW: compares lasso path with Bayesian lasso with different $\lambda$s.) -->
<hr>
<p>A full Bayesian approach requires priors for <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\sigma\)</span>, in addition to priors on the regression coefficient.</p>
<p>Markov Chain Monte Carlo MCMC is used efficiently sample realizations form the posterior distribution.</p>
<p>See <span class="citation" data-cites="WNvW">Wieringen (<a href="#ref-WNvW" role="doc-biblioref">2021</a>)</span> Chapter 2 for more on Bayesian regression and the connection to the ridge and Section 6.6 for connection to lasso.</p>
<hr>
</section>
<section id="not-only-the-point-estimate" class="level2">
<h2 class="anchored" data-anchor-id="not-only-the-point-estimate">Not only the point estimate</h2>
<p>The posterior distribution gives the</p>
<ul>
<li>point estimates for the lasso (the mode of the distribution)</li>
</ul>
<p>but</p>
<ul>
<li>also the <em>entire joint distribution</em>.</li>
</ul>
<hr>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./HTWFig63.jpg" class="img-fluid figure-img" style="width:90.0%"></p>
<p></p><figcaption class="figure-caption">Figure 6.3 in <span class="citation" data-cites="HTW">Hastie, Tibshirani, and Wainwright (<a href="#ref-HTW" role="doc-biblioref">2015</a>)</span></figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><span class="citation" data-cites="HTW">Hastie, Tibshirani, and Wainwright (<a href="#ref-HTW" role="doc-biblioref">2015</a>)</span>: 10 000 samples from the posterior.</p>
<!-- (Study Figure 6.3 in HTW for boxplots and marginal density one regression parameter - based on a _sample_ from the posterior distribution.) -->
<hr>
<section id="diabetes-example-with-blasso" class="level3">
<h3 class="anchored" data-anchor-id="diabetes-example-with-blasso">Diabetes example with <code>blasso</code></h3>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="do">## code below copied from the help(blasso)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="do">## following the lars diabetes example</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(diabetes)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(diabetes)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Ordinary Least Squares regression</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>reg.ols <span class="ot">&lt;-</span> <span class="fu">regress</span>(x, y)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Lasso regression</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>reg.las <span class="ot">&lt;-</span> <span class="fu">regress</span>(x, y, <span class="at">method=</span><span class="st">"lasso"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Bayesian Lasso regression</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>reg.blas <span class="ot">&lt;-</span> <span class="fu">blasso</span>(x, y,<span class="at">verb=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<hr>
<div class="cell">
<div class="cell-output-display">
<p><img src="W6_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid" width="1152"></p>
</div>
</div>
<hr>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="do">## plot the size of different models visited</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">#plot(reg.blas, burnin=200, which="m")</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="do">## get the summary</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="fu">summary</span>(reg.blas, <span class="at">burnin=</span><span class="dv">200</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<hr>
<div class="cell">
<div class="cell-output-display">
<p><img src="W6_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid" width="1152"></p>
</div>
</div>
<hr>
<div class="cell">

</div>
<div class="cell">

</div>
</section>
</section>
</section>
<section id="bootstrap" class="level1">
<h1>Bootstrap</h1>
<p>(HTW 6.2)</p>
<hr>
<section id="procedure-to-find-lasso-estimate-hatbetahatlambda_cv" class="level2">
<h2 class="anchored" data-anchor-id="procedure-to-find-lasso-estimate-hatbetahatlambda_cv">Procedure to find lasso estimate <span class="math inline">\(\hat{\beta}(\hat{\lambda}_{CV})\)</span></h2>
<p>(Copied word by word from HTW page 142)</p>
<p>Refer to these 6 steps as <span class="math inline">\(\hat{\beta}(\hat{\lambda}_{CV})\)</span>-loop</p>
<ol type="1">
<li>Fit a lasso path to <span class="math inline">\((X, y)\)</span> over a dense grid of values <span class="math inline">\(\Lambda=\{\lambda_l\}_{l=1}^{L}\)</span>.</li>
<li>Divide the training samples into 10 groups at random.</li>
<li>With the <span class="math inline">\(k\)</span>th group left out, fit a lasso path to the remaining <span class="math inline">\(9/10\)</span>ths, using the same grid <span class="math inline">\(\Lambda\)</span>.</li>
<li>For each <span class="math inline">\(\lambda \in \Lambda\)</span> compute the mean-squared prediction error for the left-out group.</li>
<li>Average these errors to obtain a prediction error curve over the grid <span class="math inline">\(\Lambda\)</span>.</li>
<li>Find the value <span class="math inline">\(\hat{\beta}(\hat{\lambda}_{CV})\)</span> that minimizes this curve, and then return the coefficient vector from our original fit in step (1) at that value of <span class="math inline">\(\lambda\)</span>.</li>
</ol>
<hr>
<p><strong>Observe:</strong></p>
<ul>
<li><span class="math inline">\(\lambda\)</span>-path is the same for each run of the lasso</li>
<li>the chosen <span class="math inline">\(\lambda\)</span> is then used on the orginal data</li>
</ul>
<p><strong>Q:</strong> Is it possible to use resampling to estimate the distribution of the lasso <span class="math inline">\(\hat{\beta}\)</span> estimator including the model selection (choosing <span class="math inline">\(\lambda\)</span>)?</p>
<hr>
</section>
<section id="non-parametric-paired-bootstrap" class="level2">
<h2 class="anchored" data-anchor-id="non-parametric-paired-bootstrap">Non-parametric (paired) bootstrap</h2>
<ul>
<li>Let <span class="math inline">\(F\)</span> denote the joint distribution of <span class="math inline">\((X,Y)\)</span>.</li>
<li>The empirical <span class="math inline">\(\hat{F}\)</span> is <span class="math inline">\(\frac{1}{N}\)</span> for each observation <span class="math inline">\((X,Y)\)</span> in our training data <span class="math inline">\((X_i,Y_i)\)</span>, <span class="math inline">\(i=1,\ldots,N\)</span>.</li>
<li>Drawing from <span class="math inline">\(\hat{F}\)</span> is the same as drawing from the <span class="math inline">\(N\)</span> observations in the training data with replacement.</li>
</ul>
<p>Now, we draw <span class="math inline">\(B\)</span> bootstrap samples from the training data, and for each new bootstrap sample we run through the 6 steps in the <span class="math inline">\(\hat{\beta}(\hat{\lambda}_{CV})\)</span>-loop.</p>
<ul>
<li>The result is <span class="math inline">\(B\)</span> vectors <span class="math inline">\(\hat{\beta}(\hat{\lambda}_{CV})\)</span>.</li>
<li>We plot the result as
<ul>
<li>boxplots,</li>
<li>proportion of times each element of <span class="math inline">\(\hat{\beta}(\hat{\lambda}_{CV})\)</span> is equal 0.</li>
</ul></li>
</ul>
<hr>
<section id="diabetes-example" class="level3">
<h3 class="anchored" data-anchor-id="diabetes-example">Diabetes example</h3>
<div class="cell">

</div>
<hr>
<div class="cell">
<div class="cell-output-display">
<p><img src="W6_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid" width="1152"></p>
</div>
</div>
<hr>
<div class="cell">
<div class="cell-output-display">
<p><img src="W6_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid" width="1152"></p>
</div>
</div>
<hr>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>11 x 1 sparse Matrix of class "dgCMatrix"
                    s1
(Intercept)  152.13348
age            .      
sex          -33.35229
bmi          508.13935
map          210.34606
tc             .      
ldl            .      
hdl         -138.84433
tch            .      
ltg          444.59064
glu            .      </code></pre>
</div>
</div>
<hr>
<div class="cell">

</div>
<div class="cell">

</div>
<hr>
<div class="cell">
<div class="cell-output-display">
<p><img src="W6_files/figure-html/unnamed-chunk-23-1.png" class="img-fluid" width="1152"></p>
</div>
</div>
<hr>
<div class="cell">

</div>
<hr>
<div class="cell">
<div class="cell-output-display">
<p><img src="W6_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid" width="1152"></p>
</div>
</div>
<hr>
<div class="cell">
<div class="cell-output-display">
<p><img src="W6_files/figure-html/unnamed-chunk-26-1.png" class="img-fluid" width="1152"></p>
</div>
</div>
<hr>
</section>
</section>
<section id="bootstrapping-vs-bayesian-lasso" class="level2">
<h2 class="anchored" data-anchor-id="bootstrapping-vs-bayesian-lasso">Bootstrapping vs Bayesian lasso</h2>
<p>The results from the Bayesian lasso on the proportion of times a coefficient is 0 and the boxplots are very similar to the results from the bootstrapping. The bootstrap seems to be doing the “same” as a Bayesian analysis with the Laplacian prior.</p>
<p>When the model is not so complex and the number of covariates is not too large (<span class="math inline">\(p\sim 100\)</span>) the Bayesian lasso might be as fast as the bootstrapping, but for larger problems the bootstrap “scales better”.</p>
<p>For GLMs the Bayesian solution is more demanding, but the bootstrap is as easy as for the linear model.</p>
<hr>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./HTWFig63.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figure 6.3 in <span class="citation" data-cites="HTW">Hastie, Tibshirani, and Wainwright (<a href="#ref-HTW" role="doc-biblioref">2015</a>)</span></figcaption><p></p>
</figure>
</div>
</div>
</div>
<hr>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./HTWFig64.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Figure 6.4 in <span class="citation" data-cites="HTW">Hastie, Tibshirani, and Wainwright (<a href="#ref-HTW" role="doc-biblioref">2015</a>)</span></figcaption><p></p>
</figure>
</div>
</div>
</div>
<!-- (Study and compare Figure 6.3 and 6.4 in HTW.) -->
<hr>
<div class="cell">

</div>
<hr>
</section>
<section id="bootstrapt-percentile-ci" class="level2">
<h2 class="anchored" data-anchor-id="bootstrapt-percentile-ci">Bootstrapt percentile CI</h2>
<p>To construct a <span class="math inline">\((1-\alpha)\cdot 100\)</span>% CI:</p>
<ul>
<li>order the bootstrap sample for the estimate of interest</li>
<li>read off the <span class="math inline">\((1-\alpha/2)\cdot 100\)</span> percetile</li>
<li><span class="math inline">\((\alpha/2)\cdot 100\)</span> percentile</li>
</ul>
<p>These are now the lower and upper limit of the CI.</p>
<hr>
</section>
<section id="bootstrap-bca-ci" class="level2">
<h2 class="anchored" data-anchor-id="bootstrap-bca-ci">Bootstrap BCa CI</h2>
<p>See page 34 of <a href="https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/3026839">Bootstrap confidence intervals in the master thesis of Lene Tillerli Omdal Section 3.6.2</a> and teaching material from TMA4300: <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9781118555552">Givens and Hoeting (2013)</a> chapter 9.3. NTNU-access to the full book if you are on vpn.</p>
<hr>
<section id="diabetes-example-1" class="level3">
<h3 class="anchored" data-anchor-id="diabetes-example-1">Diabetes example</h3>
<p>What if we calculated percentile bootstrap intervals - could we use that to say anything about the true underlying regression coefficients?</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Ridge first then lasso"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>               2.5%         50%      97.5%        2.5%       50%     97.5%
Int.cept  147.08515  152.140842  157.48081  146.942231 152.17445 157.29619
age       -30.29701   28.233633   84.43224    0.000000   0.00000   0.00000
sex      -170.89169  -92.265146  -26.22187 -115.349163   0.00000   0.00000
bmi       232.35050  321.111896  421.01718  329.357643 469.97818 617.77346
map       140.21620  207.461944  278.29632    0.000000 151.40541 304.09469
tc        -45.76264    2.724457   43.62582   -4.357653   0.00000   0.00000
ldl       -91.46718  -35.412937   15.64376  -22.951886   0.00000   0.00000
hdl      -206.89616 -157.552738 -109.32657 -218.711400 -80.79045   0.00000
tch        69.30617  119.322597  165.46347    0.000000   0.00000  51.93353
ltg       206.74238  276.466411  369.93742  283.107398 412.68911 556.78790
glu        61.07588  111.952224  168.07668    0.000000   0.00000  90.18665</code></pre>
</div>
</div>
<hr>
</section>
<section id="medical-example" class="level3">
<h3 class="anchored" data-anchor-id="medical-example">Medical example</h3>
<p>(See Figures from study in class notes.)</p>
<div class="cell">

</div>
<hr>
<div class="cell">

</div>
<hr>
<div class="cell">

</div>
<hr>
<div class="cell">

</div>
<hr>
</section>
</section>
<section id="bootstrap-cis-for-beta_j" class="level2">
<h2 class="anchored" data-anchor-id="bootstrap-cis-for-beta_j">Bootstrap CIs for <span class="math inline">\(\beta_j\)</span></h2>
<p>Sadly, there are two main challenges:</p>
<ul>
<li>The percentile interval is not a good choice for biased estimators, and it is not clear if the bias-corrected accelerated intervals are better</li>
<li>It has been shown that (for fixed <span class="math inline">\(p\)</span>) the asymptotic (<span class="math inline">\(N\rightarrow \infty\)</span>) distribution of the lasso has point mass at zero (which leads to that bootstrapping not having optimal properties).</li>
</ul>
<hr>
<p>The authors of the <em>penalized package</em> take the following view Section 6: A note on standard errors and confidence intervals in the Penalized user manual <a href="https://cran.r-project.org/web/packages/penalized/vignettes/penalized.pdf" class="uri">https://cran.r-project.org/web/packages/penalized/vignettes/penalized.pdf</a></p>
<p>“Unfortunately, in most applications of penalized regression it is impossible to obtain a suﬃciently precise estimate of the bias. Any bootstrap-based calculations can only give an assessment of the variance of the estimates. Reliable estimates of the bias are only available if reliable unbiased estimates are available, which is typically not the case in situations in which penalized estimates are used.”</p>
<p>“It is certainly a mistake to make conﬁdence statements that are only based on an assessment of the variance of the estimates, such as bootstrap-based conﬁdence intervals do.”</p>
<p>Reliable conﬁdence intervals around the penalized estimates can be obtained in the case of low dimensional models using the standard generalized linear model theory as implemented in lm, glm and coxph.”</p>
<!-- One possibility is the R-package [HDCI](https://cran.r-project.org/web/packages/HDCI/HDCI.pdf) -->
<!-- but the background manuscript is not published yet <https://arxiv.org/abs/1706.02150>. -->
<hr>
</section>
<section id="outline-1" class="level2">
<h2 class="anchored" data-anchor-id="outline-1">Outline</h2>
<ul>
<li>Prediction vs statistics inference: what are the aims?</li>
<li>Sampling distributions</li>
<li>Debiased lasso</li>
<li>Bayesian lasso</li>
<li>Boostrapping</li>
</ul>
<p><strong>WE are here now!</strong></p>
<ul>
<li>Sample splitting</li>
<li>Inference after selection (forward regression example, polyhedral result, PoSI)</li>
<li>Reproducibility crisis and selective inference</li>
<li>Conclusions</li>
</ul>
<hr>
</section>
</section>
<section id="sample-splitting" class="level1">
<h1>Sample splitting</h1>
<section id="what-if-we-just-split-the-data-in-two" class="level2">
<h2 class="anchored" data-anchor-id="what-if-we-just-split-the-data-in-two">What if we just split the data in two?</h2>
<p>Linear regression or logistic regression.</p>
<p>Dataset with <span class="math inline">\(p\)</span> covariates and <span class="math inline">\(N\)</span> observations. Divided into a training set of size <span class="math inline">\(aN\)</span> and a test set of <span class="math inline">\((1-a)N\)</span>, where <span class="math inline">\(a \in [0,1]\)</span>.</p>
<ul>
<li><p>Training data used to decide on <span class="math inline">\(\lambda\)</span> using CV - gives final model where some coefficients is set to 0 and some are shrunken. (The 6 steps.)</p></li>
<li><p>Test data:</p>
<ul>
<li>Fit ordinary LS or GLM model with only the non-zero lasso covariates</li>
<li>present CI and <span class="math inline">\(p\)</span>-values from LS</li>
</ul></li>
</ul>
<p><strong>Group discussion:</strong> Is this ok? What is gained and what is lost?</p>
<hr>
<!-- Only as hidden comments: -->
<!-- * We will not know anything about how often a coefficient is zero. -->
<!-- * It is only the selected model we study. -->
<!-- * The CI and p-value is for the true underlying parameters. -->

<!-- * Not suitable to assess the prediction since no shrinkage is included. -->
</section>
<section id="from-single-to-multiple-hypotheses" class="level2">
<h2 class="anchored" data-anchor-id="from-single-to-multiple-hypotheses">From single to multiple hypotheses</h2>
<p>In many situations we are not interested in testing only one hypothesis, but instead <span class="math inline">\(m\)</span> hypotheses.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Not reject <span class="math inline">\(H_0\)</span></th>
<th style="text-align: left;">Reject <span class="math inline">\(H_0\)</span></th>
<th style="text-align: left;">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(H_0\)</span> true</td>
<td style="text-align: left;"><span class="math inline">\(U\)</span></td>
<td style="text-align: left;"><span class="math inline">\(V\)</span></td>
<td style="text-align: left;"><span class="math inline">\(m_0\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(H_0\)</span> false</td>
<td style="text-align: left;"><span class="math inline">\(T\)</span></td>
<td style="text-align: left;"><span class="math inline">\(S\)</span></td>
<td style="text-align: left;"><span class="math inline">\(m - m_0\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total</td>
<td style="text-align: left;"><span class="math inline">\(m-R\)</span></td>
<td style="text-align: left;"><span class="math inline">\(R\)</span></td>
<td style="text-align: left;"><span class="math inline">\(m\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>Out of the <span class="math inline">\(m\)</span> hypotheses tested, the (unknown) number of true null hypotheses is <span class="math inline">\(m_0\)</span>.</li>
<li><span class="math inline">\(V\)</span>: the number of type I errors (false positive findings) and</li>
<li><span class="math inline">\(T\)</span>: the number of type II errors (false negative findings).</li>
<li><span class="math inline">\(U\)</span>: the number of true null hypotheses that are not rejected and</li>
<li><span class="math inline">\(S\)</span>: the number of false null hypotheses that are rejected.</li>
<li><span class="math inline">\(R\)</span>: the number of hypoteses rejected for a specific cut-off</li>
</ul>
<p>Observe: only <span class="math inline">\(m\)</span> and <span class="math inline">\(R\)</span> is observed!</p>
<hr>
</section>
<section id="familywise-error-rate" class="level2">
<h2 class="anchored" data-anchor-id="familywise-error-rate">Familywise error rate</h2>
<p>The familywise error rate (FWER) is defined as <em>the probability of one or more false positive findings</em></p>
<p><span class="math display">\[ \text{FWER} = P(V &gt; 0) \]</span> The number of false positive findings <span class="math inline">\(V\)</span> is not known in a real life situation, but still we may find a cut-off on the <span class="math inline">\(p\)</span>-value, called <span class="math inline">\(\alpha_{\text loc}\)</span>, that gives an upper limit to (controls) the FWER.</p>
<hr>
<ul>
<li>Raw <span class="math inline">\(p\)</span>-value, <span class="math inline">\(p_j\)</span>, the lowest nominal level to reject the null hypothesis.<br>
</li>
<li>Adjusted <span class="math inline">\(p\)</span>-value, <span class="math inline">\(\tilde{p}_j\)</span>, is the nominal level of the multiple (simultaneous) test procedure at which <span class="math inline">\(H_{0j}, j=1,\ldots,m\)</span> is just rejected, given the values of all test statistics involved.</li>
</ul>
<p>The adjusted <span class="math inline">\(p\)</span>-values can be defined as <span class="math display">\[\tilde{p}_j = \text{inf}\{\alpha  \mid H_{0j}\text{ is rejected at FWER level } \alpha \}\]</span></p>
<p>In a multiple testing problem where all adjusted <span class="math inline">\(p\)</span>-value below <span class="math inline">\(\alpha\)</span> are rejected, the overall type I error rate (for example FWER) will be controlled at level <span class="math inline">\(\alpha\)</span>.</p>
<hr>
<section id="the-bonferroni-method-controls-the-fwer" class="level3">
<h3 class="anchored" data-anchor-id="the-bonferroni-method-controls-the-fwer">The Bonferroni method controls the FWER</h3>
<p>Single-step methods controls for multiple testing by estimating one local significance level, <span class="math inline">\(\alpha_{\text{loc}}\)</span>, which is used as a cut-off to detect significance for each individual test.</p>
<p>The Bonferroni method is valid for all types of dependence structures between the test statistics.</p>
<p>The local significance level is <span class="math display">\[\alpha_{\text loc}=\frac{\alpha}{m}\]</span></p>
<p>The adjusted <span class="math inline">\(p\)</span>-value is <span class="math display">\[ \tilde{p}_j =\min(1,m p_j)\]</span></p>
<p>Read more here if needed: <a href="https://www.math.ntnu.no/emner/TMA4267/2017v/multtest.pdf">Short note on multiple hypothesis testing</a></p>
<hr>
</section>
</section>
<section id="high-dimensional-inference" class="level2">
<h2 class="anchored" data-anchor-id="high-dimensional-inference">High-dimensional inference</h2>
<p>(Dezeure, Bühlmann, Meier, Meinshausen, 2.1.1 + 2.2)</p>
<ul>
<li>The article has focus on frequentist methods for high-dimensional inference with confidence intervals and <span class="math inline">\(p\)</span>-values in linear and generalized linear models.</li>
<li>We will focus on linear models.</li>
</ul>
<hr>
<p><strong>Set-up:</strong></p>
<p><span class="math display">\[Y={\boldsymbol X} \beta^0 +\varepsilon\]</span></p>
<ul>
<li><span class="math inline">\({\boldsymbol X}\)</span> is <span class="math inline">\(n\times p\)</span> design matrix</li>
<li><span class="math inline">\(Y\)</span> is an <span class="math inline">\(n \times 1\)</span> response vector</li>
<li><span class="math inline">\(\varepsilon\)</span> is an <span class="math inline">\(n \times 1\)</span> error vector, independent of <span class="math inline">\({\boldsymbol X}\)</span> and i.i.d. entries with <span class="math inline">\(\text{E}(\varepsilon)_i)=0\)</span>.</li>
<li>The number of parameters <em>may</em> be larger than the sample size (then the regression parameter is not identifiable in general).</li>
</ul>
<p>The <em>active set</em> is <span class="math display">\[S_0=\{ j; \beta_j^0 \neq 0,j=1,\ldots,p\}\]</span> with cardinality (size) <span class="math inline">\(\lvert S_0 \rvert\)</span>.</p>
<p><strong>Now:</strong> construct CI and <span class="math inline">\(p\)</span>-values for <em>individual regression parameters</em> <span class="math inline">\(\beta_j^0\)</span>, <span class="math inline">\(j=1,\ldots, p\)</span>, and also with multiple testing adjustment.</p>
<p>Remark: We want inference for <em>all</em> coefficients - not only the ones that the lasso has selected.</p>
<hr>
<ul>
<li>The lasso has desirable properties for estimating <span class="math inline">\(\beta^0\)</span> in high dimensional models, in particular for prediction <span class="math inline">\({\boldsymbol X}\beta^0\)</span> or a new response <span class="math inline">\(Y_{\text{new}}\)</span>.</li>
<li>But, the distribution of the estimator is hard to characterize, and</li>
<li>it has been shown that (for fixed <span class="math inline">\(p\)</span>) the asymptotic (<span class="math inline">\(n\leftarrow \infty\)</span>) distribution of the lasso has point mass at zero (which leads to that bootstrapping not having optimal properties).</li>
</ul>
<p>For the situation <span class="math inline">\(p &gt;&gt; n\)</span> extra assumptions are needed, called the <em>compatibility condition</em> on the design matrix, and this guarantees identifiability and socalled oracle optimality results for the lasso. However, this is reported to be <em>unrealistic in practical situations</em>.</p>
<hr>
</section>
<section id="single-sample-splitting" class="level2">
<h2 class="anchored" data-anchor-id="single-sample-splitting">Single-sample splitting</h2>
<ol type="1">
<li>Split the data into two (equal) halves, <span class="math inline">\(I_1\)</span> and <span class="math inline">\(I_2\)</span>, no observations in common.</li>
<li><span class="math inline">\(I_1\)</span> is used for model selection (with the lasso), with active variables in <span class="math inline">\(\hat{S}(I_1)\)</span>.</li>
<li>The selected covariates in <span class="math inline">\(\hat{S}(I_1)\)</span> is used for estimation in <span class="math inline">\(I_2\)</span>. To construct <span class="math inline">\(p\)</span>-values, <span class="math inline">\(P_j\)</span>, for example use LS with <span class="math inline">\(t\)</span>-tests. <span class="math inline">\(P\)</span>-values for variables not selected is set to <span class="math inline">\(1\)</span>. Remark: then the number of covariates selected in <span class="math inline">\(I_1\)</span> need be smaller than the sample size for <span class="math inline">\(I_2\)</span>.</li>
<li>The raw <span class="math inline">\(p\)</span>-values is corrected for multiple testing (Bonferroni method controlling FWER) <span class="math display">\[ P_{\text{corr},j}=\min(P_j \cdot \lvert \hat{S} \rvert,1)\]</span></li>
</ol>
<p>This avoids using the data twice! But, is very sensitive to the split - giving <em>wildly</em> different <span class="math inline">\(p\)</span>-values=<span class="math inline">\(p\)</span>-value lottery</p>
<p>How can this be amended?</p>
<hr>
</section>
<section id="multiple-sample-splitting" class="level2">
<h2 class="anchored" data-anchor-id="multiple-sample-splitting">Multiple-sample splitting</h2>
<ul>
<li><p>The single-sample splitting routine is run <span class="math inline">\(B\)</span> times giving <span class="math inline">\(P_{\text{corr},j}^{[b]}\)</span> for <span class="math inline">\(b=1,\ldots,B\)</span> and <span class="math inline">\(j=1,\ldots,p\)</span>.</p></li>
<li><p>Problem: how aggregate the <span class="math inline">\(B\)</span> <span class="math inline">\(p\)</span>-values for each <span class="math inline">\(j\)</span> to give one <span class="math inline">\(p\)</span>-value?</p></li>
<li><p>The different <span class="math inline">\(b\)</span> runs have many observations in common, so the <span class="math inline">\(p\)</span>-values for covariate <span class="math inline">\(j\)</span> are correlated.</p></li>
<li><p>The authors have shown in a previous article that for dependent <span class="math inline">\(p\)</span>-values that one solution (that gives valid <span class="math inline">\(p\)</span>-values) is to take the median and multiply with 2.</p></li>
<li><p>The result is more general, and <span class="math inline">\(\gamma\)</span> is a general quantile (<span class="math inline">\(\gamma=0.5\)</span> for the median):</p></li>
</ul>
<p><span class="math display">\[Q_k(\gamma)=\min(\text{empirical }\gamma- \text{quantile} \{ P_{\text{corr},j}^{[b]}/\gamma,b=1,\ldots, B\},1)\]</span></p>
<hr>
<ul>
<li>The authors get more advanced and choose to search all <span class="math inline">\(\gamma\)</span> within the interval <span class="math inline">\((\gamma_{\text{min}},1)\)</span>, where a common choice is <span class="math inline">\(\gamma_{\text{min}}=0.05\)</span>, to get the smallest <span class="math inline">\(p\)</span>-value. However there is a price to pay: <span class="math inline">\((1-\log(\gamma_{\text{min}}))\)</span></li>
</ul>
<p><span class="math display">\[ P_j=\min((1-\log(\gamma_{\text{min}})\cdot \inf_{\gamma \in (\gamma_{\text{min}},1)} Q_j(\gamma)),1)\]</span> for <span class="math inline">\(j=1,\ldots,p\)</span>.</p>
<p>Some assumptions are necessary to assure FWER control.</p>
<hr>
<p>Confidence intervals are found</p>
<ul>
<li>from the adjusted <span class="math inline">\(p\)</span>-values <span class="math inline">\(P_j\)</span></li>
<li>using the duality of <span class="math inline">\(p\)</span>-values and two-sided confidence intervals. That is,</li>
<li>a <span class="math inline">\((1-\alpha)\)</span> 100% CI contains values <span class="math inline">\(c\)</span> where the <span class="math inline">\(p\)</span>-value is below <span class="math inline">\(\alpha\)</span> for testing <span class="math inline">\(H_0: \beta_j=c\)</span>. A closed form solution involving <span class="math inline">\(P_j\)</span> is found.</li>
<li>Both single testing and multiple corrected testing CIs are found. (Appendix A.2 in article)</li>
</ul>
<hr>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(diabetes)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>x<span class="ot">=</span><span class="fu">cbind</span>(diabetes<span class="sc">$</span>x)<span class="co">#,diabetes$x2)</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>y<span class="ot">=</span>diabetes<span class="sc">$</span>y</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>hdires<span class="ot">=</span><span class="fu">multi.split</span>(<span class="at">x=</span>x,<span class="at">y=</span>y,<span class="at">B=</span><span class="dv">1000</span>,<span class="at">fraction=</span><span class="fl">0.5</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>                   <span class="at">ci.level=</span><span class="fl">0.95</span>, <span class="at">model.selector=</span>lasso.cv,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>                   <span class="at">classical.fit=</span>lm.pval, <span class="at">classical.ci=</span>lm.ci,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>                   <span class="at">return.nonaggr =</span> <span class="cn">FALSE</span>, <span class="co">#if not adj for multiple testing</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>                   <span class="at">return.selmodels=</span><span class="cn">FALSE</span>, <span class="co">#just to have a look!</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>                   <span class="at">verbose=</span><span class="cn">FALSE</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="fu">dput</span>(hdires,<span class="st">"hdires.dd"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<hr>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>hdires<span class="ot">=</span><span class="fu">dget</span>(<span class="st">"hdires.dd"</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(hdires)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "pval"             "pval.corr"        "pvals.nonaggr"    "ci.level"        
 [5] "lci"              "uci"              "gamma.min"        "sel.models"      
 [9] "method"           "call"             "clusterGroupTest"</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>hdires<span class="sc">$</span>gamma.min</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 0.999 0.999 0.050 0.064 0.999 0.999 0.050 0.999 0.052 0.999</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">#summary(hdires$pvals.nonaggr) # if return.nonaggr=TRUE</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>hdires<span class="sc">$</span>pval.corr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>         age          sex          bmi          map           tc          ldl 
1.000000e+00 1.000000e+00 5.178832e-10 1.331537e-02 1.000000e+00 1.000000e+00 
         hdl          tch          ltg          glu 
4.533731e-01 1.000000e+00 6.863052e-08 1.000000e+00 </code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(hdires<span class="sc">$</span>lci, hdires<span class="sc">$</span>uci)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]      [,2]
age       -Inf       Inf
sex -435.92911 103.48598
bmi  363.10330 776.42063
map   66.58321 472.64439
tc        -Inf       Inf
ldl       -Inf       Inf
hdl -406.96777  14.72096
tch -764.32633 201.01275
ltg  301.70828 715.18364
glu -327.08642 254.88420</code></pre>
</div>
</div>
<hr>
<section id="hdi-with-logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="hdi-with-logistic-regression">hdi with logistic regression</h3>
<p>For modifications to the call to <code>mult.split</code> see the Appendix of the master thesis of</p>
<ul>
<li>Martina Hall: <a href="http://hdl.handle.net/11250/2453095">“Statistical Methods for early Prediction of Cerebral Palsy based on Data from Computer-based Video Analysis”</a>.</li>
<li>Dag J. Kristiansen: <a href="http://hdl.handle.net/11250/2624609">“Detecting Neuronal Activity with Lasso Penalized Logistic Regression”</a></li>
<li>Haris Fawad: ” Modelling Neuronal Activity using Lasso Regularized Logistic Regression”( Modelling Neuronal Activity using Lasso Regularized Logistic Regression) - also <a href="https://github.com/harisf/neuro-lasso">git repo neuro-lasso</a>.</li>
</ul>
</section>
<section id="hdi---also-with-other-solutions" class="level3">
<h3 class="anchored" data-anchor-id="hdi---also-with-other-solutions">hdi - also with other solutions</h3>
<p>In the <code>hdi</code> package also solutions for <em>debiasing</em> the lasso estimator is included. (See HTW 6.4 or the Dezure et al article.)</p>
<hr>
</section>
<section id="summing-up" class="level3">
<h3 class="anchored" data-anchor-id="summing-up">Summing up</h3>
<p>What is the take home message from this “Sample splitting” story?</p>
</section>
</section>
</section>
<section id="inference-after-selection" class="level1">
<h1>Inference after selection</h1>
<p>(Taylor and Tibshirani, 2015 and HTW 6.3)</p>
<section id="the-plot" class="level2">
<h2 class="anchored" data-anchor-id="the-plot">The plot</h2>
<p>Let us leave the lasso for a while.</p>
<p>1980: small data sets, planned hypothesis to test ready before data collected, no model selection. Only fit model and look at CI and p-values.</p>
<p>After 1980: larger data sets and looking at data to give best model. New challenge: <em>how to do inference after selection</em>.</p>
<p>This is an important topic that is not a part of ANY statistical courses at IMF.</p>
<p>The main question is:</p>
<ul>
<li>we have used a selection method (forward selection, lasso) to find potential association between covariates and response,</li>
<li>with focus on interpreting the selected model: how can we assess the strength (read: CI and <span class="math inline">\(p\)</span>-value) of these findings?</li>
</ul>
<hr>
<p>The answer includes:</p>
<ul>
<li>we have “cherry picked” the strongest associations, and we can thus not just report CI and <span class="math inline">\(p\)</span>-values based on the final model - when all is done on the same data set.</li>
</ul>
<p>In this story we now focus on <em>understanding how our model selection influences the inference on the final model</em>.</p>
<p>The technical solutions are of less importance, and is not presented with enough mathematical detail so that we understand the method in detail.</p>
<p><em>Remark: the single and multiple sample splitting strategy isvalid.</em></p>
<hr>
</section>
<section id="forward-stepwise-regression" class="level2">
<h2 class="anchored" data-anchor-id="forward-stepwise-regression">Forward stepwise regression</h2>
<p><strong>Aim:</strong> Multiple linear regression - where forward stepwise regression is used to select the model</p>
<p><span class="math display">\[y_i=\beta_0+\sum_{j=1}^p x_{ij}\beta_j + \varepsilon_i\]</span></p>
<ul>
<li>Start with an empty model (only intercept)</li>
</ul>
<p>While some stopping criterion not reach - perform step</p>
<ul>
<li>At step <span class="math inline">\(k\)</span> add the predictor that gives the most decrease in the sums of squares of error (here now - to follow the notation denoted by RSS instead of what we previously called SSE)</li>
</ul>
<p><span class="math display">\[ \text{RSS}=\sum_{i=1}^N (y_i -\hat{y}_i)^2\]</span> where <span class="math inline">\(\hat{y}_i\)</span> is the predicted value for observation <span class="math inline">\(i\)</span> in this model.</p>
<hr>
<p>If we have a model with <span class="math inline">\(k-1\)</span> predictors and we would like to add one more predictor (the <span class="math inline">\(k\)</span> to be added):</p>
<p><span class="math display">\[ \text{R}_k=\frac{1}{\sigma^2}(\text{RSS}_{k-1}-\text{RSS}_{k})\sim \chi^2_1\]</span> where here <span class="math inline">\(\sigma^2\)</span> is assumed known.</p>
<p>This can be seen as an hypothesis test:</p>
<p><span class="math inline">\(H_0: \text{The new predictor is not relevant}\)</span> vs. <span class="math inline">\(H_1: \text{The new predictor is relevant}\)</span> with a <span class="math inline">\(p\)</span>-value calculated from the upper tail of the <span class="math inline">\(\chi^2_1\)</span>-distribution.</p>
<hr>
<p><strong>Alternative scenario:</strong></p>
<p>For simplicity - we look at the <span class="math inline">\(k=1\)</span>:</p>
<p>The order in which the predictors is to be entered in to the model was decided before the data was collected (or at least before the data was analysed or plottet).</p>
<p>Then: we are all good - and this <span class="math inline">\(p\)</span>-value from the <span class="math inline">\(\chi^2_1\)</span>-distribution will be a valid <span class="math inline">\(p\)</span>-value.</p>
<p>What does this mean in practice?</p>
<hr>
<p><strong>Back to original scenario:</strong></p>
<p>Assume we are at step <span class="math inline">\(k=1\)</span>, and will add the first predictor which is the one with the largest <span class="math inline">\(R_1\)</span>.</p>
<p>Will the distribution of the <em>maximal</em> <span class="math inline">\(R_1\)</span> be the same as the distribution of a given predefined <span class="math inline">\(R_1\)</span>?</p>
<p>Distribution to the maximum given that we have <span class="math inline">\(p\)</span> predictors:</p>
<p><span class="math display">\[ P(\max R_1 \ge c)=1-P(\max R_1 &lt; c)=1-P(\text{all p } R_1 \text{ are} &lt;c)\]</span></p>
<p>Study Figure 1 in the article for a plot of nominal vs.&nbsp;actual <span class="math inline">\(p\)</span>-value for <span class="math inline">\(p=10\)</span> and <span class="math inline">\(p=50\)</span>. The figure was made using Monte Carlo sampling.</p>
<hr>
<p><strong>Moving on to</strong> <span class="math inline">\(k&gt;1\)</span></p>
<ul>
<li>We would like to obtain valid (“correct”) <span class="math inline">\(p\)</span>-values for all steps, not only for <span class="math inline">\(k=1\)</span>.</li>
<li>Monte Carlo solution would be elaborate.</li>
</ul>
<p>The method used in the article is to calculate a <span class="math inline">\(p\)</span>-value for the covariate at step <span class="math inline">\(k\)</span> by conditioning on the fact that already the strongest <span class="math inline">\(k-1\)</span> predictors in this sequential set-up has already been chosen.</p>
<p>The <span class="math inline">\(p\)</span>-value at step <span class="math inline">\(k\)</span> would be dependent on the number of covariates <span class="math inline">\(p\)</span>.</p>
<p>We now change focus and look at the distribution of the estimated regression coefficient for the covariate added at step <span class="math inline">\(k\)</span>, because that can be used to construct both a CI for the coefficient and a <span class="math inline">\(p\)</span>-value for testing if the coefficient is different from zero.</p>
<hr>
</section>
<section id="the-polyhedral-result" class="level2">
<h2 class="anchored" data-anchor-id="the-polyhedral-result">The polyhedral result</h2>
<p>(for details consult HTW 6.3 or articles references to in the Taylor and Tibshirani article)</p>
<p><strong>Distribution for regression coefficient:</strong></p>
<ul>
<li>Assume that we are at some step <span class="math inline">\(k\)</span>, and that <span class="math inline">\(k-1\)</span> covariates are in the model.</li>
<li>We have found the new covariate to include, and fitted the model with the <span class="math inline">\(k\)</span> covariates.</li>
<li>Standard theory tells us that the estimator <span class="math inline">\(\hat{\beta}\)</span> for covariate <span class="math inline">\(k\)</span> is unbiased and follows a normal distribution with some variance <span class="math inline">\(\tau^2\)</span>.</li>
</ul>
<p><span class="math display">\[ \hat{\beta} \sim N(\beta,\tau^2)\]</span></p>
<hr>
<p>But, this is given that we only had these <span class="math inline">\(k\)</span> covariates available at the start. We will instead <em>condition on</em> selection event.</p>
<p>It turns out that the selection event can be written in a <em>polyhedral form</em> <span class="math inline">\(A y \le b\)</span> for some matrix <span class="math inline">\(A\)</span> and some vector <span class="math inline">\(b\)</span>.</p>
<p>At each step of the forward selection we have a competition among all <span class="math inline">\(p\)</span> variables, and the <span class="math inline">\(A\)</span> and <span class="math inline">\(b\)</span> is used to construct the competition.</p>
<hr>
<p>The correct distribution of the estimator <span class="math inline">\(\hat{\beta}\)</span> for covariate now has a <em>truncated normal distribution</em></p>
<p><span class="math display">\[ \hat{\beta} \sim TN^{c,d}(\beta,\tau^2)\]</span></p>
<p>i.e.&nbsp;the <em>same</em> normal distribution, but scaled to lie within the interval <span class="math inline">\((c,d)\)</span>.</p>
<p>The limits <span class="math inline">\((c,d)\)</span> depends on both the data and the selection evetns that lead to the current model.</p>
<p><em>The formulae for these limits are somewhat complicated but easily computable</em>.</p>
<p>This truncated normal distribution is used to calculate <em>selection-adjusted</em> <span class="math inline">\(p\)</span>-values and confidence interval.</p>
<p>(Study Figure 3. in the article by Taylor and Tibshirani.)</p>
<hr>
</section>
<section id="polyhedral-lasso-result" class="level2">
<h2 class="anchored" data-anchor-id="polyhedral-lasso-result">Polyhedral lasso result</h2>
<p>The same methodology can be used for the lasso, here also the selection of predictors can be described as a polyhedral region of the form <span class="math inline">\(Ay\le b\)</span> - for a fixed value <span class="math inline">\(\lambda\)</span>.</p>
<p>For the lass the <span class="math inline">\(A\)</span> and <span class="math inline">\(b\)</span> will depend on</p>
<ul>
<li>the predictors</li>
<li>the active set</li>
<li><span class="math inline">\(\lambda\)</span></li>
</ul>
<p>but not on <span class="math inline">\(y\)</span>.</p>
<p>The methods are on closed form, but the values <span class="math inline">\(c\)</span> and <span class="math inline">\(d\)</span> may be of complicated form.</p>
<hr>
<p>The R package <code>selectiveInference</code> can be used to find post selection <span class="math inline">\(p\)</span>-values both for forward stepwise selection and for the lasso.</p>
<p>See the package help for details.</p>
<p>Study Figure 5 from the article for an example of Naive and Selection-adjusted intervals for the lasso.</p>
<hr>
<section id="further-improvements" class="level3">
<h3 class="anchored" data-anchor-id="further-improvements">Further improvements</h3>
<p>The method yields rather wide confidence intervals for the regression parameters (given that we translate the <span class="math inline">\(p\)</span>-values into CIs).</p>
<p>There exists improvements to the results, in particular a method called <em>carving</em> which is explained in the you-tube videos from a course with <a href="https://lsa.umich.edu/stats/people/faculty/psnigdha.html">Snigdha Paragrahi</a></p>
<ul>
<li><a href="https://www.youtube.com/watch?v=qofrkW-DL7c&amp;t=3682s">Tutorial I</a></li>
<li><a href="https://www.youtube.com/watch?v=rGHf6BPeqBg&amp;t=1105s">Tutorial II</a></li>
</ul>
<hr>
</section>
</section>
<section id="posi" class="level2">
<h2 class="anchored" data-anchor-id="posi">PoSI</h2>
<p>(HTW 6.5)</p>
<ul>
<li>The POSI method also fits a selected submodel and</li>
<li>adjust the standard CIs by <em>accounting for all possible models that might have been delivered by the selection procedure</em>.</li>
<li>This means the method can be used on published results where the complete selection process is not explained in detail.</li>
<li>This lack of information of the selection process leads to <em>very wide CIs</em>.</li>
</ul>
<p>Inference is based on the submodel <span class="math inline">\(M\)</span> chosen, and on the projection of <span class="math inline">\({\boldsymbol X} \beta\)</span> onto the space spanned by the submodel <span class="math inline">\(M\)</span>:</p>
<p><span class="math display">\[\beta_M=({\boldsymbol X}_M^T{\boldsymbol X}_M)^{-1}{\boldsymbol X}_M^T{\boldsymbol X} \beta\]</span></p>
<hr>
<p>The method considers a confidence interval for the <span class="math inline">\(j\)</span>th element of <span class="math inline">\(\beta_M\)</span> of the form <span class="math display">\[\text{CI}_{jM}=\hat{\beta}_{jM}\pm K \hat{\sigma}v_{jM}\]</span> where <span class="math inline">\(v_{jM}^2=({\boldsymbol X}_M^T{\boldsymbol X}_M)^{-1}_{jj}\)</span></p>
<p>The constant <span class="math inline">\(K\)</span> is found to satisfy <span class="math display">\[ P(\beta_{jM}\in \text{CI}_{jM}) \le 1-2\alpha\]</span> <em>over all possible selection models</em>.</p>
<hr>
<p><span class="math inline">\(K\)</span> is a function of the data matrix <span class="math inline">\({\boldsymbol X}\)</span> and the maximum number of nonzero component allowed in <span class="math inline">\(\beta_M\)</span>. An upper bound on <span class="math inline">\(K\)</span> is known from a result on simultaneous intervals by Scheffe.</p>
<p>HTW page 161: Reports on the diabetes data with submodels of size 5, where the 95% CI value of K is 4.42 (“little less than 2 hours of computing”).</p>
<p>Details may be found in the PoSI 2013 article (reference Berk et al below).</p>
<hr>
<section id="posi-r-package" class="level3">
<h3 class="anchored" data-anchor-id="posi-r-package">PoSI R-package</h3>
<p><em>In linear LS regression, calculate for a given design matrix the multiplier</em> <span class="math inline">\(K\)</span> of coefficient standard errors such that the confidence intervals [b - K<em>SE(b), b + K</em>SE(b)] have a guaranteed coverage probability for all coefficient estimates b in any submodels after performing arbitrary model selection.</p>
<p>Results for the Boston housing data is available in the help section.</p>
<p><a href="https://cran.r-project.org/web/packages/PoSI/index.html" class="uri">https://cran.r-project.org/web/packages/PoSI/index.html</a></p>
<hr>
</section>
</section>
<section id="post-selection-inference-and-the-reproducibility-crisis" class="level2">
<h2 class="anchored" data-anchor-id="post-selection-inference-and-the-reproducibility-crisis">Post selection inference and the reproducibility crisis</h2>
<p>The <em>incorrect</em> use of CIs and <span class="math inline">\(p\)</span>-values in models found from model selection <em>and</em> inference on the same data - is though to be one of the main contributors to the <em>reproducibility crisis in science</em>.</p>
<p><a href="https://hdsr.mitpress.mit.edu/pub/l39rpgyc/release/1">Selective Inference: The Silent Killer of Replicability by Yoav Benjamini Published on Dec 16, 2020</a></p>
<hr>
</section>
</section>
<section id="conclusion-1" class="level1">
<h1>Conclusion</h1>
<section id="how-will-you-perform-inference" class="level2">
<h2 class="anchored" data-anchor-id="how-will-you-perform-inference">How will you perform inference</h2>
<p>on Data Analysis Project 1?</p>
<p>We discuss:</p>
<ol type="1">
<li>You can afford a test set</li>
<li>You have no test set</li>
</ol>
<hr>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<p>(also given in the text and not repeated here)</p>
<ul>
<li><a href="https://arxiv.org/pdf/1509.09169.pdf">Wessel N. van Wieringen: Lecture notes on ridge regression</a></li>
<li>A. Chatterjee and S. N. Lahiri (2011). Bootstrapping Lasso Estimators. Journal of the American Statistical Association. Vol. 106, No.&nbsp;494 (June 2011), pp.&nbsp;608-625 (18 pages)</li>
<li>Single/multi-sampling splitting part of <a href="https://projecteuclid.org/download/pdfview_1/euclid.ss/1449670857">Dezeure, Bühlmann, Meier, Meinshausen (2015)</a>. “High-Dimensional Inference: Confidence Intervals, p-Values and R-Software hdi”. Statistical Science, 2015, Vol. 30, No.&nbsp;4, 533–558 DOI: 10.1214/15-STS527 (only the single/multiple sample splitting part in 2.1.1 and 2.2 for linear regression, and using the method in practice).</li>
<li><a href="https://www.pnas.org/content/112/25/7629">Taylor and Tibshirani (2015)</a>: Statistical learning and selective inference, PNAS, vol 112, no 25, pages 7629-7634. (Soft version of HTW 6.3.2)</li>
<li>Berk, Richard; Brown, Lawrence; Buja, Andreas; Zhang, Kai; Zhao, Linda. Valid post-selection inference. Ann. Statist. 41 (2013), no. 2, 802–837. doi:10.1214/12-AOS1077.</li>
</ul>
</section>
<section id="left-overs" class="level1">
<h1>Left overs</h1>
<p>To check that all is here</p>
<section id="bootstrap-confidence-intervals" class="level2">
<h2 class="anchored" data-anchor-id="bootstrap-confidence-intervals">Bootstrap confidence intervals</h2>
</section>
<section id="percentile-interval" class="level2">
<h2 class="anchored" data-anchor-id="percentile-interval">Percentile interval</h2>
</section>
<section id="bias-corrected-accelerated-interval" class="level2">
<h2 class="anchored" data-anchor-id="bias-corrected-accelerated-interval">Bias corrected accelerated interval</h2>
</section>
<section id="single-hypothesis-test" class="level2">
<h2 class="anchored" data-anchor-id="single-hypothesis-test">Single hypothesis test</h2>
<p><span class="math display">\[H_{0}\colon \beta_j=0 \hspace{0.5cm} \text{vs.} \hspace{0.5cm} H_{1}\colon \beta_j \neq 0\]</span></p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Not reject <span class="math inline">\(H_0\)</span></th>
<th style="text-align: left;">Reject <span class="math inline">\(H_0\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(H_0\)</span> true</td>
<td style="text-align: left;">Correct</td>
<td style="text-align: left;">Type I error</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(H_0\)</span> false</td>
<td style="text-align: left;">Type II error</td>
<td style="text-align: left;">Correct</td>
</tr>
</tbody>
</table>
<hr>
<ul>
<li>Two types of errors are possible, type I error and type II error.</li>
<li>A type I error would be to reject <span class="math inline">\(H_0\)</span> when <span class="math inline">\(H_0\)</span> is true, that is concluding that there is a linear association between the response and the predictor where there is no such association. This is called a <em>false positive finding</em>.</li>
<li>A type II error would be to fail to reject <span class="math inline">\(H_0\)</span> when the alternative hypothesis <span class="math inline">\(H_1\)</span> is true, that is not detecting that there is a linear association between the response and the covariate. This is called a <em>false negative finding</em>.</li>
</ul>
<hr>
</section>
<section id="p-value" class="level2">
<h2 class="anchored" data-anchor-id="p-value"><span class="math inline">\(p\)</span>-value</h2>
<ul>
<li>A <span class="math inline">\(p\)</span>-value <span class="math inline">\(p(X)\)</span> is a test statistic satisfying <span class="math inline">\(0 \leq p({\boldsymbol Y}) \leq 1\)</span> for every vector of observations <span class="math inline">\(\boldsymbol{Y}\)</span>.</li>
<li>Small values give evidence that <span class="math inline">\(H_1\)</span> is true.</li>
<li>In single hypothesis testing, if the <span class="math inline">\(p\)</span>-value is less than the chosen significance level (chosen upper limit for the probability of committing a type I error), then we reject the null hypothesis, <span class="math inline">\(H_0\)</span>.</li>
<li>The chosen significance level is often referred to as <span class="math inline">\(\alpha\)</span>.</li>
</ul>
<hr>
<p>A <span class="math inline">\(p\)</span>-value is <em>valid</em> if <span class="math display">\[ P(p(\boldsymbol{Y}) \leq \alpha) \leq \alpha \]</span> for all <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(0 \leq \alpha \leq 1\)</span>, whenever <span class="math inline">\(H_0\)</span> is true, that is, if the <span class="math inline">\(p\)</span>-value is valid, rejection on the basis of the <span class="math inline">\(p\)</span>-value ensures that the probability of type I error does not exceed <span class="math inline">\(\alpha\)</span>.</p>
<hr>
<p>An <em>exact</em> <span class="math inline">\(p\)</span>-value satisfies <span class="math display">\[P(p(\boldsymbol{Y}) \leq \alpha) = \alpha\]</span> for all <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(0 \leq \alpha \leq 1\)</span>.</p>
<ul>
<li>The exact <span class="math inline">\(p\)</span>-value is uniformly distributed when the null hypothesis is true.</li>
<li>This is a fact that is often misunderstood by users of <span class="math inline">\(p\)</span>-values.</li>
<li>The incorrect urban myth is that <span class="math inline">\(p\)</span>-values from true null hypotheses are close to one, when the correct fact is that all values in intervals of the same length are equally probable (which is a property of the uniform distribution).</li>
</ul>
</section>
<section id="false-discovery-rate" class="level2">
<h2 class="anchored" data-anchor-id="false-discovery-rate">False discovery rate</h2>
<p>The false discovery rate (FDR) is defined as <span class="math display">\[ \text{FDR}= \text{E}(\frac{V}{R})  \text{ for }R&gt;0\]</span> and <span class="math display">\[ \text{FDR}= 0 \text{ else} \]</span></p>

</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-casi" class="csl-entry" role="doc-biblioentry">
Efron, Bradley, and Trevor Hastie. 2016. <em>Computer Age Statistical Inference - Algorithms, Evidence, and Data Science</em>. Cambridge University Press. <a href="https://hastie.su.domains/CASI/">https://hastie.su.domains/CASI/</a>.
</div>
<div id="ref-HTW" class="csl-entry" role="doc-biblioentry">
Hastie, Trevor, Roberg Tibshirani, and Martin Wainwright. 2015. <em>Statistical Learning with Sparsity: The Lasso and Generalizations</em>. CRC Press. <a href="https://hastie.su.domains/StatLearnSparsity/">https://hastie.su.domains/StatLearnSparsity/</a>.
</div>
<div id="ref-WNvW" class="csl-entry" role="doc-biblioentry">
Wieringen, Wessel N. van. 2021. <span>“Lecture Notes on Ridge Regression.”</span> <a href="https://arxiv.org/pdf/1509.09169v7.pdf">https://arxiv.org/pdf/1509.09169v7.pdf</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>