% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={MA8701 Advanced methods in statistical inference and learning},
  pdfauthor={Mette Langaas},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{MA8701 Advanced methods in statistical inference and learning}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{W6: Statistical inference for penalized GLM methods}
\author{Mette Langaas}
\date{2/19/23}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, breakable, borderline west={3pt}{0pt}{shadecolor}, enhanced, interior hidden, sharp corners, frame hidden]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}
\hypertarget{before-we-begin}{%
\section{Before we begin}\label{before-we-begin}}

\hypertarget{outline}{%
\subsection{Outline}\label{outline}}

\hypertarget{l11}{%
\subsubsection{L11}\label{l11}}

\begin{itemize}
\tightlist
\item
  Prediction vs statistics inference: what are the aims?
\item
  Sampling distributions
\item
  Debiased lasso
\item
  Bayesian lasso
\item
  Boostrapping
\end{itemize}

\hypertarget{l12}{%
\subsubsection{L12}\label{l12}}

\begin{itemize}
\tightlist
\item
  Selective inference
\item
  Sample splitting
\item
  Inference after selection (forward regression example, polyhedral
  result, lasso result PoSI)
\item
  Reproducibility crisis
\item
  Conclusions
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{literature}{%
\subsection{Literature}\label{literature}}

\textbf{Main source:}

\begin{itemize}
\tightlist
\item
  {[}HTW{]} Hastie, Tibshirani, Wainwright: ``Statistical Learning with
  Sparsity: The Lasso and Generalizations''. CRC press.
  \href{https://trevorhastie.github.io/}{Ebook}. Chapter 6.0, 6.1, 6.2,
  6.4, 6.5. (Results from 6.3 through Taylor and Tibshirani (2015))
\end{itemize}

Also: brush up on bootstrap intervals from TMA4300, where
\href{https://onlinelibrary.wiley.com/doi/book/10.1002/9781118555552}{Givens
and Hoeting (2013)} is on the reading list. See specifically chapter 9
(9.2.1 and 9.3 will be used here). NTNU-access to the full book if you
are on vpn.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Secondary sources:}

\begin{itemize}
\item
  \href{https://www.math.ntnu.no/emner/TMA4267/2017v/multtest.pdf}{Short
  note on multiple hypothesis testing} in TMA4267 Linear Statistical
  Models, Kari K. Halle, Øyvind Bakke and Mette Langaas, March 15, 2017.
\item
  Single/multi-sampling splitting part of
  \href{https://projecteuclid.org/download/pdfview_1/euclid.ss/1449670857}{Dezeure,
  Bühlmann, Meier, Meinshausen (2015)}. ``High-Dimensional Inference:
  Confidence Intervals, p-Values and R-Software hdi''. Statistical
  Science, 2015, Vol. 30, No.~4, 533--558 DOI: 10.1214/15-STS527 (only
  the single/multiple sample splitting part in 2.1.1 and 2.2 for linear
  regression, and using the method in practice). Or Chapter 11 of
  \href{https://link.springer.com/book/10.1007/978-3-642-20192-9}{Bühlmann
  and de Geer (2011)}
\item
  \href{https://www.pnas.org/content/112/25/7629}{Taylor and Tibshirani
  (2015)}: Statistical learning and selective inference, PNAS, vol 112,
  no 25, pages 7629-7634. (Soft version of HTW 6.3.2)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{./P2overview.png}

}

\caption{Overview of Part 2}

\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistical-inference}{%
\section{Statistical inference}\label{statistical-inference}}

We have now heard about the South African heart disease data, and we
will also look at at a regression problem with prediction of disease
progression in diabetes.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{prediction-vs-statistical-inference}{%
\subsection{Prediction vs statistical
inference}\label{prediction-vs-statistical-inference}}

\hypertarget{prediction}{%
\subsubsection{Prediction}\label{prediction}}

\begin{itemize}
\tightlist
\item
  Predict the value of the progression variable for a person with
  diabetes.
\item
  Predict the probability of heart disease for a person from the
  population in the South African heart disease example.
\end{itemize}

\hypertarget{inference}{%
\subsubsection{Inference}\label{inference}}

\begin{itemize}
\tightlist
\item
  Assess the goodness of the prediction (MSE, error rate, ROC-AUC) -
  with uncertainty.
\item
  Interpret the GLM-model - which covariates are included?
\item
  Confidence interval for the model regression parameters.
\item
  Testing hypotheses about the model regression parameters.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistics-vs-machine-learning}{%
\subsection{Statistics vs Machine
learning}\label{statistics-vs-machine-learning}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{./StatsvsMLPredInf.jpg}

}

\caption{Figures redrawn from Robert Tibshiran´s Breiman lecture at the
NIPS 2015 \url{https://www.youtube.com/watch?v=RKQJEvc02hc\&t=81s}.
(Conference on Neural Information Processing System)}

\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{known-sampling-distributions}{%
\subsection{Known sampling
distributions}\label{known-sampling-distributions}}

For the linear regression and logistic regression we know the sampling
distribution of the regression coefficient estimators.

Then it is easy to construct confidence intervals and perform hypothesis
tests.

What are the known results?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{multiple-linear-regression}{%
\subsubsection{Multiple linear
regression}\label{multiple-linear-regression}}

\[{\boldsymbol Y=X \boldsymbol{\beta}}+{\boldsymbol \varepsilon}\] where
\(\varepsilon\sim N_N({\boldsymbol 0},\sigma^2{\boldsymbol I})\)
(independent observation pairs).

\[\hat{\beta}_{\text{LS}}=({\boldsymbol X}^T{\boldsymbol X})^{-1} {\boldsymbol X}^T {\boldsymbol Y}\]
with
\(\hat{\beta}_{\text LS}\sim N_{p}(\beta,\sigma^2({\boldsymbol X}^T{\boldsymbol X})^{-1})\).

Restricted maximum likelihood estimator for \(\sigma^2\):
\[\hat{\sigma}^2=\frac{1}{N-p}({\boldsymbol Y}-{\boldsymbol X}\hat{\beta}_{\text LS})^T({\boldsymbol Y}-{\boldsymbol X}\hat{\beta}_{\text LS})=\frac{\text{SSE}}{N-p}\]
with \(\frac{(N-p)\hat{\sigma}^2}{\sigma^2} \sim \chi^2_{N-p}\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Statistic for inference about \(\beta_j\), \(c_{jj}\) is diagonal
element \(j\) of \(({\boldsymbol X}^T{\boldsymbol X})^{-1}\).
\[T_j=\frac{\hat{\beta}_{\text LS,j}-\beta_j}{\sqrt{c_{jj}}\hat{\sigma}}\sim t_{N-p}\]

or, inference can be done asymptotically and then replace the \(t\) with
the normal distribution.

\(T_j\) is the starting point for constructing CIs for \(\beta_j\) and
testing hypotheses about \(\beta_j\).

Observe: the least squares estimator is \emph{unbiased}!

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{logistic-regression}{%
\subsubsection{Logistic regression}\label{logistic-regression}}

\(\hat{\boldsymbol{\beta}}\) is now not on closed form, but
asymptotically when \(N \rightarrow \infty\)
\[\hat{\boldsymbol\beta} \approx N_{p}(\boldsymbol\beta,(\boldsymbol X^T\hat{\boldsymbol{W}}\boldsymbol X)^{-1})\]
where \(\boldsymbol W=diag(\hat{\pi}_i(1-\hat{\pi}_i))\), so that
inference can be based on the asymptotic normality of each element of
the regression estimate vector.

Observe: the logistic regression parameter estimator is \emph{unbiased}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{confidence-interval-generic-set-up}{%
\subsection{Confidence interval --- generic
set-up}\label{confidence-interval-generic-set-up}}

\textbf{Set-up}

\begin{itemize}
\tightlist
\item
  We have a random sample \(Y_1,Y_2,\ldots,Y_N\) from
\item
  some distribution \(F\) with some (unkonwn) parameter \(\theta\).
\item
  Let \(y_1,y_2,\ldots,y_N\) be the observed values for the random
  sample.
\end{itemize}

\textbf{Statistics}

\begin{itemize}
\tightlist
\item
  We have two statistics \(\hat{\theta}_L(Y_1,Y_2,\ldots,Y_N)\) and
  \(\hat{\theta}_U(Y_1,Y_2,\ldots,Y_N)\) so that
\end{itemize}

\[P(\hat{\theta}_L(Y_1,Y_2,\ldots,Y_N)\le \theta \le \hat{\theta}_U(Y_1,Y_2,\ldots,Y_N))=1-\alpha\]
where \(\alpha\in [0,1]\)

\textbf{Confidence interval}

The numerical interval
\[[\hat{\theta}_L(y_1,y_2,\ldots,y_N),\hat{\theta}_U(y_1,y_2,\ldots,y_N)]\]
is called a \((1-\alpha)\) 100\% confidence interval.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sampling-distribution-for-ridge-and-lasso}{%
\subsection{Sampling distribution for ridge and
lasso?}\label{sampling-distribution-for-ridge-and-lasso}}

\hypertarget{multippel-linear-ridge-regression}{%
\subsubsection{Multippel linear ridge
regression}\label{multippel-linear-ridge-regression}}

\[ \hat{\beta}_{\text{ridge}}=({\boldsymbol X}^T{\boldsymbol X}+\lambda {\boldsymbol I})^{-1} {\boldsymbol X}^T {\boldsymbol Y}\]

\[\hat{\beta}(\lambda)_{\text{ridge}} \sim N \{ (\boldsymbol{X}^T \boldsymbol{X} + \lambda \boldsymbol{I}_{p})^{-1} \boldsymbol{X}^T \boldsymbol{X} \, \beta,\]

\[\sigma^2 ( \boldsymbol{X}^T \boldsymbol{X} + \lambda \boldsymbol{I}_{p} )^{-1}  \boldsymbol{X}^T \boldsymbol{X} ( \boldsymbol{X}^T \boldsymbol{X} + \lambda \boldsymbol{I}_{p} )^{-1}  \}.\]

\[\text{df}(\lambda)=\text{tr}({\boldsymbol H}_{\lambda})=\text{tr}({\boldsymbol X}({\boldsymbol X}^T{\boldsymbol X}+ \lambda {\boldsymbol I})^{-1}{\boldsymbol X}^T)=\cdots=\sum_{j=1}^p \frac{d_j^2}{d_j^2+\lambda}\]

\begin{itemize}
\tightlist
\item
  What can we do with that?
\item
  What if the design matrix is orthogonal, does that help?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{logistic-ridge}{%
\subsubsection{Logistic ridge}\label{logistic-ridge}}

For large sample sizes the ridge logistic regression estimator is
approximately multivariate normal (Wieringen (2021) Section 5.3).

\[\hat{\boldsymbol\beta}(\lambda) \approx N_{p}(\boldsymbol\beta-\lambda (\boldsymbol X^T\hat{\boldsymbol{W}}\boldsymbol X+\lambda \boldsymbol I)^{-1}\boldsymbol \beta),\]
\[(\boldsymbol X^T\hat{\boldsymbol{W}}\boldsymbol X+\lambda \boldsymbol I)^{-1}-(\boldsymbol X^T\hat{\boldsymbol{W}}\boldsymbol X+\lambda \boldsymbol I)^{-2})\]
This is based on the asymptotic normality of the score function
(gradient of the loglikelihood - here the penalized loglikelihood).

\begin{itemize}
\tightlist
\item
  What can we do with that?
\item
  What if the design matrix is orthogonal, does that help?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{lasso}{%
\subsubsection{Lasso}\label{lasso}}

Some results using approximations to ridge (for mean and variance, see
Wieringen (2021) p 97), but else \emph{no parametric version of sampling
distribution} known.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{what-is-our-aim}{%
\subsubsection{What is our aim?}\label{what-is-our-aim}}

Penalized estimation: reduce variance by introducing (strong) bias.

The squared bias then is a major part of the mean squared error, and the
variance is thus a minor part.

But, do we need to use the ridge or lasso estimator to construct a
confidence interval for \(\boldsymbol \beta\) or test if \(\beta_j=0\)?

\begin{itemize}
\tightlist
\item
  \(p>N\)
\item
  \(p<N\)?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{debiased-desparsified-lasso}{%
\subsubsection{Debiased (desparsified)
lasso}\label{debiased-desparsified-lasso}}

(HTW Section 6.4)

\[\hat{\beta}^d=\hat{\beta}_{\lambda}+\frac{1}{N}{\boldsymbol M}{\boldsymbol X}^T(\boldsymbol Y-\boldsymbol X \hat{\beta}_{\lambda})\]

the matrix \({\boldsymbol M}\) is some approximation to the inverse of
\[\hat{\boldsymbol \Sigma}=\frac{1}{N}{\boldsymbol X}^T{\boldsymbol X}\]
Use the debiased estimator to form CI from:

\[\hat{\beta}^d \sim N(\beta,\frac{\sigma^2}{N}{\boldsymbol M}\hat{\boldsymbol \Sigma}{\boldsymbol M}^T)\]

Interpretation of debiasing: assume we want to minimize the residual sum
of squares using an approximate Newton step starting at the lasso
estimator.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Two solutions are based on

\begin{itemize}
\tightlist
\item
  neighbour-based methods to impose sparsity
\item
  optimization problem to get
  \(\hat{\boldsymbol \Sigma}\hat{\boldsymbol M}\approx \boldsymbol I\)
  while the variance of the debiased estimator is small.
\end{itemize}

See Hastie, Tibshirani, and Wainwright (2015) page 159 for references to
these solutions.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./HTWFig613.jpg}

}

\caption{Figure 6.13 in Hastie, Tibshirani, and Wainwright (2015)}

\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

This is absolutely not straightforward.

The \emph{adaptive and biased nature} of the estimation procedures makes
it challenging to perform inference.

\begin{itemize}
\tightlist
\item
  We will \emph{discuss} other (in addition to the debiasing) solutions
  to finding confidence intervals for regression parameters for lasso,
  and for constructing \(p\)-values for testing hypotheses about the
  regression parameters.
\item
  We will address some \emph{philosophical principles behind inference}
\item
  and mention topics that can be studied further for the interested
  student!
\end{itemize}

Warning: there seems not to be consensus, but many interesting
approaches and ideas that we may consider.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{diabetes-data}{%
\subsection{Diabetes data}\label{diabetes-data}}

In a medical study the aim was to explain the ethiology of diabetes
progression. Data was collected from \(n=442\) diabetes patients, and
from each patient the following measurements are available:

\begin{itemize}
\tightlist
\item
  \texttt{age} (in years) at baseline
\item
  \texttt{sex} (0=female and 1=male) at baseline
\item
  body mass index (\texttt{bmi}) at baseline
\item
  mean arterial blood pressure (\texttt{map}) at baseline
\item
  six blood serum measurements: total cholesterol (\texttt{tc}), ldl
  cholesterol (\texttt{ldl}), hdl cholesterol (\texttt{hdl}),
  \texttt{tch}, \texttt{ltg}, glucose \texttt{glu}, all at baseline,
\item
  a quantitative measurement of disease progression one year after
  baseline (\texttt{prog})
\end{itemize}

All measurements except \texttt{sex} are continuous. There are 10
covariates.

The response is the disease progression \texttt{prog} - thus a
regression problem.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Data can be

\begin{itemize}
\tightlist
\item
  downloaded from
  \url{https://web.stanford.edu/~hastie/StatLearnSparsity_files/DATA/diabetes.html}
  in three variants: raw, standardized and 442 \(\times\) 64 matrix with
  quadratic terms (not used here).
\item
  Or, loaded from the \texttt{lars} package, that is automatically
  loaded in the \texttt{monomvn} package (where \texttt{blasso} is
  found).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{W6_files/figure-pdf/unnamed-chunk-5-1.pdf}

\hypertarget{bayesian-ridge-and-lasso}{%
\section{Bayesian ridge and lasso}\label{bayesian-ridge-and-lasso}}

(HTW 6.1 for lasso, WNvW Section 5.5 and 6.6)

For penalized models there exists Bayesian equivalents. We will here
focus on the multiple linear regression model.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bayesian-set-up}{%
\subsection{Bayesian set-up}\label{bayesian-set-up}}

In the Bayesian statistics the regression parameters \(\beta\) are
random quantities, and in addition to the likelihood also a prior for
the regression parameters (and other parameters) are needed. When a
\emph{conjugate prior} the posterior distribution may be derived
analytically.

Multiple linear regression: distribution of response - where we for
simplicity assume that we have centred covariates and centred response
(so no intercept term)

\[ {\boldsymbol y}\mid \beta, \sigma \sim N({\boldsymbol X}\beta,\sigma^2 {\boldsymbol I})\]
This gives the likelihood:

\[ L(\boldsymbol \beta \mid \boldsymbol y,\boldsymbol X, \sigma) \propto 
(\sigma^{-N/2}) \exp[- \frac{1}{2 \sigma^2}(\boldsymbol y -\boldsymbol X \boldsymbol \beta)^T(\boldsymbol y -\boldsymbol X \boldsymbol \beta)]\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{prior-for-regression-parameters-ridge}{%
\subsection{Prior for regression parameters
(ridge)}\label{prior-for-regression-parameters-ridge}}

In Part 1 we worked with multiple imputation and one method for drawing
observations was the Bayesian linear regression.

We will use the same priors here, for the \(\sigma^2\) we use a inverse
Gamma prior. For the regression coefficents a normal prior is used.

\[\beta \mid \sigma \sim \prod_{j=1}^p \sqrt{\frac{\lambda}{2 \sigma^2}}\exp(-\frac{\lambda}{2 \sigma^2}\beta_j^2)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{W6_files/figure-pdf/unnamed-chunk-6-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{posterior-for-regression-parameters-ridge}{%
\subsection{Posterior for regression parameters
(ridge)}\label{posterior-for-regression-parameters-ridge}}

\[ \beta, \mid \boldsymbol X, \boldsymbol Y \sigma^2 \propto \exp[-\frac{1}{2\sigma^2}](\boldsymbol \beta-\hat{\boldsymbol \beta}({\lambda}))^T ({\boldsymbol X}^T \boldsymbol X+\lambda \boldsymbol I)(\boldsymbol \beta-\hat{\boldsymbol \beta}({\lambda}))]\]
The posterior mean is the ridge estimator
\(\hat{\boldsymbol \beta}(\lambda)\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{prior-for-regression-parameters-lasso}{%
\subsection{Prior for regression parameters
(lasso)}\label{prior-for-regression-parameters-lasso}}

\[\beta \mid \lambda, \sigma \sim \prod_{j=1}^p \frac{\lambda}{2 \sigma}\exp(-\frac{\lambda}{\sigma}\lvert \beta_j \rvert)\]

This prior is called an i.i.d. \emph{Laplacian} (or double exponential)
prior.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{W6_files/figure-pdf/unnamed-chunk-7-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{posterior-for-regression-parameters-lasso}{%
\subsection{Posterior for regression parameters
(lasso)}\label{posterior-for-regression-parameters-lasso}}

It can be shown that the negative log of the posterior density for
\(\beta \mid {\boldsymbol y}, \lambda, \sigma\) is (up to an additive
constant)

\[\frac{1}{2\sigma^2} \Vert {\boldsymbol y}-{\boldsymbol X}\beta\Vert_2^2 +\frac{\lambda}{\sigma} \Vert \beta \Vert_1\]

Does this look familiar?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

For a fixed value of \(\sigma\) and \(\lambda\) - the \(\beta\) giving
the minimum of the negative log posterior is the \emph{lasso} estimate
where the regularization parameter is \(\sigma \lambda\).

The minimum negative log posterior will then be the same as the maximum
log posterior - and the maximum of a distribution is called the
\emph{mode} of the distribution.

The lasso estimate is the \emph{posterior mode} in the Bayesian model.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./HTWFig61.jpg}

}

\caption{Figure 6.1 in Hastie, Tibshirani, and Wainwright (2015)}

\end{figure}

From Hastie, Tibshirani, and Wainwright (2015): a 95\% posterior
credibility interval covers zero.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{./HTWFig62.jpg}

}

\caption{Figure 6.2 in Hastie, Tibshirani, and Wainwright (2015)}

\end{figure}

Hastie, Tibshirani, and Wainwright (2015): MCMC with 10 000 samples.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

A full Bayesian approach requires priors for \(\lambda\) and \(\sigma\),
in addition to priors on the regression coefficient.

Markov Chain Monte Carlo MCMC is used efficiently sample realizations
form the posterior distribution.

See Wieringen (2021) Chapter 2 for more on Bayesian regression and the
connection to the ridge and Section 6.6 for connection to lasso.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{not-only-the-point-estimate}{%
\subsection{Not only the point
estimate}\label{not-only-the-point-estimate}}

The posterior distribution gives the

\begin{itemize}
\tightlist
\item
  point estimates for the lasso (the mode of the distribution)
\end{itemize}

but

\begin{itemize}
\tightlist
\item
  also the \emph{entire joint distribution}.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./HTWFig63.jpg}

}

\caption{Figure 6.3 in Hastie, Tibshirani, and Wainwright (2015)}

\end{figure}

Hastie, Tibshirani, and Wainwright (2015): 10 000 samples from the
posterior.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{diabetes-example-with-blasso}{%
\subsubsection{\texorpdfstring{Diabetes example with
\texttt{blasso}}{Diabetes example with blasso}}\label{diabetes-example-with-blasso}}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# code below copied from the help(blasso)}
\DocumentationTok{\#\# following the lars diabetes example}
\FunctionTok{data}\NormalTok{(diabetes)}
\FunctionTok{attach}\NormalTok{(diabetes)}

\DocumentationTok{\#\# Ordinary Least Squares regression}
\NormalTok{reg.ols }\OtherTok{\textless{}{-}} \FunctionTok{regress}\NormalTok{(x, y)}

\DocumentationTok{\#\# Lasso regression}
\NormalTok{reg.las }\OtherTok{\textless{}{-}} \FunctionTok{regress}\NormalTok{(x, y, }\AttributeTok{method=}\StringTok{"lasso"}\NormalTok{)}

\DocumentationTok{\#\# Bayesian Lasso regression}
\NormalTok{reg.blas }\OtherTok{\textless{}{-}} \FunctionTok{blasso}\NormalTok{(x, y,}\AttributeTok{verb=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{W6_files/figure-pdf/unnamed-chunk-12-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# plot the size of different models visited}
\CommentTok{\#plot(reg.blas, burnin=200, which="m")}

\DocumentationTok{\#\# get the summary}
\NormalTok{s }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(reg.blas, }\AttributeTok{burnin=}\DecValTok{200}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{W6_files/figure-pdf/unnamed-chunk-14-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bootstrap}{%
\section{Bootstrap}\label{bootstrap}}

(HTW 6.2)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{procedure-to-find-lasso-estimate-hatbetahatlambda_cv}{%
\subsection{\texorpdfstring{Procedure to find lasso estimate
\(\hat{\beta}(\hat{\lambda}_{CV})\)}{Procedure to find lasso estimate \textbackslash hat\{\textbackslash beta\}(\textbackslash hat\{\textbackslash lambda\}\_\{CV\})}}\label{procedure-to-find-lasso-estimate-hatbetahatlambda_cv}}

(Copied word by word from HTW page 142)

Refer to these 6 steps as \(\hat{\beta}(\hat{\lambda}_{CV})\)-loop

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Fit a lasso path to \((X, y)\) over a dense grid of values
  \(\Lambda=\{\lambda_l\}_{l=1}^{L}\).
\item
  Divide the training samples into 10 groups at random.
\item
  With the \(k\)th group left out, fit a lasso path to the remaining
  \(9/10\)ths, using the same grid \(\Lambda\).
\item
  For each \(\lambda \in \Lambda\) compute the mean-squared prediction
  error for the left-out group.
\item
  Average these errors to obtain a prediction error curve over the grid
  \(\Lambda\).
\item
  Find the value \(\hat{\beta}(\hat{\lambda}_{CV})\) that minimizes this
  curve, and then return the coefficient vector from our original fit in
  step (1) at that value of \(\lambda\).
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Observe:}

\begin{itemize}
\tightlist
\item
  \(\lambda\)-path is the same for each run of the lasso
\item
  the chosen \(\lambda\) is then used on the orginal data
\end{itemize}

\textbf{Q:} Is it possible to use resampling to estimate the
distribution of the lasso \(\hat{\beta}\) estimator including the model
selection (choosing \(\lambda\))?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{non-parametric-paired-bootstrap}{%
\subsection{Non-parametric (paired)
bootstrap}\label{non-parametric-paired-bootstrap}}

\begin{itemize}
\tightlist
\item
  Let \(F\) denote the joint distribution of \((X,Y)\).
\item
  The empirical \(\hat{F}\) is \(\frac{1}{N}\) for each observation
  \((X,Y)\) in our training data \((X_i,Y_i)\), \(i=1,\ldots,N\).
\item
  Drawing from \(\hat{F}\) is the same as drawing from the \(N\)
  observations in the training data with replacement.
\end{itemize}

Now, we draw \(B\) bootstrap samples from the training data, and for
each new bootstrap sample we run through the 6 steps in the
\(\hat{\beta}(\hat{\lambda}_{CV})\)-loop.

\begin{itemize}
\tightlist
\item
  The result is \(B\) vectors \(\hat{\beta}(\hat{\lambda}_{CV})\).
\item
  We plot the result as

  \begin{itemize}
  \tightlist
  \item
    boxplots,
  \item
    proportion of times each element of
    \(\hat{\beta}(\hat{\lambda}_{CV})\) is equal 0.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{diabetes-example}{%
\subsubsection{Diabetes example}\label{diabetes-example}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{W6_files/figure-pdf/unnamed-chunk-18-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{W6_files/figure-pdf/unnamed-chunk-19-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{verbatim}
11 x 1 sparse Matrix of class "dgCMatrix"
                   s1
(Intercept) 152.13348
age           .      
sex           .      
bmi         493.42401
map         172.00436
tc            .      
ldl           .      
hdl         -94.48438
tch           .      
ltg         428.55200
glu           .      
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{W6_files/figure-pdf/unnamed-chunk-23-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{W6_files/figure-pdf/unnamed-chunk-25-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{W6_files/figure-pdf/unnamed-chunk-26-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bootstrapping-vs-bayesian-lasso}{%
\subsection{Bootstrapping vs Bayesian
lasso}\label{bootstrapping-vs-bayesian-lasso}}

The results from the Bayesian lasso on the proportion of times a
coefficient is 0 and the boxplots are very similar to the results from
the bootstrapping. The bootstrap seems to be doing the ``same'' as a
Bayesian analysis with the Laplacian prior.

When the model is not so complex and the number of covariates is not too
large (\(p\sim 100\)) the Bayesian lasso might be as fast as the
bootstrapping, but for larger problems the bootstrap ``scales better''.

For GLMs the Bayesian solution is more demanding, but the bootstrap is
as easy as for the linear model.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{./HTWFig63.jpg}

}

\caption{Figure 6.3 in Hastie, Tibshirani, and Wainwright (2015)}

\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{./HTWFig64.jpg}

}

\caption{Figure 6.4 in Hastie, Tibshirani, and Wainwright (2015)}

\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bootstrap-percentile-ci}{%
\subsection{Bootstrap percentile CI}\label{bootstrap-percentile-ci}}

To construct a \((1-\alpha)\cdot 100\)\% CI:

\begin{itemize}
\tightlist
\item
  order the bootstrap sample for the estimate of interest
\item
  read off the \((1-\alpha/2)\cdot 100\) percetile
\item
  \((\alpha/2)\cdot 100\) percentile
\end{itemize}

These are now the lower and upper limit of the CI.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bootstrap-bca-ci}{%
\subsection{Bootstrap BCa CI}\label{bootstrap-bca-ci}}

See page 34 of
\href{https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/3026839}{Bootstrap
confidence intervals in the master thesis of Lene Tillerli Omdal Section
3.6.2} and teaching material from TMA4300:
\href{https://onlinelibrary.wiley.com/doi/book/10.1002/9781118555552}{Givens
and Hoeting (2013)} chapter 9.3. NTNU-access to the full book if you are
on vpn.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{diabetes-example-1}{%
\subsubsection{Diabetes example}\label{diabetes-example-1}}

What if we calculated percentile bootstrap intervals - could we use that
to say anything about the true underlying regression coefficients?

\begin{verbatim}
[1] "Ridge first then lasso"
\end{verbatim}

\begin{verbatim}
               2.5%         50%      97.5%        2.5%       50%     97.5%
Int.cept  147.08515  152.140842  157.48081  146.942231 152.17445 157.29619
age       -30.29701   28.233633   84.43224    0.000000   0.00000   0.00000
sex      -170.89169  -92.265146  -26.22187 -115.349163   0.00000   0.00000
bmi       232.35050  321.111896  421.01718  329.357643 469.97818 617.77346
map       140.21620  207.461944  278.29632    0.000000 151.40541 304.09469
tc        -45.76264    2.724457   43.62582   -4.357653   0.00000   0.00000
ldl       -91.46718  -35.412937   15.64376  -22.951886   0.00000   0.00000
hdl      -206.89616 -157.552738 -109.32657 -218.711400 -80.79045   0.00000
tch        69.30617  119.322597  165.46347    0.000000   0.00000  51.93353
ltg       206.74238  276.466411  369.93742  283.107398 412.68911 556.78790
glu        61.07588  111.952224  168.07668    0.000000   0.00000  90.18665
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{medical-example}{%
\subsubsection{Medical example}\label{medical-example}}

Results from the publication
\href{https://www.archives-pmr.org/article/S0003-9993(20)31154-0/fulltext}{Skandsen
et al 2021} discussed in class.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bootstrap-cis-forbeta_j}{%
\subsection{\texorpdfstring{Bootstrap CIs
for\(\beta_j\)}{Bootstrap CIs for\textbackslash beta\_j}}\label{bootstrap-cis-forbeta_j}}

Sadly, there are two main challenges:

\begin{itemize}
\tightlist
\item
  The percentile interval is not a good choice for biased estimators,
  and it is not clear if the bias-corrected accelerated intervals are
  better
\item
  It has been shown that (for fixed \(p\)) the asymptotic
  (\(N\rightarrow \infty\)) distribution of the lasso has point mass at
  zero (which leads to that bootstrapping not having optimal
  properties).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The authors of the \emph{penalized package} take the following view
Section 6: A note on standard errors and confidence intervals in the
Penalized user manual
\url{https://cran.r-project.org/web/packages/penalized/vignettes/penalized.pdf}

``Unfortunately, in most applications of penalized regression it is
impossible to obtain a suﬃciently precise estimate of the bias. Any
bootstrap-based calculations can only give an assessment of the variance
of the estimates. Reliable estimates of the bias are only available if
reliable unbiased estimates are available, which is typically not the
case in situations in which penalized estimates are used.''

``It is certainly a mistake to make conﬁdence statements that are only
based on an assessment of the variance of the estimates, such as
bootstrap-based conﬁdence intervals do.''

Reliable conﬁdence intervals around the penalized estimates can be
obtained in the case of low dimensional models using the standard
generalized linear model theory as implemented in lm, glm and coxph.''

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{selective-inference}{%
\section{Selective inference}\label{selective-inference}}

\hypertarget{what-is-selective-inference}{%
\subsection{What is selective
inference?}\label{what-is-selective-inference}}

Selective inference is concerned with testing hypotheses suggested by
the data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{from-selective-inference-seminar}{%
\subsection{From selective inference
seminar}\label{from-selective-inference-seminar}}

Source: \url{https://sites.google.com/view/selective-inference-seminar}

Broadly construed, selective inference means \emph{searching for
interesting patterns in data}, usually with \emph{inferential guarantees
that account for the search process}.

It encompasses:

\begin{itemize}
\tightlist
\item
  Multiple testing: testing many hypotheses at once (and paying
  disproportionate attention to rejections)
\item
  Post-selection inference: examining the data to decide what question
  to ask, or what model to use, then carrying out one or more
  appropriate inferences
\item
  Adaptive / interactive inference: sequentially asking one question
  after another of the same data set, where each question is informed by
  the answers to preceding questions
\item
  Cheating: cherry-picking, double dipping, data snooping, data
  dredging, p-hacking, HARKing, and other low-down dirty rotten tricks;
  basically any of the above, but done wrong!
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-splitting}{%
\section{Sample splitting}\label{sample-splitting}}

\hypertarget{what-if-we-just-split-the-data-in-two}{%
\subsection{What if we just split the data in
two?}\label{what-if-we-just-split-the-data-in-two}}

\hypertarget{lasso---linear-or-logistic-regression}{%
\subsubsection{Lasso - linear or logistic
regression}\label{lasso---linear-or-logistic-regression}}

Dataset with \(p\) covariates and \(N\) observations. Divided into a
training set of size \(aN\) and a test set of \((1-a)N\), where
\(a \in [0,1]\).

\begin{itemize}
\item
  Training data used to decide on \(\lambda\) using CV - gives final
  model where some coefficients is set to 0 and some are shrunken. (The
  6 steps.)
\item
  Test data:

  \begin{itemize}
  \tightlist
  \item
    Fit ordinary LS or GLM model with \emph{only the non-zero lasso
    covariates}
  \item
    present CI and \(p\)-values.
  \end{itemize}
\end{itemize}

\textbf{Group discussion:} Is this ok? What is gained and what is lost?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{single-hypothesis-test}{%
\subsection{Single hypothesis test}\label{single-hypothesis-test}}

\[H_{0}\colon \beta_j=0 \hspace{0.5cm} \text{vs.} \hspace{0.5cm} H_{1}\colon \beta_j \neq 0\]

\begin{longtable}[]{@{}lll@{}}
\toprule()
& Not reject \(H_0\) & Reject \(H_0\) \\
\midrule()
\endhead
\(H_0\) true & Correct & Type I error \\
\(H_0\) false & Type II error & Correct \\
\bottomrule()
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  Two types of errors are possible, type I error and type II error.
\item
  A type I error would be to reject \(H_0\) when \(H_0\) is true, that
  is concluding that there is a linear association between the response
  and the predictor where there is no such association. This is called a
  \emph{false positive finding}.
\item
  A type II error would be to fail to reject \(H_0\) when the
  alternative hypothesis \(H_1\) is true, that is not detecting that
  there is a linear association between the response and the covariate.
  This is called a \emph{false negative finding}.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{p-value}{%
\subsection{\texorpdfstring{\(p\)-value}{p-value}}\label{p-value}}

\begin{itemize}
\tightlist
\item
  A \(p\)-value \(p(X)\) is a test statistic satisfying
  \(0 \leq p({\boldsymbol Y}) \leq 1\) for every vector of observations
  \(\boldsymbol{Y}\).
\item
  Small values give evidence that \(H_1\) is true.
\item
  In single hypothesis testing, if the \(p\)-value is less than the
  chosen significance level (chosen upper limit for the probability of
  committing a type I error), then we reject the null hypothesis,
  \(H_0\).
\item
  The chosen significance level is often referred to as \(\alpha\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

A \(p\)-value is \emph{valid} if
\[ P(p(\boldsymbol{Y}) \leq \alpha) \leq \alpha \] for all \(\alpha\),
\(0 \leq \alpha \leq 1\), whenever \(H_0\) is true, that is, if the
\(p\)-value is valid, rejection on the basis of the \(p\)-value ensures
that the probability of type I error does not exceed \(\alpha\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

An \emph{exact} \(p\)-value satisfies
\[P(p(\boldsymbol{Y}) \leq \alpha) = \alpha\] for all \(\alpha\),
\(0 \leq \alpha \leq 1\).

\begin{itemize}
\tightlist
\item
  The exact \(p\)-value is uniformly distributed when the null
  hypothesis is true.
\item
  This is a fact that is often misunderstood by users of \(p\)-values.
\item
  The incorrect urban myth is that \(p\)-values from true null
  hypotheses are close to one, when the correct fact is that all values
  in intervals of the same length are equally probable (which is a
  property of the uniform distribution).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{from-single-to-multiple-hypotheses}{%
\subsection{From single to multiple
hypotheses}\label{from-single-to-multiple-hypotheses}}

We test \(m\) hypotheses.

\begin{longtable}[]{@{}llll@{}}
\toprule()
& Not reject \(H_0\) & Reject \(H_0\) & Total \\
\midrule()
\endhead
\(H_0\) true & \(U\) & \(V\) & \(m_0\) \\
\(H_0\) false & \(T\) & \(S\) & \(m - m_0\) \\
Total & \(m-R\) & \(R\) & \(m\) \\
\bottomrule()
\end{longtable}

\begin{itemize}
\tightlist
\item
  Out of the \(m\) hypotheses tested, the (unknown) number of true null
  hypotheses is \(m_0\).
\item
  \(V\): the number of type I errors (false positive findings) and
\item
  \(T\): the number of type II errors (false negative findings).
\item
  \(U\): the number of true null hypotheses that are not rejected and
\item
  \(S\): the number of false null hypotheses that are rejected.
\item
  \(R\): the number of hypoteses rejected for a specific cut-off
\end{itemize}

\textbf{Question: which of these quantities do we observe for a specific
\(p\)-value cut-off?}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{familywise-error-rate}{%
\subsection{Familywise error rate}\label{familywise-error-rate}}

The familywise error rate (FWER) is defined as \emph{the probability of
one or more false positive findings}

\[ \text{FWER} = P(V > 0) \] The number of false positive findings \(V\)
is not known in a real life situation, but still we may find a cut-off
on the \(p\)-value, called \(\alpha_{\text loc}\), that gives an upper
limit to (controls) the FWER.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  Raw \(p\)-value, \(p_j\), the lowest nominal level to reject the null
  hypothesis.\\
\item
  Adjusted \(p\)-value, \(\tilde{p}_j\), is the nominal level of the
  multiple (simultaneous) test procedure at which
  \(H_{0j}, j=1,\ldots,m\) is just rejected, given the values of all
  test statistics involved.
\end{itemize}

The adjusted \(p\)-values can be defined as
\[\tilde{p}_j = \text{inf}\{\alpha  \mid H_{0j}\text{ is rejected at FWER level } \alpha \}\]

In a multiple testing problem where all adjusted \(p\)-value below
\(\alpha\) are rejected, the overall type I error rate (for example
FWER) will be controlled at level \(\alpha\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{the-bonferroni-method-controls-the-fwer}{%
\subsubsection{The Bonferroni method controls the
FWER}\label{the-bonferroni-method-controls-the-fwer}}

Single-step methods controls for multiple testing by estimating one
local significance level, \(\alpha_{\text{loc}}\), which is used as a
cut-off to detect significance for each individual test.

The Bonferroni method is valid for all types of dependence structures
between the test statistics.

The local significance level is \[\alpha_{\text loc}=\frac{\alpha}{m}\]

The adjusted \(p\)-value is \[ \tilde{p}_j =\min(1,m p_j)\]

Read more here if needed:
\href{https://www.math.ntnu.no/emner/TMA4267/2017v/multtest.pdf}{Short
note on multiple hypothesis testing}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{high-dimensional-inference-hdi}{%
\subsection{High-dimensional inference
(hdi)}\label{high-dimensional-inference-hdi}}

(Dezeure, Bühlmann, Meier, Meinshausen, 2.1.1 + 2.2)

\begin{itemize}
\tightlist
\item
  The article has focus on frequentist methods for high-dimensional
  inference with confidence intervals and \(p\)-values in linear and
  generalized linear models.
\item
  We will focus on linear models.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Set-up:}

\[Y={\boldsymbol X} \beta^0 +\varepsilon\]

\begin{itemize}
\tightlist
\item
  \({\boldsymbol X}\) is \(n\times p\) design matrix
\item
  \(Y\) is an \(n \times 1\) response vector
\item
  \(\varepsilon\) is an \(n \times 1\) error vector, independent of
  \({\boldsymbol X}\) and i.i.d. entries with
  \(\text{E}(\varepsilon_i)=0\).
\item
  The number of parameters \emph{may} be larger than the sample size
  (then the regression parameter is not identifiable in general).
\end{itemize}

The \emph{active set} is \[S_0=\{ j; \beta_j^0 \neq 0,j=1,\ldots,p\}\]
with cardinality (size) \(\lvert S_0 \rvert\).

\textbf{Now:} construct CI and \(p\)-values for \emph{individual
regression parameters} \(\beta_j^0\), \(j=1,\ldots, p\), and also with
multiple testing adjustment.

Remark: We want inference for \emph{all} coefficients - not only the
ones that the lasso has selected.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  The lasso has desirable properties for estimating \(\beta^0\) in high
  dimensional models, in particular for prediction
  \({\boldsymbol X}\beta^0\) or a new response \(Y_{\text{new}}\).
\item
  But, the distribution of the estimator is hard to characterize, and
\item
  it has been shown that (for fixed \(p\)) the asymptotic
  (\(n\rightarrow \infty\)) distribution of the lasso has point mass at
  zero (which leads to that bootstrapping not having optimal
  properties).
\end{itemize}

For the situation \(p >> n\) extra assumptions are needed, called the
\emph{compatibility condition} on the design matrix, and this guarantees
identifiability and socalled oracle optimality results for the lasso.
However, this is reported to be \emph{unrealistic in practical
situations}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{single-sample-splitting}{%
\subsection{Single-sample splitting}\label{single-sample-splitting}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Split the data into two (equal) halves, \(I_1\) and \(I_2\), no
  observations in common.
\item
  \(I_1\) is used for model selection (with the lasso), with active
  variables in \(\hat{S}(I_1)\).
\item
  The selected covariates in \(\hat{S}(I_1)\) is used for estimation in
  \(I_2\). To construct \(p\)-values, \(P_j\), for example use LS with
  \(t\)-tests. \(P\)-values for variables not selected is set to \(1\).
  Remark: then the number of covariates selected in \(I_1\) need be
  smaller than the sample size for \(I_2\).
\item
  The raw \(p\)-values is corrected for multiple testing (Bonferroni
  method controlling FWER)
  \[ P_{\text{corr},j}=\min(P_j \cdot \lvert \hat{S} \rvert,1)\]
\end{enumerate}

This avoids using the data twice! But, is very sensitive to the split -
giving \emph{wildly} different \(p\)-values=\(p\)-value lottery

How can this be amended?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{multiple-sample-splitting}{%
\subsection{Multiple-sample splitting}\label{multiple-sample-splitting}}

\begin{itemize}
\item
  The single-sample splitting routine is run \(B\) times giving
  \(P_{\text{corr},j}^{[b]}\) for \(b=1,\ldots,B\) and \(j=1,\ldots,p\).
\item
  Problem: how aggregate the \(B\) \(p\)-values for each \(j\) to give
  one \(p\)-value?
\item
  The different \(b\) runs have many observations in common, so the
  \(p\)-values for covariate \(j\) are correlated.
\item
  The authors have shown in a previous article that for dependent
  \(p\)-values that one solution (that gives valid \(p\)-values) is to
  take the median and multiply with 2.
\item
  The result is more general, and \(\gamma\) is a general quantile
  (\(\gamma=0.5\) for the median):
\end{itemize}

\[Q_k(\gamma)=\min(\text{empirical }\gamma- \text{quantile} \{ P_{\text{corr},j}^{[b]}/\gamma,b=1,\ldots, B\},1)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  The authors get more advanced and choose to search all \(\gamma\)
  within the interval \((\gamma_{\text{min}},1)\), where a common choice
  is \(\gamma_{\text{min}}=0.05\), to get the smallest \(p\)-value.
  However there is a price to pay: \((1-\log(\gamma_{\text{min}}))\)
\end{itemize}

\[ P_j=\min((1-\log(\gamma_{\text{min}})\cdot \inf_{\gamma \in (\gamma_{\text{min}},1)} Q_j(\gamma)),1)\]
for \(j=1,\ldots,p\).

Some assumptions are necessary to assure FWER control.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Confidence intervals are found by ``inversion''

\begin{itemize}
\tightlist
\item
  from the adjusted \(p\)-values \(P_j\)
\item
  using the duality of \(p\)-values and two-sided confidence intervals.
  That is, a \((1-\alpha)\) 100\% CI contains values \(c\) where the
  \(p\)-value is below \(\alpha\) for testing \(H_0: \beta_j=c\).
\item
  A closed form solution involving \(P_j\) is found.
\item
  Both single testing and multiple corrected testing CIs are found.
  (Appendix A.2 in article)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(diabetes)}
\NormalTok{x}\OtherTok{=}\FunctionTok{cbind}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{x)}\CommentTok{\#,diabetes$x2)}
\NormalTok{y}\OtherTok{=}\NormalTok{diabetes}\SpecialCharTok{$}\NormalTok{y}

\NormalTok{hdires}\OtherTok{=}\FunctionTok{multi.split}\NormalTok{(}\AttributeTok{x=}\NormalTok{x,}\AttributeTok{y=}\NormalTok{y,}\AttributeTok{B=}\DecValTok{1000}\NormalTok{,}\AttributeTok{fraction=}\FloatTok{0.5}\NormalTok{,}
                   \AttributeTok{ci.level=}\FloatTok{0.95}\NormalTok{, }\AttributeTok{model.selector=}\NormalTok{lasso.cv,}
                   \AttributeTok{classical.fit=}\NormalTok{lm.pval, }\AttributeTok{classical.ci=}\NormalTok{lm.ci,}
                   \AttributeTok{return.nonaggr =} \ConstantTok{FALSE}\NormalTok{, }\CommentTok{\#if not adj for multiple testing}
                   \AttributeTok{return.selmodels=}\ConstantTok{FALSE}\NormalTok{, }\CommentTok{\#just to have a look!}
                   \AttributeTok{verbose=}\ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{dput}\NormalTok{(hdires,}\StringTok{"hdires.dd"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hdires}\OtherTok{=}\FunctionTok{dget}\NormalTok{(}\StringTok{"hdires.dd"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(hdires)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "pval"             "pval.corr"        "pvals.nonaggr"    "ci.level"        
 [5] "lci"              "uci"              "gamma.min"        "sel.models"      
 [9] "method"           "call"             "clusterGroupTest"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#summary(hdires$pvals.nonaggr) \# if return.nonaggr=TRUE}
\NormalTok{hdires}\SpecialCharTok{$}\NormalTok{gamma.min}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 0.999 0.999 0.050 0.062 0.999 0.999 0.076 0.999 0.053 0.999
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\normalsize

\begin{verbatim}
    adjusted pvalue    lowerCI   upperCI
age    1.000000e+00       -Inf       Inf
sex    1.000000e+00 -435.36819 106.48904
bmi    3.537003e-10  370.71236 777.71218
map    1.525473e-02   63.76631 472.25384
tc     1.000000e+00       -Inf       Inf
ldl    1.000000e+00       -Inf       Inf
hdl    5.416138e-01 -411.95903  20.84983
tch    1.000000e+00 -764.83148 204.03679
ltg    5.982750e-08  312.01305 717.79228
glu    1.000000e+00 -332.40694 242.89069
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hdi-with-logistic-regression}{%
\subsubsection{hdi with logistic
regression}\label{hdi-with-logistic-regression}}

For modifications to the call to \texttt{mult.split} see the Appendix of
the master thesis of

\begin{itemize}
\tightlist
\item
  Martina Hall: \href{http://hdl.handle.net/11250/2453095}{``Statistical
  Methods for early Prediction of Cerebral Palsy based on Data from
  Computer-based Video Analysis''}.
\item
  Dag J. Kristiansen:
  \href{http://hdl.handle.net/11250/2624609}{``Detecting Neuronal
  Activity with Lasso Penalized Logistic Regression''}
\item
  Haris Fawad: '' Modelling Neuronal Activity using Lasso Regularized
  Logistic Regression''( Modelling Neuronal Activity using Lasso
  Regularized Logistic Regression) - also
  \href{https://github.com/harisf/neuro-lasso}{git repo neuro-lasso}.
\end{itemize}

\hypertarget{hdi---also-with-other-solutions}{%
\subsubsection{hdi - also with other
solutions}\label{hdi---also-with-other-solutions}}

In the \texttt{hdi} package also solutions for \emph{debiasing} the
lasso estimator is included. (See HTW 6.4 or the Dezure et al article.)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{summing-up}{%
\subsubsection{Summing up}\label{summing-up}}

What is the take home message from this ``Sample splitting'' story?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{inference-after-selection}{%
\section{Inference after selection}\label{inference-after-selection}}

(Taylor and Tibshirani, 2015 and HTW 6.3)

\hypertarget{the-plot}{%
\subsection{The plot}\label{the-plot}}

Let us leave the lasso for a while.

1980: small data sets, planned hypothesis to test ready before data
collected, no model selection. Only fit model and look at CI and
p-values.

After 1980: larger data sets and looking at data to give best model. New
challenge: \emph{how to do inference after selection}.

This is an important topic that is not a part of ANY statistical courses
at IMF.

The main question is:

\begin{itemize}
\tightlist
\item
  we have used a selection method (forward selection, lasso) to find
  potential association between covariates and response,
\item
  with focus on interpreting the selected model: how can we assess the
  strength (read: CI and \(p\)-value) of these findings?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The answer includes:

\begin{itemize}
\tightlist
\item
  we have ``cherry picked'' the strongest associations, and we can thus
  not just report CI and \(p\)-values based on the final model - when
  all is done on the same data set.
\end{itemize}

In this story we now focus on \emph{understanding how our model
selection influences the inference on the final model}.

The technical solutions are of less importance, and is not presented
with enough mathematical detail so that we understand the method in
detail.

\emph{Remark: the single and multiple sample splitting strategy is
valid.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{forward-stepwise-regression}{%
\subsection{Forward stepwise
regression}\label{forward-stepwise-regression}}

\textbf{Aim:} Multiple linear regression - where forward stepwise
regression is used to select the model

\[y_i=\beta_0+\sum_{j=1}^p x_{ij}\beta_j + \varepsilon_i\]

\begin{itemize}
\tightlist
\item
  Start with an empty model (only intercept)
\end{itemize}

While some stopping criterion not reach - perform step

\begin{itemize}
\tightlist
\item
  At step \(k\) add the predictor that gives the most decrease in the
  sums of squares of error (SSE)
\end{itemize}

\[ \text{SEE}=\sum_{i=1}^N (y_i -\hat{y}_i)^2\] where \(\hat{y}_i\) is
the predicted value for observation \(i\) in this model.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

If we have a model with \(k-1\) predictors and we would like to add one
more predictor (the \(k\) to be added):

\[ \text{R}_k=\frac{1}{\sigma^2}(\text{SSE}_{k-1}-\text{SSE}_{k})\sim \chi^2_1\]
where here \(\sigma^2\) is assumed known.

This can be seen as an hypothesis test:

\(H_0: \text{The new predictor is not relevant}\) vs.
\(H_1: \text{The new predictor is relevant}\) with a \(p\)-value
calculated from the upper tail of the \(\chi^2_1\)-distribution.

Why this test statistic? This is a special case of \(H_0: C\beta=d\),
see
\href{https://www.math.ntnu.no/emner/TMA4267/2017v/TMA4267V2017Part3.pdf}{part
3 of TMA4267 in 2017 (in particular L13+14)} where we now assume
\(\sigma^2\) to be known (not Fisher but \(\chi^2\)).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Alternative scenario:}

For simplicity - we look at the \(k=1\):

The order in which the predictors is to be entered in to the model was
decided before the data was collected (or at least before the data was
analysed or plottet).

Then: we are all good - and this \(p\)-value from the
\(\chi^2_1\)-distribution will be a valid \(p\)-value.

What does this mean in practice?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Back to original scenario:}

Assume we are at step \(k=1\), and will add the first predictor which is
the one with the largest \(R_1\).

Will the distribution of the \emph{maximal} \(R_1\) be the same as the
distribution of a given predefined \(R_1\)?

Distribution to the maximum given that we have \(p\) predictors:

\[ P(\max R_1 \ge c)=1-P(\max R_1 < c)=1-P(\text{all p } R_1 \text{ are} <c)\]

Study Figure 1 in the article for a plot of nominal vs.~actual
\(p\)-value for \(p=10\) and \(p=50\), and see below a possible R
reproduction.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{ecdf-of-p-values-under-the-null-for-first-step-of-forward-selection}{%
\subsubsection{\texorpdfstring{ECDF of \(p\)-values under the null for
first step of forward
selection}{ECDF of p-values under the null for first step of forward selection}}\label{ecdf-of-p-values-under-the-null-for-first-step-of-forward-selection}}

\includegraphics{W6_files/figure-pdf/unnamed-chunk-39-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Moving on to} \(k>1\)

\begin{itemize}
\tightlist
\item
  We would like to obtain valid (``correct'') \(p\)-values for all
  steps, not only for \(k=1\).
\item
  Monte Carlo solution would be elaborate.
\end{itemize}

The method used in the article is to calculate a \(p\)-value for the
covariate at step \(k\) by conditioning on the fact that the strongest
\(k-1\) predictors in this sequential set-up has already been chosen.

The \(p\)-value to be calculated at step \(k\) would be dependent on the
number of covariates \(p\).

We now change focus and look at the distribution of the estimated
regression coefficient for the covariate added at step \(k\), because
that can be used to construct both a CI for the coefficient and a
\(p\)-value for testing if the coefficient is different from zero.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{the-polyhedral-result}{%
\subsection{The polyhedral result}\label{the-polyhedral-result}}

(for details consult HTW 6.3 or articles references to in the Taylor and
Tibshirani article)

\textbf{Distribution for regression coefficient:}

\begin{itemize}
\tightlist
\item
  Assume that we are at some step \(k\), and that \(k-1\) covariates are
  in the model.
\item
  We have found the new covariate to include, and fitted the model with
  the \(k\) covariates.
\item
  Standard theory tells us that the estimator \(\hat{\beta}\) for
  covariate \(k\) is unbiased and follows a normal distribution with
  some variance \(\tau^2\).
\end{itemize}

\[ \hat{\beta} \sim N(\beta,\tau^2)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

But, this is given that we only had these \(k\) covariates available at
the start. We will instead \emph{condition on} selection event.

It turns out that the selection event can be written in a
\emph{polyhedral form} \(A y \le b\) for some matrix \(A\) and some
vector \(b\).

At each step of the forward selection we have a competition among all
\(p\) variables, and the \(A\) and \(b\) is used to construct the
competition.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The correct distribution of the estimator \(\hat{\beta}\) for covariate
now has a \emph{truncated normal distribution}

\[ \hat{\beta} \sim TN^{c,d}(\beta,\tau^2)\]

i.e.~the \emph{same} normal distribution, but scaled to lie within the
interval \((c,d)\).

The limits \((c,d)\) depends on both the data and the selection events
that lead to the current model.

\emph{The formulae for these limits are somewhat complicated but easily
computable}.

This truncated normal distribution is used to calculate
\emph{selection-adjusted} \(p\)-values and confidence interval.

(Study Figure 3 in Taylor and Tibshirani (2015).)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{ecdf-of-polyheder-p-values-under-the-null-for-first-step-of-forward-selection}{%
\subsubsection{\texorpdfstring{ECDF of polyheder \(p\)-values under the
null for first step of forward
selection}{ECDF of polyheder p-values under the null for first step of forward selection}}\label{ecdf-of-polyheder-p-values-under-the-null-for-first-step-of-forward-selection}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{ecdf-of-polyheder-p-values-under-the-null-for-first-step-of-forward-selection-1}{%
\subsubsection{\texorpdfstring{ECDF of polyheder \(p\)-values under the
null for first step of forward
selection}{ECDF of polyheder p-values under the null for first step of forward selection}}\label{ecdf-of-polyheder-p-values-under-the-null-for-first-step-of-forward-selection-1}}

\includegraphics{W6_files/figure-pdf/unnamed-chunk-41-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{comparing-simple-and-polyheder-p-values-for-the-first-step-of-forward-selection}{%
\subsubsection{\texorpdfstring{Comparing simple and polyheder
\(p\)-values for the first step of forward
selection}{Comparing simple and polyheder p-values for the first step of forward selection}}\label{comparing-simple-and-polyheder-p-values-for-the-first-step-of-forward-selection}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{W6_files/figure-pdf/unnamed-chunk-44-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{polyhedral-lasso-result}{%
\subsection{Polyhedral lasso result}\label{polyhedral-lasso-result}}

The same methodology can be used for the lasso, here also the selection
of predictors can be described as a polyhedral region of the form
\(Ay\le b\) - for a fixed value \(\lambda\).

For the lasso the \(A\) and \(b\) will depend on

\begin{itemize}
\tightlist
\item
  the predictors
\item
  the active set
\item
  \(\lambda\)
\end{itemize}

but not on \(y\).

The methods are on closed form, but the values \(c\) and \(d\) may be of
complicated form.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The R package \texttt{selectiveInference} can be used to find post
selection \(p\)-values both for forward stepwise selection and for the
lasso.

See the package help for details.

Study Figure 5 from the article for an example of Naive and
Selection-adjusted intervals for the lasso.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{selective-inference-with-the-diabetes-data}{%
\subsection{Selective inference with the diabetes
data}\label{selective-inference-with-the-diabetes-data}}

\hypertarget{forward-selection-diabetes}{%
\subsubsection{Forward selection
diabetes}\label{forward-selection-diabetes}}

\begin{verbatim}
      [,1] [,2] 
 [1,] "1"  "age"
 [2,] "2"  "sex"
 [3,] "3"  "bmi"
 [4,] "4"  "map"
 [5,] "5"  "tc" 
 [6,] "6"  "ldl"
 [7,] "7"  "hdl"
 [8,] "8"  "tch"
 [9,] "9"  "ltg"
[10,] "10" "glu"
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\includegraphics{W6_files/figure-pdf/unnamed-chunk-46-1.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{verbatim}

Call:
fsInf(obj = fsfit)

Standard deviation of noise (specified or estimated) sigma = 54.154

Sequential testing results with alpha = 0.100
 Step Var     Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea
    1   3  949.435  17.532   0.000   790.681 1037.113       0.049      0.050
    2   9  614.951  10.163   0.000   521.696  887.192       0.049      0.050
    3   4  262.275   4.291   0.010    90.437  363.617       0.049      0.048
    4   5 -206.670  -3.266   0.684  -279.583 1539.967       0.049      0.050
    5   2 -148.375  -2.648   0.689  -273.862 1234.380       0.050      0.050
    6   6  538.586   3.664   0.025   208.452 5364.275       0.050      0.050
    7   8  135.265   1.121   0.900      -Inf  577.340       0.000      0.050
    8  10   67.141   1.027   0.033   100.724      Inf       0.050      0.000
    9   7   99.718   0.470   0.629 -2450.846 1220.006       0.050      0.050
   10   1  -10.012  -0.168   0.644  -527.324 1058.916       0.050      0.050

Estimated stopping point from ForwardStop rule = 3
\end{verbatim}

\begin{verbatim}

Call:
fsInf(obj = fsfit, k = 3, type = "all")

Standard deviation of noise (specified or estimated) sigma = 54.154

Testing results at step = 3, with alpha = 0.100
 Var    Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea
   3 603.074   9.604   0.001   329.956  700.275       0.049      0.050
   9 543.872   8.669   0.000   444.878  816.142       0.048      0.050
   4 262.275   4.291   0.010    90.437  363.617       0.049      0.048
\end{verbatim}

\begin{verbatim}

Call:
fsInf(obj = fsfit, type = "aic")

Standard deviation of noise (specified or estimated) sigma = 54.154

Testing results at step = 6, with alpha = 0.100
 Var     Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea
   3  529.873   8.061   0.071  -101.266  618.999       0.050      0.049
   9  804.192  10.014   0.000   636.648 1316.723       0.050      0.050
   4  327.220   5.211   0.012   104.438  428.643       0.049      0.049
   5 -757.938  -4.716   0.016 -3805.607 -303.334       0.050      0.049
   2 -226.511  -3.778   0.495  -378.663  794.641       0.050      0.050
   6  538.586   3.664   0.025   208.452 5364.275       0.050      0.050

Estimated stopping point from AIC rule = 6
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

For comparison, the suggested forward model with variabls bmi, ltg and
map - with naive \(p\)-values.

\footnotesize

\begin{verbatim}

Call:
lm(formula = y ~ x[, 3] + x[, 9] + x[, 4])

Residuals:
     Min       1Q   Median       3Q      Max 
-140.229  -40.637   -2.187   38.269  139.804 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  152.133      2.653  57.342  < 2e-16 ***
x[, 3]       603.074     64.677   9.324  < 2e-16 ***
x[, 9]       543.872     64.619   8.417 5.56e-16 ***
x[, 4]       262.275     62.962   4.166 3.74e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 55.78 on 438 degrees of freedom
Multiple R-squared:  0.4801,    Adjusted R-squared:  0.4765 
F-statistic: 134.8 on 3 and 438 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\normalsize

\hypertarget{lasso-diabetes}{%
\subsubsection{Lasso diabetes}\label{lasso-diabetes}}

\footnotesize

\begin{verbatim}
[1] 0.2527843
\end{verbatim}

\begin{verbatim}
 [1]    0.00000  -33.33808  508.19096  210.35372    0.00000    0.00000
 [7] -138.84778    0.00000  444.56109    0.00000
\end{verbatim}

\begin{verbatim}

Call:
fixedLassoInf(x = x, y = y, beta = beta, lambda = lambda * n)

Standard deviation of noise (specified or estimated) sigma = 54.154

Testing results at lambda = 111.731, with alpha = 0.100

 Var     Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea
   2 -235.776  -3.913   0.117  -325.205   96.516       0.049      0.050
   3  523.562   8.047   0.000   416.203  631.275       0.049      0.049
   4  326.236   5.190   0.000   212.282  430.335       0.048      0.049
   7 -289.117  -4.420   0.003  -397.090 -136.813       0.049      0.050
   9  474.292   7.247   0.000   366.602  582.958       0.050      0.048

Note: coefficients shown are partial regression coefficients
\end{verbatim}

\begin{verbatim}
[1] 1.168127e-01 1.092168e-15 3.912618e-05 2.928151e-03 6.562529e-13
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\normalsize

\hypertarget{further-improvements}{%
\subsubsection{Further improvements}\label{further-improvements}}

The method yields rather wide confidence intervals for the regression
parameters (given that we translate the \(p\)-values into CIs).

There exists improvements to the results, in particular a method called
\emph{carving} which is explained in the you-tube videos from a course
with
\href{https://lsa.umich.edu/stats/people/faculty/psnigdha.html}{Snigdha
Paragrahi}

\begin{itemize}
\tightlist
\item
  \href{https://www.youtube.com/watch?v=qofrkW-DL7c\&t=3682s}{Tutorial
  I}
\item
  \href{https://www.youtube.com/watch?v=rGHf6BPeqBg\&t=1105s}{Tutorial
  II}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{posi}{%
\subsection{PoSI}\label{posi}}

(HTW 6.5)

\begin{itemize}
\tightlist
\item
  The POSI method also fits a selected submodel and
\item
  adjust the standard CIs by \emph{accounting for all possible models
  that might have been delivered by the selection procedure}.
\item
  This means the method can be used on published results where the
  complete selection process is not explained in detail.
\item
  This lack of information of the selection process leads to \emph{very
  wide CIs}.
\end{itemize}

Inference is based on the submodel \(M\) chosen, and on the projection
of \({\boldsymbol X} \beta\) onto the space spanned by the submodel
\(M\):

\[\beta_M=({\boldsymbol X}_M^T{\boldsymbol X}_M)^{-1}{\boldsymbol X}_M^T{\boldsymbol X} \beta\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The method considers a confidence interval for the \(j\)th element of
\(\beta_M\) of the form
\[\text{CI}_{jM}=\hat{\beta}_{jM}\pm K \hat{\sigma}v_{jM}\] where
\(v_{jM}^2=({\boldsymbol X}_M^T{\boldsymbol X}_M)^{-1}_{jj}\)

The constant \(K\) is found to satisfy
\[ P(\beta_{jM}\in \text{CI}_{jM}) \le 1-2\alpha\] \emph{over all
possible selection models}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\(K\) is a function of the data matrix \({\boldsymbol X}\) and the
maximum number of nonzero component allowed in \(\beta_M\). An upper
bound on \(K\) is known from a result on simultaneous intervals by
Scheffe.

HTW page 161: Reports on the diabetes data with submodels of size 5,
where the 95\% CI value of K is 4.42 (``little less than 2 hours of
computing'').

Details may be found in the PoSI 2013 article (reference Berk et al
below).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{posi-r-package}{%
\subsubsection{PoSI R-package}\label{posi-r-package}}

\emph{In linear LS regression, calculate for a given design matrix the
multiplier} \(K\) of coefficient standard errors such that the
confidence intervals {[}b - K\emph{SE(b), b + K}SE(b){]} have a
guaranteed coverage probability for all coefficient estimates b in any
submodels after performing arbitrary model selection.

Results for the Boston housing data is available in the help section.

\url{https://cran.r-project.org/web/packages/PoSI/index.html}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{post-selection-inference-and-the-reproducibility-crisis}{%
\subsection{Post selection inference and the reproducibility
crisis}\label{post-selection-inference-and-the-reproducibility-crisis}}

The \emph{incorrect} use of CIs and \(p\)-values in models found from
model selection \emph{and} inference on the same data - is though to be
one of the main contributors to the \emph{reproducibility crisis in
science}.

\href{https://hdsr.mitpress.mit.edu/pub/l39rpgyc/release/1}{Selective
Inference: The Silent Killer of Replicability by Yoav Benjamini
Published on Dec 16, 2020}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conclusion-1}{%
\section{Conclusion}\label{conclusion-1}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{how-will-you-perform-inference}{%
\subsection{How will you perform
inference}\label{how-will-you-perform-inference}}

on your data analysis roject?

We discuss:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  You can afford a test set
\item
  You have no test set
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{references}{%
\section{References}\label{references}}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-BootLasso2011}{}}%
Chatterjee, A., and S. N. Lahiri. 2011. {``Bootstrapping Lasso
Estimators.''} \emph{Journal of the American Statistical Association}
106 (494): 608--25. \url{http://www.jstor.org/stable/41416396}.

\leavevmode\vadjust pre{\hypertarget{ref-hdi2015}{}}%
Dezeure, Ruben, Peter Bühlmann, Lukas Meier, and Nicolai Meinshausen.
2015. {``High-Dimensional Inference: Confidence Intervals, p-Values and
{R}-Software {hdi}.''} \emph{Statistical Science} 30 (4): 533--58.

\leavevmode\vadjust pre{\hypertarget{ref-casi}{}}%
Efron, Bradley, and Trevor Hastie. 2016. \emph{Computer Age Statistical
Inference - Algorithms, Evidence, and Data Science}. Cambridge
University Press. \url{https://hastie.su.domains/CASI/}.

\leavevmode\vadjust pre{\hypertarget{ref-HTW}{}}%
Hastie, Trevor, Robert J. Tibshirani, and Martin Wainwright. 2015.
\emph{Statistical Learning with Sparsity: The Lasso and
Generalizations}. CRC Press.
\url{https://hastie.su.domains/StatLearnSparsity/}.

\leavevmode\vadjust pre{\hypertarget{ref-TaylorTibshirani2015}{}}%
Taylor, Jonathan, and Robert J. Tibshirani. 2015. {``Statistical
Learning and Selective Inference.''} \emph{Proceedings of the National
Academy of Sciences} 112 (25): 7629--34.
\url{https://doi.org/10.1073/pnas.1507583112}.

\leavevmode\vadjust pre{\hypertarget{ref-WNvW}{}}%
Wieringen, Wessel N. van. 2021. {``Lecture Notes on Ridge Regression.''}
\url{https://arxiv.org/pdf/1509.09169v7.pdf}.

\end{CSLReferences}



\end{document}
