---
title: "MA8701 Advanced methods in statistical inference and learning"
subtitle: "L10: Shrinkage methods for the GLM"
author: "Mette Langaas"
date: "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: ../Part1/references.bib
nocite: |
  @casi, @esl, @WNvW
format: 
  html: 
    toc: true
    code-fold: true
    toc-location: left
    toc-depth: 3
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
    keep-tex: true
  beamer:
    incremental: false
    aspectratio: 43
    navigation: frame
---

```{r}
#| eval: true
#| echo: false
#| warning: false
suppressPackageStartupMessages(library(knitr))
knitr::opts_chunk$set(echo = FALSE, message=FALSE,warning = FALSE, error = FALSE)
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(reticulate))
#reticulate::use_python("/usr/bin/python3",required=TRUE)
suppressPackageStartupMessages(library(ggpubr))
suppressPackageStartupMessages(library(cowplot))
suppressPackageStartupMessages(library(magick))
suppressPackageStartupMessages(library(pheatmap))
suppressPackageStartupMessages(library(GGally))
suppressPackageStartupMessages(library(bestglm))
suppressPackageStartupMessages(library(glmnet))
suppressPackageStartupMessages(library(nortest))
suppressPackageStartupMessages(library(knitr))
#suppressPackageStartupMessages(library(ggroc))
suppressPackageStartupMessages(library(kableExtra))
suppressPackageStartupMessages(library(corrplot))
#suppressPackageStartupMessages(library(monomvn))
#suppressPackageStartupMessages(library(HDCI))
suppressPackageStartupMessages(library(lars)) #data diabetes
#suppressPackageStartupMessages(library(hdi))
```

# Before we begin

## Literature

* [ELS] The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition (Springer Series in Statistics, 2009) by Trevor Hastie, Robert Tibshirani, and Jerome Friedman. [Ebook](https://web.stanford.edu/~hastie/Papers/ESLII.pdf). Chapter 4.4.1-4.4.3 (4.4.4 is covered in 3.2 of HTW).

* [HTW] Hastie, Tibshirani, Wainwright: "Statistical Learning with Sparsity: The Lasso and Generalizations". CRC press. [Ebook](https://hastie.su.domains/StatLearnSparsity/). Chapter 3.2,3.7, 5.4.3

and for the interested student

* [WNvW] [Wessel N. van Wieringen: Lecture notes on ridge regression](https://arxiv.org/pdf/1509.09169v7.pdf) Chapter 5.

---

Supplemental sources useful for week 6 (see also the section on "Preparing for inference for the lasso and ridge")

* [Bootstrap confidence intervals in the master thesis of Lene Tillerli Omdal Section 3.6.2](https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/3026839) and teaching material from TMA4300 - see the wikipage for that course.
* [Short note on multiple hypothesis testing](https://www.math.ntnu.no/emner/TMA4267/2017v/multtest.pdf) in TMA4267 Linear Statistical Models, Kari K. Halle, Øyvind Bakke and Mette Langaas, March 15, 2017. 

---

<!-- # Multiple imputation combined with shrinkage methods -->

<!-- ## First case -->

<!-- Assume that the aim is prediction, and only this is studied for model assessment (no need for hypothesis tests or confidence intervals for regression parameters). -->

<!-- Further assume that there is enough data to set aside a test set for model assessment, maybe 20-80 split. -->

<!-- How is it now possible to combine multiple imputation and lasso/ridge type methods on the training set? -->

<!-- "Tja" - if Rubins rules could be used on the predictions on the test set? -->

<!-- --- -->

<!-- ## Second case -->

<!-- Assume that the aim is model selection (find the important covariates) and that inference about the fitted coefficients is of interest. -->

<!-- We will in W6 learn how to used bootstrapping to provide a confidence interval for the regression coefficients, such that there needs to be a bootstrap loop. -->

<!-- How can now multiple imputation, lasso/ridge type method (which need bootsrapping) be combined? -->

<!-- --- -->

<!-- ## Short review of a few solution -->

<!-- Warning: this is an area of research with few good solutions! Aka _"a fertile ground for further research"_ -->

<!-- @ZhaoLong2017: "Variable selection in the presence of missing data: imputation-based methods" lists three stragegies. (NB: here model assessment is not discussed, only model selection) -->

<!-- 1) Apply existing variable selection methods to each imputed dataset and then combine variable selection results across all imputed datasets. -->

<!-- This is similar to what I presented for the suicide crisis syndrome data set, and is presented in @vanBuuren Section 5.4 Stepwise model selection, in particular 5.4.2 for the D1 method that is implemented in the MICE R package.  -->

<!-- It is not clear how this should be done if the model selection method is a lasso variant. Could the penalty parameter be decided using Rubins rules and then used to fit lasso on each data set again and would then the same model be selected for each imputed data set?  -->

<!-- 2) Apply existing variable selection methods to stacked imputed datasets. -->

<!-- 3) Combine resampling techniques (such as bootstrap) with imputation -->

<!-- ## MI and model assessment -->

<!-- If ridge regression is used, then the same active set is used for each of the imputed data sets, and in addition we know that the parameter estimates are normally distributed (however biased). This means that model selection is not performed (except for hyperparameter tuning which is done using cross-validation for data set). Is it wise to use Rubin´s rules for the ridge regression coefficients when these are biased? -->


<!-- When searching for literature: some may use ridge/lasso for the imputation model in the multiple imputation, but we are interested in using ridge/lasso as the analysis model -->


# Generalized linear models
(HTW 3.1, 3.2, and TMA4315 GLM background)

## The model
The GLM model has three ingredients: 

1) Random component
2) Systematic component
3) Link function

We look into that for the binomial distribution - to get multiple linear regression and logistic regression.

* Write in class

---

## Explaining $\beta$ in logistic regression

* The ratio $\frac{P(Y_i=1)}{P(Y_i=0)}=\frac{\pi_i}{1-\pi_1}$ is called the _odds_. 

* If $\pi_i=\frac{1}{2}$ then the odds is $1$, and if $\pi_i=\frac{1}{4}$ then the odds is $\frac{1}{3}$. We may make a table for probability vs. odds in R:

```{r}
pivec=seq(0.1,0.9,0.1)
odds=pivec/(1-pivec)
kable(t(data.frame(pivec,odds)),digits=c(2,2))%>%
  kable_styling()
```

* Odds may be seen to be a better scale than probability to represent chance, and is used in betting. In addition, odds are unbounded above. 

---

We look at the link function (inverse of the response function). Let us assume that our linear predictor has $k$ covariates present

\begin{align*}
\eta_i&= \beta_0+\beta_1 x_{i1}+\beta_2 x_{i2}+\cdots + \beta_k x_{ik}\\
\pi_i&= \frac{\exp(\eta_i)}{1+\exp(\eta_i)}\\
\eta_i&=\ln(\frac{\pi_i}{1-\pi_i})\\
\ln(\frac{\pi_i}{1-\pi_i})&=\beta_0+\beta_1 x_{i1}+\beta_2 x_{i2}+\cdots + \beta_k x_{ik}\\
\frac{\pi_i}{1-\pi_i}=&\frac{P(Y_i=1)}{P(Y_i=0)}=\exp(\beta_0)\cdot \exp(\beta_1 x_{i1})\cdots\exp(\beta_k x_{ik})
\end{align*}

We have a _multiplicative model_ for the odds. 

---

**So, what if we increase $x_{1i}$ to $x_{1i}+1$?**

If the covariate $x_{1i}$ increases by one unit (while all other covariates are kept fixed) then the odds is multiplied by $\exp(\beta_1)$:

\begin{align*}
\frac{P(Y_i=1\mid x_{i1}+1)}{P(Y_i=0)\mid x_{i1}+1)}&=\exp(\beta_0)\cdot \exp(\beta_1 (x_{i1}+1))\cdots\exp(\beta_k x_{ik})\\
&=\exp(\beta_0)\cdot \exp(\beta_1 x_{i1})\exp(\beta_1)\cdots\exp(\beta_k x_{ik})\\
&=\frac{P(Y_i=1\mid x_{i1})}{P(Y_i=0\mid x_{i1})}\cdot \exp(\beta_1)\\
\end{align*}

This means that if $x_{i1}$ increases by $1$ then: if $\beta_1<0$ we get a decrease in the odds, if $\beta_1=0$ no change, and if $\beta_1>0$ we have an increase.
In the logit model $\exp(\beta_1)$ is easier to interpret than $\beta_1$.

---

The response function as a function of the covariate $x$ and not of $\eta$. Solid lines: $\beta_0=0$ and $\beta_1$ is $0.8$ (blue), $1$ (red) and $2$ (orange), and dashed lines with $\beta_0=1$.

```{r}
ggplot(data.frame(x=c(-6,5)), aes(x))+
  xlab(expression(x))+ 
  ylab(expression(mu))+
    stat_function(fun=function(x) exp(x)/(1+exp(x)), geom="line", colour="red")+
    stat_function(fun=function(x) exp(2*x)/(1+exp(2*x)), geom="line", colour="orange")+
          stat_function(fun=function(x) exp(0.8*x)/(1+exp(0.8*x)), geom="line", colour="blue")+
    stat_function(fun=function(x) exp(1+x)/(1+exp(1+x)), geom="line", colour="red",linetype="dashed")+
    stat_function(fun=function(x) exp(1+2*x)/(1+exp(1+2*x)), geom="line", colour="orange",linetype="dashed")+
          stat_function(fun=function(x) exp(1+0.8*x)/(1+exp(1+0.8*x)), geom="line", colour="blue",linetype="dashed")+
  scale_colour_manual("0+k x",values = c("red", "orange","blue"),labels=c("1","2","0.8"))
```

---

## Parameter estimation

First logistic regression, then ridge and lasso logistic regression - and (maybe) elastic net logistic regression.

### Logistic regression

* Maximum likelihood estimation = maximize the likelihood of the data. We write for the loglikelihood ${\cal{l}}(\beta_0,\beta; {\boldsymbol y}, {\boldsymbol X})$.

* We write out the loglikelihood for the binomial with logit link =logistic regression. 

---

### Algorithms

To understand the ridge and lasso logistic regression we first look at the _iteratively reweighted least squares_ (IRLS) - as a result of the Newton Raphson method for the logistic regression (unpenalized).

### Properties

The parameter estimator is asymptotically normal. Unbiased with variance the inverse of the Fisher information matrix - as known TMA4315.

---

In class we now scroll down to the South African data set and look at the data and the logistic regression.

---

### Penalized logistic regression

* For penalized method we instead minimize the negative loglikelihood scaled with $\frac{1}{N}$.

* The ridge and lasso penalty is added to the scaled negative loglikelihood.

* Write in class

---

### Algorithms

* The likelihood for the GLM is differentiable, and the ridge and lasso objective functions are convex - and can be solved with socalled "standard convex optimization methods". 
* But, by popular demand also special algorithms are available - building on the cyclic coordinate descent.

### Ridge logistic regression IRWLS 

---


### Lasso logistic regression fitting algoritm
(HTW page 116)

```{r,echo=TRUE,eval=FALSE}
OUTER LOOP: start with lambdamax and decrement

      MIDDLE LOOP (with warm start) 
         
         compute quadratic approximation 
         for current beta-estimates
         
              INNER LOOP: cyclic coordinate descent
              to minimize quadratic approximation 
              added the lasso penalty
```            

---         

### Criteria for choosing $\lambda$

We use cross-validation to choose $\lambda$.

For regression we choose $\lambda$ by minimizing the (mean) squared error.

For (ridge and) lasso logistic regression we may choose:

* misclassification error rate on the validation set 
* ROC-AUC or PR-AUC
* binomial deviance

---

# Evaluation metrics

## Confusion matrix, sensitivity, specificity 
(from TMA4268)

In a two class problem - assume the classes are labelled "-" (non disease,0) and "+" (disease,1). In a population setting we define the following event and associated number of observations.

|  |Predicted -|Predicted + |Total|
|:-------|:--------------|:---------------|:----------|
|True - | True Negative TN  | False Positive FP | N|
|True +|False Negative FN|True Positive TP|P|
|Total|N*|P*| |

(N in this context not to be confused with our sample size...)

---

**Sensitivity** (recall) is the proportion of correctly classified positive observations: $\frac{\# \text{True Positive}}{\# \text{Condition Positive}}=\frac{\text{TP}}{\text{P}}$. 

**Specificity** is the proportion of correctly classified negative observations: $\frac{\# \text{True Negative}}{\# \text{Condition Negative}}=\frac{\text{TN}}{\text{N}}$.

We would like that a classification rule have both a high sensitivity and a high specificity.

---

Other useful quantities:

| Name | Definition | Synonoms |
|:-----------------|:---------|:-----------------|
|False positive rate | FP/N| Type I error, 1-specificity|
|True positive rate|TP/P|1-Type II error, power, sensitivity, recall|
|Positive predictive value (PPV) |TP/P*|Precision, 1-false discovery proportion|
|Negative predictive value (NPV)| TN/N*| |

Where the PPV can be used together with the sensitivity to make a precision-recall curve more suitable for low case rates.

---

## ROC curves 
(also from TMA4268)

The receiver operating characteristics (ROC) curve gives a graphical display of the sensitivity against specificity, as the threshold value (cut-off on probability of success or disease) is moved over the range of all possible values. 
An ideal classifier will give a ROC curve which hugs the top left corner, while a straight line represents a classifier with a random guess of the outcome. 

---

## ROC-AUC 

* The **ROC-AUC** score is the area under the ROC curve. It ranges between the values 0 and 1, where a higher value indicates a better classifier. 
<!-- An AUC score equal to 1 would imply that all observations are correctly classified.  -->
* The AUC score is useful for comparing the performance of different classifiers, as all possible threshold values are taken into account.
* The ROC-AUC is closely connected to the robust U statistics.
* If the prevalence (case proportion) is very low (0.01ish), the ROC-AUC may be misleading, and the PR-AUC is more commonly used.

---

## ROC-PR

To be added.

---

## Deviance

The _deviance_ is based on the likelihood ratio test statistic. 

The derivation assumes that data can be grouped into covariate patterns, with $G$ groups (else interval solutions are used in practice).

**Saturated model:**
If we were to provide a perfect fit to our data then we would estimate $\pi_j$ by the observed frequency for the group, $\hat{y}_j=y_j$.

**Candidate model:** the model with the current choice of $\lambda$.

$$D_{\lambda}=2(l(\text{saturated model})-l(\text{candidate model}_{\lambda}))$$
The **null deviance** is replacing the candidate model with a model where $\hat{y}_i=\frac{1}{N}\sum_{i=1}^N y_i$ (the case proportion).

---

# Example: South African heart disease
(ELS 4.4.2)

## Group discussion

Comment on what is done and the results. Where are the CIs and $p$-values for the ridge and lasso version?

---

## Data set

The data is presented in ELS Section 4.4.2, and downloaded from <http://statweb.stanford.edu/~tibs/ElemStatLearn.1stEd/> with information in the file `SAheat.info` and data in `SAheart.data`.

* This is a retrospective sample of males in a heart-disease high-risk region in South Africa. 
* It consists of 462 observations on the 10 variables. All subjects are male in the age range 15-64. 
* There are 160 cases (individuals who have suffered from a conorary heart disease) and 302 controls (individuals who have not suffered from a conorary heart disease).    
* The overall prevalence in the region was 5.1%.

---

The response value (`chd`) and covariates

* `chd` : conorary heart disease \{yes, no\} coded by the numbers \{1, 0\}
* `sbp` : systolic blood pressure  
* `tobacco` : cumulative tobacco (kg)  
* `ldl` : low density lipoprotein cholesterol
* `adiposity` : a numeric vector
* `famhist` : family history of heart disease. Categorical variable with two levels: \{Absent, Present\}.
* `typea` : type-A behavior
* `obesity` : a numerical value
* `alcohol` : current alcohol consumption
* `age` : age at onset

_The goal is to identify important risk factors._ 

---

## Data description

We start by loading and looking at the data:

```{r,echo=TRUE}
ds=read.csv("./SAheart.data",sep=",")[,-1]
ds$chd=as.factor(ds$chd)
ds$famhist=as.factor(ds$famhist)
dim(ds)
colnames(ds)
head(ds)

# to be easier to compare with lasso and ridge, we standardize the xs
xs=model.matrix(chd~.,data=ds)[,-1] # to take care of categorical variables, but not include the intercept column
xss=scale(xs)
ys=as.numeric(ds[,10])-1 # not factor, must be numeric else errors...
head(xss)
table(ys)

dss=data.frame(ys,xss)
colnames(dss)[1]="chd"
apply(dss,2,sd)
apply(dss,2,mean)
```

The coloring is done according to the response variable, where green represents a case $Y=1$ and red represents a control $Y=0$.

---

```{r, warning=FALSE, message=FALSE}
ggpairs(ds, ggplot2::aes(color=chd), #upper="blank",  
        lower = list(continuous = wrap("points", alpha = 0.3, size=0.2)))
```

---

```{r, warning=FALSE, message=FALSE}
corrplot(cor(xss),type="upper")
```

**Q:** Comment on the correlation between covariates, and what that may lead to?

---

## Logistic regression

We now fit a (multiple) logistic regression model using the `glm` function and the full data set. In order to fit a logistic model, the `family` argument must be set equal to `="binomial"`. The `summary` function prints out the estimates of the coefficients, their standard errors and z-values. As for a linear regression model, the significant coefficients are indicated by stars where the significant codes are included in the `R` printout.

---

```{r,echo=TRUE}
glm_heart = glm(chd~.,data=dss, family="binomial")
summary(glm_heart)
exp(coef(glm_heart))
```

---

A very surprising result here is that `sbp` and `obesity` are NOT significant and `obesity` has negative sign. This is a result of the correlation between covariates. In separate models with only `sbp` or only `obesity` each is positive and significant. 

**Q:** How would you interpret the estimated coefficient for `tobacco`?

---

## Ridge logistic regression

```{r,echo=TRUE}
ridgefit=glmnet(x=xss,y=ys,alpha=0,standardize=FALSE,family="binomial") # already standardized
plot(ridgefit,xvar="lambda",label=TRUE)
```

---

```{r}
cv.ridge=cv.glmnet(x=xss,y=ys,alpha=0,standardize=FALSE,family="binomial")
print(paste("The lamda giving the smallest CV error",cv.ridge$lambda.min))
print(paste("The 1sd err method lambda",cv.ridge$lambda.1se))

plot(cv.ridge)
```

---

```{r}
# use 1sd error rule default
plot(ridgefit,xvar="lambda",label=TRUE);
abline(v=log(cv.ridge$lambda.1se));
```

---

```{r}
print(cbind(coef(ridgefit,s=cv.ridge$lambda.1se),coef(glm_heart)))
# now possible to compare since the glm was also on standardized variables
```

---

## Lasso logistic regression

Numbering in plots is order of covariates, so:


```{r,echo=TRUE}
cbind(1:9,colnames(xss))

lassofit=glmnet(x=xss,y=ys,alpha=1,standardize=FALSE,family="binomial") # already standardized
```

---

```{r}
plot(lassofit,xvar="lambda",label=TRUE)
```

---

```{r}
cv.lasso=cv.glmnet(x=xss,y=ys,alpha=1,standardize=FALSE,,family="binomial")
print(paste("The lamda giving the smallest CV error",cv.lasso$lambda.min))
print(paste("The 1sd err method lambda",cv.lasso$lambda.1se))

plot(cv.lasso)
```

---

```{r}
# use 1sd error rule default
plot(lassofit,xvar="lambda",label=TRUE);
abline(v=log(cv.lasso$lambda.1se));

```

---

```{r}
resmat=cbind(coef(lassofit,s=cv.lasso$lambda.1se),coef(ridgefit,s=cv.ridge$lambda.1se),coef(glm_heart))
colnames(resmat)=c("lasso","ridge","logistic")
print(resmat)
```

---

## Elastic net logistic regression


```{r,echo=TRUE}
cbind(1:9,colnames(xss))

elfit=glmnet(x=xss,y=ys,alpha=0.5,standardize=FALSE,family="binomial") # already standardized
```

---

```{r}
plot(elfit,xvar="lambda",label=TRUE)
```

---

```{r}
cv.el=cv.glmnet(x=xss,y=ys,alpha=1,standardize=FALSE,,family="binomial")
print(paste("The lamda giving the smallest CV error",cv.el$lambda.min))
print(paste("The 1sd err method lambda",cv.el$lambda.1se))

plot(cv.el)
```

---

```{r}
# use 1sd error rule default
plot(elfit,xvar="lambda",label=TRUE);
abline(v=log(cv.el$lambda.1se));

```

---

```{r}
resmat=cbind(coef(elfit,s=cv.el$lambda.1se),coef(lassofit,s=cv.lasso$lambda.1se),coef(ridgefit,s=cv.ridge$lambda.1se),coef(glm_heart))
colnames(resmat)=c("elactic", "lasso","ridge","logistic")
print(resmat)
```

# Computational details for the glmnet 
(HTW 3.7)

`glmnet` is the implementation in R of the elastic net from  HTW-book, and the package is maintained by Trevor Hastie.

The package fits generalized linear models using penalized maximum likelihood of elastic net type (lasso and ridge are special cases).

The logistic lasso is fitted using a quadratic approximation for the
negative log-likelihood in a "proximal-Newton iterative approach".

## Software links

* [R  glmnet on CRAN](https://cran.r-project.org/web/packages/glmnet/index.html)
with [resources](http://www.stanford.edu/~hastie/glmnet).
   + [Getting started](https://glmnet.stanford.edu/articles/glmnet.html)
   + [GLM with glmnet](https://glmnet.stanford.edu/articles/glmnetFamily.html)

---

For Python there are different options. 

* [Python glmnet](https://web.stanford.edu/~hastie/glmnet_python/) is recommended by Hastie et al.
* [scikit-learn](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification) (seems to mostly be for regression? is there lasso for classification here?)

---

## glmnet inputs

```{r,eval=FALSE,echo=TRUE}
glmnet(x, y, 
 family = c("gaussian", "binomial", "poisson", "multinomial","cox", "mgaussian"),
 weights = NULL, offset = NULL, alpha = 1, nlambda = 100, 
 lambda.min.ratio = ifelse(nobs < nvars, 0.01, 1e-04),
 lambda = NULL, standardize = TRUE, intercept = TRUE,
 thresh = 1e-07, dfmax = nvars + 1, 
 pmax = min(dfmax * 2 + 20, nvars), 
 exclude = NULL, penalty.factor = rep(1, nvars),
 lower.limits = -Inf, upper.limits = Inf, maxit = 1e+05,
 type.gaussian = ifelse(nvars < 500, "covariance", "naive"),
 type.logistic = c("Newton", "modified.Newton"),
 standardize.response = FALSE, 
 type.multinomial = c("ungrouped","grouped"), 
 relax = FALSE, trace.it = 0, ...)
```

---

## cv.glmnet inputs

```{r,eval=FALSE,echo=TRUE}
cv.glmnet(x, y, weights = NULL, offset = NULL, lambda = NULL,
  type.measure = c("default", "mse", "deviance", "class", "auc", "mae","C"),
  nfolds = 10, foldid = NULL, 
  alignment = c("lambda", "fraction"), grouped = TRUE, 
  keep = FALSE, parallel = FALSE,
  gamma = c(0, 0.25, 0.5, 0.75, 1), relax = FALSE, trace.it = 0, ...)
```

type.measure defaults to deviance (accoring to help(cv.glmnet)). The last is for Cox models.

---

### Family

we have only covered `gaussian` (the default) and `binomial`.

Each family has implemented the deviance measure. Poisson regression and Cox proportional hazard (survival analysis) is also implemented in glmnet.

---

### Penalties

The elastic net is implemented, with three possible adjustment parameters.

$$ \text{minimize}_{\beta_0,\beta} \{ -\frac{1}{N} l(y;\beta_0,\beta)+\lambda \sum_{j=1}^p
\gamma_j ((1-\alpha)\beta_j^2+\alpha \lvert \beta_j \rvert)\}$$

* $\lambda$: the penalty, default a grid of 100 values is chosen, to cover the lasso path on the log scale.
* $\alpha$: elastic net parameter $\in [0,1]$. This is usually manually selected by a grid search over 3-5 values. Default is $\alpha=1$ (lasso), and with $\alpha=0$ we get ridge.
* $\gamma_j$: penalty modifier for each covariate to be able to always include ($\gamma_j==0$), or exclude ($\gamma_j=\text{Inf}$), or give individual penalty modifications. Default $\lambda_j=1$.

---

For the $\lambda$ penalty the maximal value is for

* linear regression: $\lambda_{\text max}=\text{max}_j \lvert \hat{\beta}_{LS,j} \rvert$ (standardized coefficients) or, should there also be a factor 1/N?
* logistic regression: $\lambda_{\text max}=\text{max}_{j}\lvert {\boldsymbol x}_j ^T ({\boldsymbol y}-\bar{p}) \rvert$ where $\bar p$ is the mean case rate.


### Additional modifications

* Coefficient bounds can be set (possible since coordinate descent is used)
* Some coefficients can be excluded from the penalization (than thus forced in).
* Offset can be added (popular if rate models for Poisson is used)
* For binary and multinomial data factors or matrices can be input.
* Sparse matrices with covariates can be supplied.

---

## Lasso variants

Elastic net is already in glmnet (alpha-parameter).

Other lasso variants have their own R packages:

* The group lasso <https://cran.r-project.org/web/packages/grplasso/grplasso.pdf>
* The fused lasso <https://cran.r-project.org/web/packages/genlasso/genlasso.pdf>
* The sparse group lasso <https://arxiv.org/pdf/2208.02942> and <https://cran.r-project.org/web/packages/sparsegl/vignettes/sparsegl.html>

* Bayesian lasso blasso function for normal data in package monomvn <https://rdrr.io/cran/monomvn/man/monomvn-package.html>

* Elastic net for ordinal data: <https://cran.r-project.org/web/packages/ordinalNet/ordinalNet.pdf>


# Preparing for inference for the lasso and ridge

## Confidence interval

**Set-up**

* We have a random sample $Y_1,Y_2,\ldots,Y_N$ from 
* some distribution $F$ with some (unkonwn) parameter $\theta$. 
* Let $y_1,y_2,\ldots,y_N$ be the observed values for the random sample.

**Statistics**

* We have two statistics $\hat{\theta}_L(Y_1,Y_2,\ldots,Y_N)$ and $\hat{\theta}_U(Y_1,Y_2,\ldots,Y_N)$ so that 

$$P(\hat{\theta}_L(Y_1,Y_2,\ldots,Y_N)\le \theta \le \hat{\theta}_U(Y_1,Y_2,\ldots,Y_N))=1-\alpha$$
where $\alpha\in [0,1]$

---

**Confidence interval**

The numerical interval 
$$[\hat{\theta}_L(y_1,y_2,\ldots,y_N),\hat{\theta}_U(y_1,y_2,\ldots,y_N)]$$
is called a $(1-\alpha)$ 100% confidence interval.

## Bootstrap confidence intervals

## Percentile interval

## Bias corrected accelerated interval

---

## Single hypothesis test

$$H_{0}\colon \beta_j=0 \hspace{0.5cm} \text{vs.} \hspace{0.5cm} H_{1}\colon \beta_j \neq 0$$

| | Not reject $H_0$ | Reject $H_0$|
|:-------|:--------------|:---------------|
| $H_0$ true  |  Correct | Type I error|
|$H_0$ false  | Type II error | Correct|

---

* Two types of errors are possible, type I error and type II error. 
* A type I error would be to reject $H_0$ when $H_0$ is true, that is concluding that there is a linear association between the response and the predictor where there is no such association. This is called a _false positive finding_.
* A type II error would be to fail to reject $H_0$ when the alternative hypothesis $H_1$ is true, that is not detecting that there is a linear association between the response and the covariate. This is called a _false negative finding_.


---

## $p$-value
 
* A $p$-value $p(X)$ is a test statistic satisfying $0 \leq p({\boldsymbol Y}) \leq 1$ for every vector of observations $\boldsymbol{Y}$. 
* Small values give evidence that $H_1$ is true. 
* In single hypothesis testing, if the $p$-value is less than the chosen significance level (chosen upper limit for the probability of committing a type I error), then we reject the null hypothesis, $H_0$. 
* The chosen significance level is often referred to as $\alpha$.
 
---
 
A $p$-value is _valid_ if
$$ P(p(\boldsymbol{Y}) \leq \alpha) \leq \alpha $$
for all $\alpha$, $0 \leq \alpha \leq 1$, whenever $H_0$ is true, that is, if the $p$-value is valid, rejection on the basis of the $p$-value ensures that the probability of type I error does not exceed $\alpha$. 

---

An _exact_ $p$-value satisfies
$$P(p(\boldsymbol{Y}) \leq \alpha) = \alpha$$
for all $\alpha$, $0 \leq \alpha \leq 1$.

* The exact $p$-value is uniformly distributed when the null hypothesis is true. 
* This is a fact that is often misunderstood by users of $p$-values. 
* The incorrect urban myth is that $p$-values from true null hypotheses are close to one, when the correct fact is that all values in intervals of the same length are equally probable (which is a property of the uniform distribution).

---

## From single to multiple hypotheses

In many situations we are not interested in testing only one hypothesis, but instead $m$ hypotheses. 

| | Not reject $H_0$ | Reject $H_0$| Total |
|:-------|:--------------|:---------------|:------|
| $H_0$ true  |  $U$ | $V$| $m_0$|
|$H_0$ false  | $T$ | $S$ |$m - m_0$|
| Total | $m-R$ | $R$ | $m$ |

* Out of the $m$ hypotheses tested, the (unknown) number of true null hypotheses is $m_0$. 
* $V$: the number of type I errors (false positive findings) and 
* $T$: the number of type II errors (false negative findings). 
* $U$: the number of true null hypotheses that are not rejected and 
* $S$: the number of false null hypotheses that are rejected.
* $R$: the number of hypoteses rejected for a specific cut-off

Observe: only $m$ and $R$ is observed!

---

## Familywise error rate

The familywise error rate (FWER) is defined as _the probability of one or more false positive findings_

$$ \text{FWER} = P(V > 0) $$
The number of false positive findings $V$ is not known in a real life situation, but still we may find a cut-off on the $p$-value, called $\alpha_{\text loc}$, that gives an upper limit to (controls) the FWER.

---

* Raw $p$-value, $p_j$, the lowest nominal level to reject the null hypothesis.  
* Adjusted $p$-value, $\tilde{p}_j$, is the nominal level of the multiple (simultaneous) test procedure at which $H_{0j}, j=1,\ldots,m$ is just rejected, given the values of all test statistics involved. 

The adjusted $p$-values can be defined as 
$$\tilde{p}_j = \text{inf}\{\alpha  \mid H_{0j}\text{ is rejected at FWER level } \alpha \}$$

In a multiple testing problem where all adjusted $p$-value below $\alpha$ are rejected, the overall type I error rate (for example FWER) will be controlled at level $\alpha$.
 
---

## The Bonferroni method controls the FWER

Single-step methods controls for multiple testing by estimating one local significance level, $\alpha_{\text{loc}}$, which is used as a cut-off to detect significance for each individual test.

The Bonferroni method is valid for all types of dependence structures between the test statistics. 

The local significance level is $$\alpha_{\text loc}=\frac{\alpha}{m}$$

The adjusted $p$-value is
$$ \tilde{p}_j =\min(1,m p_j)$$


# Exercises

This week the best way to spend the time is to work on the Data Analysis Project 1.

But, also good to study the R-code for the South African heart disease example, and make some changes.

**Smart:** save this file as an .Rmd file and then run `purl(file.Rmd)` to produce a file with only the R-commands. (At the html-version you choose Code-Download Rmd on the top of the file).

* Change the CV criterion to auc and to class. Are there changes to what is the best choice for $\lambda$?

# References

::: {#refs}
:::



