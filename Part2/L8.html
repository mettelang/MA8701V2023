<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mette Langaas">
<meta name="dcterms.date" content="2023-02-02">

<title>MA8701 Advanced methods in statistical inference and learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="L8_files/libs/clipboard/clipboard.min.js"></script>
<script src="L8_files/libs/quarto-html/quarto.js"></script>
<script src="L8_files/libs/quarto-html/popper.min.js"></script>
<script src="L8_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="L8_files/libs/quarto-html/anchor.min.js"></script>
<link href="L8_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="L8_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="L8_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="L8_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="L8_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#before-we-begin" id="toc-before-we-begin" class="nav-link active" data-scroll-target="#before-we-begin">Before we begin</a>
  <ul class="collapse">
  <li><a href="#literature" id="toc-literature" class="nav-link" data-scroll-target="#literature">Literature</a></li>
  <li><a href="#linear-regression-set-up" id="toc-linear-regression-set-up" class="nav-link" data-scroll-target="#linear-regression-set-up">Linear regression set-up</a></li>
  </ul></li>
  <li><a href="#lasso" id="toc-lasso" class="nav-link" data-scroll-target="#lasso">Lasso</a>
  <ul class="collapse">
  <li><a href="#minimization-problem" id="toc-minimization-problem" class="nav-link" data-scroll-target="#minimization-problem">Minimization problem</a>
  <ul class="collapse">
  <li><a href="#budget-version" id="toc-budget-version" class="nav-link" data-scroll-target="#budget-version">Budget version</a></li>
  <li><a href="#penalty-version" id="toc-penalty-version" class="nav-link" data-scroll-target="#penalty-version">Penalty version</a></li>
  </ul></li>
  <li><a href="#small-notational-difference-in-the-two-textbooks" id="toc-small-notational-difference-in-the-two-textbooks" class="nav-link" data-scroll-target="#small-notational-difference-in-the-two-textbooks">Small notational difference in the two textbooks</a></li>
  <li><a href="#parameter-estimation" id="toc-parameter-estimation" class="nav-link" data-scroll-target="#parameter-estimation">Parameter estimation</a></li>
  <li><a href="#observations" id="toc-observations" class="nav-link" data-scroll-target="#observations">Observations</a></li>
  <li><a href="#compare-ridge-and-lasso" id="toc-compare-ridge-and-lasso" class="nav-link" data-scroll-target="#compare-ridge-and-lasso">Compare ridge and Lasso</a></li>
  </ul></li>
  <li><a href="#parameter-estimation-1" id="toc-parameter-estimation-1" class="nav-link" data-scroll-target="#parameter-estimation-1">Parameter estimation</a>
  <ul class="collapse">
  <li><a href="#one-covariate" id="toc-one-covariate" class="nav-link" data-scroll-target="#one-covariate">One covariate</a></li>
  <li><a href="#two-covariates" id="toc-two-covariates" class="nav-link" data-scroll-target="#two-covariates">Two covariates</a></li>
  <li><a href="#orthogonal-covariates" id="toc-orthogonal-covariates" class="nav-link" data-scroll-target="#orthogonal-covariates">Orthogonal covariates</a></li>
  <li><a href="#group-discussion" id="toc-group-discussion" class="nav-link" data-scroll-target="#group-discussion">Group discussion</a></li>
  <li><a href="#algorithmic-solutions" id="toc-algorithmic-solutions" class="nav-link" data-scroll-target="#algorithmic-solutions">Algorithmic solutions</a></li>
  <li><a href="#cyclic-coordinate-descent" id="toc-cyclic-coordinate-descent" class="nav-link" data-scroll-target="#cyclic-coordinate-descent">Cyclic coordinate descent</a></li>
  <li><a href="#group-discussion-1" id="toc-group-discussion-1" class="nav-link" data-scroll-target="#group-discussion-1">Group discussion</a></li>
  <li><a href="#cyclic-coordinate-descent-and-lambda" id="toc-cyclic-coordinate-descent-and-lambda" class="nav-link" data-scroll-target="#cyclic-coordinate-descent-and-lambda">Cyclic coordinate descent and <span class="math inline">\(\lambda\)</span></a></li>
  <li><a href="#gasoline-lasso" id="toc-gasoline-lasso" class="nav-link" data-scroll-target="#gasoline-lasso">Gasoline lasso</a></li>
  <li><a href="#conditions-for-a-solution-to-the-penalty-version" id="toc-conditions-for-a-solution-to-the-penalty-version" class="nav-link" data-scroll-target="#conditions-for-a-solution-to-the-penalty-version">Conditions for a solution to the penalty version</a></li>
  </ul></li>
  <li><a href="#degrees-of-freedom" id="toc-degrees-of-freedom" class="nav-link" data-scroll-target="#degrees-of-freedom">Degrees of freedom</a></li>
  <li><a href="#properties-of-the-lasso-estimator-and-solution" id="toc-properties-of-the-lasso-estimator-and-solution" class="nav-link" data-scroll-target="#properties-of-the-lasso-estimator-and-solution">Properties of the lasso estimator and solution</a>
  <ul class="collapse">
  <li><a href="#uniqueness" id="toc-uniqueness" class="nav-link" data-scroll-target="#uniqueness">Uniqueness</a></li>
  <li><a href="#sparsity" id="toc-sparsity" class="nav-link" data-scroll-target="#sparsity">Sparsity</a></li>
  <li><a href="#shrinkage" id="toc-shrinkage" class="nav-link" data-scroll-target="#shrinkage">Shrinkage</a></li>
  <li><a href="#moments" id="toc-moments" class="nav-link" data-scroll-target="#moments">Moments</a></li>
  <li><a href="#oracle-property" id="toc-oracle-property" class="nav-link" data-scroll-target="#oracle-property">Oracle property</a></li>
  <li><a href="#is-really-the-mse-for-lasso-smaller-than-for-ls" id="toc-is-really-the-mse-for-lasso-smaller-than-for-ls" class="nav-link" data-scroll-target="#is-really-the-mse-for-lasso-smaller-than-for-ls">Is really the MSE for lasso smaller than for LS?</a></li>
  <li><a href="#what-needs-to-be-improved" id="toc-what-needs-to-be-improved" class="nav-link" data-scroll-target="#what-needs-to-be-improved">What needs to be improved?</a></li>
  </ul></li>
  <li><a href="#summing-up" id="toc-summing-up" class="nav-link" data-scroll-target="#summing-up">Summing up</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">MA8701 Advanced methods in statistical inference and learning</h1>
<p class="subtitle lead">L8: Lasso regression for the linear model</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mette Langaas </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 2, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<div class="cell">

</div>
<div class="cell">

</div>
<section id="before-we-begin" class="level1">
<h1>Before we begin</h1>
<section id="literature" class="level2">
<h2 class="anchored" data-anchor-id="literature">Literature</h2>
<ul>
<li><p>[ESL] The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition (Springer Series in Statistics, 2009) by Trevor Hastie, Robert Tibshirani, and Jerome Friedman. <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">Ebook</a>. Chapter 3.2.3, 3.4.2</p></li>
<li><p>[HTW] Hastie, Tibshirani, Wainwrigh: “Statistical Learning with Sparsity: The Lasso and Generalizations”. CRC press. <a href="https://hastie.su.domains/StatLearnSparsity/">Ebook</a>. Chapter 2.2-2.5, 3.7.</p></li>
</ul>
<p>and for the interested student</p>
<ul>
<li>[WNvW] <a href="https://arxiv.org/pdf/1509.09169v7.pdf">Wessel N. van Wieringen: Lecture notes on ridge regression</a> Chapter 6.1-6.5, 6.7</li>
</ul>
<hr>
</section>
<section id="linear-regression-set-up" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-set-up">Linear regression set-up</h2>
<p><span class="math display">\[{\mathbf Y=X \boldsymbol{\beta}}+{\boldsymbol\varepsilon}\]</span></p>
<ul>
<li>where <span class="math inline">\({\mathbf Y}\)</span> is a <span class="math inline">\(N \times 1\)</span> random column vector,</li>
<li><span class="math inline">\({\mathbf X}\)</span> a <span class="math inline">\(N \times p\)</span> design matrix with row for observations and columns for covariates, and</li>
<li><span class="math inline">\({\boldsymbol{\varepsilon}}\)</span> is a <span class="math inline">\(N \times 1\)</span> random column vector.</li>
</ul>
<p>As in L7, covariates are standardized and response is centered, but the design matrix need (in general) not have full rank (but for LS to exist we will often assume full rank - we often would like to compare to LS).</p>
</section>
</section>
<section id="lasso" class="level1">
<h1>Lasso</h1>
<p>Origin:</p>
<ul>
<li>the acronym is <em>Least Absolute Shrinkage and Selection Operator</em>, and that the</li>
<li>lasso was invented by Robert Tibshirani and published in an article in <a href="https://www.jstor.org/stable/2346178?seq=1">JRSSB in 1996</a></li>
</ul>
<p>HTW page 8: “the method lassos the coefficients for the model”</p>
<hr>
<p>Now we will do what looks at first sight as a small change from the ridge - we will use</p>
<ul>
<li>a budget on the absolute value instead of squared value - moving from the <span class="math inline">\(L_2\)</span> to the <span class="math inline">\(L_1\)</span> norm.</li>
</ul>
<p>But, this will have a large impact on the parameter estimates -</p>
<ul>
<li>both shrinking (to get a better MSE of our predictions) - and performing model selection (by shrinking all the way down to 0 - in a continuous way).</li>
</ul>
<p>Again, we will not shrink the intercept <span class="math inline">\(\beta_0\)</span>, because then the this will depend on the origin of the response, and we will work with standardized covariates and centered response.</p>
<hr>
<section id="minimization-problem" class="level2">
<h2 class="anchored" data-anchor-id="minimization-problem">Minimization problem</h2>
<section id="budget-version" class="level3">
<h3 class="anchored" data-anchor-id="budget-version">Budget version</h3>
<p>We want to constrain the size of the estimated regression parameters, so we give the sum of squared regression coefficients a budget <span class="math inline">\(t\)</span>.</p>
<p>Minimize the squared error loss</p>
<p><span class="math display">\[ \sum_{i=1}^N (y_i-\sum_{j=1}^p x_{ij}\beta_j )^2 \]</span> subject to <span class="math inline">\(\sum_{j=1}^p \lvert \beta_j\rvert \le t\)</span>. The solution is called <span class="math inline">\(\hat{\beta}_{\text{lasso}}\)</span>.</p>
<hr>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./ILS67lasso.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure from An Introduction to Statistical Learning, with applications in R (Springer, 2013) with permission from the authors: G. James, D. Witten, T. Hastie and R. Tibshirani.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
<section id="penalty-version" class="level3">
<h3 class="anchored" data-anchor-id="penalty-version">Penalty version</h3>
<!-- $\frac{1}{2N}$ or not? -->
<p><span class="math display">\[ \hat{\beta}_{\text{lasso}}= \text{argmin}_{\beta} [\sum_{i=1}^N (y_i-\sum_{j=1}^p x_{ij}\beta_j )^2 + \lambda \sum_{j=1}^p \lvert \beta_j\rvert ] \]</span> again, <span class="math inline">\(\lambda \ge 0\)</span> is a complexity (regularization, penalty) parameter controlling the amount of shrinkage.</p>
<ul>
<li>The larger <span class="math inline">\(\lambda\)</span> the greater the amount of shrinkage</li>
<li>The shrinkage is towards 0</li>
</ul>
<p>This version of the problem is also called the Lagrangian form.</p>
<p>The budget and penalty minimization problems are equivalent ways to write the ridge regression and there is a one-to-one correspondence between the budget <span class="math inline">\(t\)</span> and the penalty <span class="math inline">\(\lambda\)</span>.</p>
<!-- (Where to read about Lagrangian duality?) -->
<!-- (Exercise 3.5: reparameterize covariates using centering) -->
<hr>
</section>
</section>
<section id="small-notational-difference-in-the-two-textbooks" class="level2">
<h2 class="anchored" data-anchor-id="small-notational-difference-in-the-two-textbooks">Small notational difference in the two textbooks</h2>
<ul>
<li>In HTW an extra <span class="math inline">\(\frac{1}{2N}\)</span> factor for the squared error for the ridge and the lasso,</li>
<li>which is just for ease of interpretation of a future shrinkage parameter to be included</li>
<li>(<em>to make that shrinkage parameter comparable across different sample sizes in the use of cross-validation</em>).</li>
<li>We will also see this effect of a scaling by a factor for the squares error in the derivation of a lasso formula for the one covariate and ortogonal design matrix case (below).</li>
<li>The factor does not influence the solution of the minimization of the squared-error loss we consider now (only the <span class="math inline">\(\lambda\)</span>).</li>
</ul>
<hr>
</section>
<section id="parameter-estimation" class="level2">
<h2 class="anchored" data-anchor-id="parameter-estimation">Parameter estimation</h2>
<ul>
<li><p>As explained, centred covariates and responses are used - and the intercept term is removed from the model. Then <span class="math inline">\({\mathbf X}\)</span> does not include a column with 1s and has dimension <span class="math inline">\(N \times p\)</span>.</p></li>
<li><p>The use of the absolute value in the penalty term makes the solution in general non-linear in <span class="math inline">\(y_i\)</span>, and no closed form (analytic) solution is available.</p></li>
<li><p>As for ridge, cross-validation is used (as kind of a standard) to choose an optimal <span class="math inline">\(\lambda\)</span>.</p></li>
</ul>
<hr>
</section>
<section id="observations" class="level2">
<h2 class="anchored" data-anchor-id="observations">Observations</h2>
<ul>
<li><p>If we make the budget <span class="math inline">\(t\)</span> sufficiently small some of the coefficients will be exactly zero.</p></li>
<li><p>If <span class="math inline">\(t\)</span> is chosen larger than <span class="math inline">\(t_0=\sum_{j=1}^p \lvert \hat{\beta}_{{\text {LS}},j} \rvert\)</span> the lasso estimates equal the LS estimates.</p></li>
<li><p>The nature of the shrinkage is complex.</p></li>
<li><p>Closed form (analytic) estimator of the lasso estimator is only available for one covariate, two covariates and for an orthonormal design matrix.</p></li>
</ul>
<hr>
</section>
<section id="compare-ridge-and-lasso" class="level2">
<h2 class="anchored" data-anchor-id="compare-ridge-and-lasso">Compare ridge and Lasso</h2>
<p>These figures are taken from <span class="citation" data-cites="ESL">Hastie, Tibshirani, and Friedman (<a href="#ref-ESL" role="doc-biblioref">2009</a>)</span> and are based on the “prostate cancer example”. The response is the log of PSA (level of a prostae specific antigen), <span class="math inline">\(N=97\)</span> and there are <span class="math inline">\(p=8\)</span> covariates.</p>
<hr>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./ESLFig38.jpg" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Figure 3.8 from <span class="citation" data-cites="ESL">Hastie, Tibshirani, and Friedman (<a href="#ref-ESL" role="doc-biblioref">2009</a>)</span></figcaption><p></p>
</figure>
</div>
</div>
</div>
<hr>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./ESLFig310.jpg" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Figure 3.10 from <span class="citation" data-cites="ESL">Hastie, Tibshirani, and Friedman (<a href="#ref-ESL" role="doc-biblioref">2009</a>)</span>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
</section>
<section id="parameter-estimation-1" class="level1">
<h1>Parameter estimation</h1>
<p>The lasso regression estimator is not one, but a whole sequence of estimators for <span class="math inline">\(\beta\)</span> - one for each choice of penalty parameter <span class="math inline">\(\lambda&gt;0\)</span>.</p>
<p>This sequence is called the <em>lasso regularization path</em>.</p>
<section id="one-covariate" class="level2">
<h2 class="anchored" data-anchor-id="one-covariate">One covariate</h2>
<p>This case - explicit solution! New word: soft thresholding” - (this is as opposed to hard thresholding as is used when a coefficient is set to 0=aka model subset selection)</p>
<p>This can also be explained using the theory of subgradients.</p>
<hr>
<div class="cell">
<div class="cell-output-display">
<p><img src="L8_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<hr>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./ILS610.png" class="img-fluid figure-img" style="width:90.0%"></p>
<p></p><figcaption class="figure-caption">Figure from An Introduction to Statistical Learning, with applications in R (Springer, 2013) with permission from the authors: G. James, D. Witten, T. Hastie and R. Tibshirani.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
<section id="two-covariates" class="level2">
<h2 class="anchored" data-anchor-id="two-covariates">Two covariates</h2>
<p>Also explicit solution - in <span class="citation" data-cites="Lasso1996">Tibshirani (<a href="#ref-Lasso1996" role="doc-biblioref">1996</a>)</span>, (without loss of generality) assume the LS estimators are both <em>positive</em>. <span class="math inline">\(t\)</span> is from the budget version of the lasso</p>
<p><span class="math display">\[\hat{\beta}_1=(\frac{t}{2}+\frac{\hat{\beta}_{\text{LS},1}-\hat{\beta}_{\text{LS},2}}{2})^{+}\]</span> <span class="math display">\[\hat{\beta}_1=(\frac{t}{2}-\frac{\hat{\beta}_{\text{LS},1}-\hat{\beta}_{\text{LS},2}}{2})^{+}\]</span></p>
<p>Observe, that the correlation of the covariates (i.e.&nbsp;as estimated proportional to the the <span class="math inline">\(X^T X\)</span> matrix for centered covariates) is not directly part of the solution (but it is in the construction of the LS estimator).</p>
<hr>
<p>See Figure 4 from <span class="citation" data-cites="Lasso1996">Tibshirani (<a href="#ref-Lasso1996" role="doc-biblioref">1996</a>)</span> in class: <span class="math inline">\(N=100\)</span> data points from the model <span class="math inline">\(y=6 x_1+3x_2\)</span> with no noise, but where <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are drawn from <span class="math inline">\(N_2(\mathbf 0, diag(\rho))\)</span>.</p>
<!-- Check: why rho a part of WNvW page 98- Example 6.1 and Exerc 6.7? -->
<hr>
</section>
<section id="orthogonal-covariates" class="level2">
<h2 class="anchored" data-anchor-id="orthogonal-covariates">Orthogonal covariates</h2>
<p><span class="math display">\[{\mathbf Y=X \boldsymbol{\beta}}+{\boldsymbol\varepsilon}\]</span> where <span class="math inline">\({\mathbf Y}\)</span> is a <span class="math inline">\(N \times 1\)</span> random column vector, <span class="math inline">\({\mathbf X}\)</span> a <span class="math inline">\(N \times (p+1)\)</span> design matrix with row for observations and columns for covariates, and <span class="math inline">\({\boldsymbol{\varepsilon}}\)</span> <span class="math inline">\(N \times 1\)</span> random column vector.</p>
<p>Futher, <span class="math inline">\(\text{E}(\boldsymbol{\varepsilon})=\mathbf{0}\)</span> and <span class="math inline">\(\text{Cov}(\varepsilon)=\text{E}(\varepsilon \varepsilon^T)=\sigma^2\mathbf{I}\)</span>.</p>
<p>NOW: Assume that the design matrix <span class="math inline">\(\mathbf{X}\)</span> is ortogonal, that is, <span class="math inline">\(\mathbf{X}^T\mathbf{X}=\mathbf{I}_{pp}=(\mathbf{X}^T\mathbf{X})^{-1}\)</span>.</p>
<p>Derive the lasso regression parameter (for a given <span class="math inline">\(\lambda\)</span>).</p>
<p>Just a few starting observations - and then we may use the soft-thresholding result for each covariate separately.</p>
<hr>
</section>
<section id="group-discussion" class="level2">
<h2 class="anchored" data-anchor-id="group-discussion">Group discussion</h2>
<p>Write down the optimization problem and see how far you get!</p>
<hr>
</section>
<section id="algorithmic-solutions" class="level2">
<h2 class="anchored" data-anchor-id="algorithmic-solutions">Algorithmic solutions</h2>
<p><span class="citation" data-cites="WNvW">Wieringen (<a href="#ref-WNvW" role="doc-biblioref">2020</a>)</span> Section 6.4</p>
<p>In general there is no closed form solution to the lasso parameter estimation (except the special cases handled above).</p>
<ul>
<li><span class="citation" data-cites="Lasso1996">Tibshirani (<a href="#ref-Lasso1996" role="doc-biblioref">1996</a>)</span>: reformulated lasso optimization to a quadratic program, optimizing a quadratic form subject to linear constraints. For small <span class="math inline">\(p\)</span> viable, but not for larger <span class="math inline">\(p\)</span> due to the many linear constraints.</li>
<li>The loss function of the lasso can be optimized by iterative application of the (generalized) ridge regression using local a quadratic approximation of the absolute value function. Difficult if covariates superlinear.</li>
<li>Gradient ascent - but using a generalized derivative, G<span class="math inline">\(\^{a}\)</span>teaux derivative. R penalized package by Goeman (2010).</li>
<li>LARS: see Section of <span class="citation" data-cites="ESL">Hastie, Tibshirani, and Friedman (<a href="#ref-ESL" role="doc-biblioref">2009</a>)</span> Section 3.4.4 and <span class="citation" data-cites="HTW">Hastie, Tibshirani, and Wainwright (<a href="#ref-HTW" role="doc-biblioref">2015</a>)</span> Section 5.6 (not on reading list, but interesting)</li>
</ul>
<hr>
</section>
<section id="cyclic-coordinate-descent" class="level2">
<h2 class="anchored" data-anchor-id="cyclic-coordinate-descent">Cyclic coordinate descent</h2>
<p><span class="citation" data-cites="WNvW">Wieringen (<a href="#ref-WNvW" role="doc-biblioref">2020</a>)</span> Section 6.4. <span class="citation" data-cites="HTW">Hastie, Tibshirani, and Wainwright (<a href="#ref-HTW" role="doc-biblioref">2015</a>)</span> Section 2.4.2 and 5.4</p>
<p>We have for one covariate (predictor) seen how the soft threshold is the solution to the lasso regression parameter estimation and for a orthonormal design matrix we have seen that we may perform the estimation separately for each covariate (for a given value of <span class="math inline">\(\lambda\)</span>).</p>
<p>Now the idea is to</p>
<ul>
<li><em>repeatedly</em> cycle through the predictors in some fixed (but arbitrary) order.</li>
<li>When we come to covariate <span class="math inline">\(j\)</span> we update the lasso estimator for <span class="math inline">\(\lambda_j\)</span> by holding the other coefficients fixed and minimizing the objective function for covariate <span class="math inline">\(j\)</span> -</li>
<li>but then the response is not <span class="math inline">\(y\)</span> but the difference between <span class="math inline">\(y\)</span> and the linear predictor of all the other covariates except <span class="math inline">\(j\)</span>.</li>
</ul>
<hr>
<p>This is motivated by rewriting the optimization problem</p>
<p><span class="math display">\[ \sum_{i=1}^N (y_i-\sum_{j=1}^p x_{ij}\beta_j )^2 + \lambda \sum_{j=1}^p \lvert \beta_j\rvert \]</span> <span class="math display">\[ \sum_{i=1}^N (y_i-\sum_{k\neq j} x_{ik}\beta_k -x_{ij}\beta_j )^2 + \lambda \sum_{k \neq j} \lvert \beta_k\rvert + \lambda \lvert \beta_j \rvert \]</span></p>
<hr>
<p>If we want to minimize this for <span class="math inline">\(\beta_j\)</span> then the sum of absolute values may be dropped from the expression we want work with, and we may define</p>
<p><span class="math display">\[\tilde{y}_i=y_i-\sum_{k\neq j} x_{ik}\beta_k\]</span> we may write the minimization problem for one <span class="math inline">\(\beta_j\)</span> at a time</p>
<p><span class="math display">\[\sum_{i=1}^N (\tilde{y}_i -x_{ij}\beta_j )^2 + \lambda \lvert \beta_j \rvert\]</span> This problem we know the solution to, this we would for one covariate to be the soft-threshold function.</p>
<p>We will then need some intitialization for the <span class="math inline">\(\beta\)</span>s and then cycle through the one-parameter problems until convergence.</p>
<p>We may view this as a “coordinate-wise minimization scheme”</p>
<hr>
</section>
<section id="group-discussion-1" class="level2">
<h2 class="anchored" data-anchor-id="group-discussion-1">Group discussion</h2>
<p>Write down in pseudo code the steps of the cyclic coordinate descent algorithm.</p>
<!-- ### Multiple regression from simple univariate regression -->
<!-- @ELS Section 3.2.3  -->
<!-- Algorithm 3.1 Regression by Successive Orthogonalization -->
<!-- 1) Initialize $z_0=x_0=1$ -->
<!-- 2) For $j=1,2,\ldots, p$ -->
<hr>
<p>Result HTW page 110: Additive function to minimize: <span class="math display">\[ f(\beta)=g(\beta)+\sum_{j=1}^p h_j(\beta_j)\]</span></p>
<p><span class="math inline">\(g\)</span> differentiable and convex, <span class="math inline">\(h\)</span> univariate and convex. It is found that the (cyclic) coordinate descent algorithm is <em>guaranteed to converge</em> to the global minimizer.</p>
<!-- Forward stepwise regression:  -->
<!-- * build a model sequentially by adding one variable at a time.  -->
<!-- * At each step the best variable to include in the active set is identified and -->
<!-- * then the LS-fit is (re)computed for all the active variables. -->
<hr>
</section>
<section id="cyclic-coordinate-descent-and-lambda" class="level2">
<h2 class="anchored" data-anchor-id="cyclic-coordinate-descent-and-lambda">Cyclic coordinate descent and <span class="math inline">\(\lambda\)</span></h2>
<p>If <span class="math inline">\(\lambda=0\)</span> (and the design matrix has full rank) this will be a univariate regression of partial residuals onto each covariate, and then cycling through the covariates until convergence. This is not a very efficient method to find the least squares estimators.</p>
<p>We are interested in the full lasso path, not just the result for a given <span class="math inline">\(\lambda\)</span>.</p>
<hr>
<ol type="1">
<li>Start with a value of <span class="math inline">\(\lambda\)</span> such that the optimal solution is equal to a vector of all zeros.</li>
</ol>
<p>This will happen at <span class="math inline">\(\lambda_{\text max}=\max_{\text{j}} \lvert \hat{\beta}_{\text{LS},j} \rvert\)</span>.</p>
<ol start="2" type="1">
<li><p>Then we decrease <span class="math inline">\(\lambda\)</span> by a small amount and run cyclic coordinate descent until convergence.</p></li>
<li><p>Then we decrease <span class="math inline">\(\lambda\)</span> again, but this time we choose the result at the previous <span class="math inline">\(\lambda\)</span> as a so-called “warm start”.</p></li>
</ol>
<p>This method is referred to as <em>pathwise coordinate descent</em>.</p>
<hr>
</section>
<section id="gasoline-lasso" class="level2">
<h2 class="anchored" data-anchor-id="gasoline-lasso">Gasoline lasso</h2>
<p>This is how lasso is fit using the glmnet R-package.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>ds <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">"./sniffer.dat"</span>,<span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">apply</span>(ds[,<span class="sc">-</span><span class="dv">5</span>],<span class="dv">2</span>,scale)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> ds[,<span class="dv">5</span>]<span class="sc">-</span><span class="fu">mean</span>(ds[,<span class="dv">5</span>])</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Now we fit a lasso model; for this we use the default `alpha=1`</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>fit.lasso<span class="ot">=</span><span class="fu">glmnet</span>(x,y)<span class="co">#,lambda=newlambda)</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit.lasso,<span class="at">xvar=</span><span class="st">"lambda"</span>,<span class="at">label=</span><span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="L8_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>cv.lasso<span class="ot">=</span><span class="fu">cv.glmnet</span>(x,y)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">#which.min(cv.lasso$cvm)</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.lasso)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="L8_files/figure-html/unnamed-chunk-8-2.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit.lasso,<span class="at">xvar=</span><span class="st">"lambda"</span>,<span class="at">label=</span><span class="cn">TRUE</span>);</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fu">log</span>(cv.lasso<span class="sc">$</span>lambda<span class="fl">.1</span>se))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="L8_files/figure-html/unnamed-chunk-8-3.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(cv.lasso)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>5 x 1 sparse Matrix of class "dgCMatrix"
                       s1
(Intercept) -1.440776e-15
TankTemp     .           
GasTemp      3.524047e+00
TankPres     .           
GasPres      4.207423e+00</code></pre>
</div>
</div>
<hr>
</section>
<section id="conditions-for-a-solution-to-the-penalty-version" class="level2">
<h2 class="anchored" data-anchor-id="conditions-for-a-solution-to-the-penalty-version">Conditions for a solution to the penalty version</h2>
<p>(HTW page 9)</p>
<p>The details are found in HTW Chapter 5 (not on our reading list), but the student familiar with convex analysis, dual problems and Karush-Kuhn-Tucker (KKT) conditions might find Chapter 5 of interest.</p>
<p>Convex analysis theory: necessary and sufficient conditions for a solution to the lasso penalty problem is</p>
<p><span class="math display">\[ \frac{1}{N}\langle {\mathbf x}_j,{\mathbf y}-{\mathbf X}\beta \rangle+\lambda s_j=0 \mbox{ for } j=1,\ldots,p\]</span></p>
<p>where <span class="math inline">\(\langle a,b \rangle=a^T b\)</span> denotes the inner product. Each <span class="math inline">\(s_j\)</span> is an unknow quantity, equal to</p>
<ul>
<li><span class="math inline">\(\text{sign}(\beta_j)\)</span> if <span class="math inline">\(\beta_j\neq 0\)</span></li>
<li>some value in <span class="math inline">\([-1,1]\)</span> otherwise (socalled <em>subgradient</em> of the absolute value function).</li>
</ul>
<p>We may solve this problem in <span class="math inline">\((\hat{\beta},\hat{s})\)</span>, instead of the penalty version.</p>
</section>
</section>
<section id="degrees-of-freedom" class="level1">
<h1>Degrees of freedom</h1>
<p>(HTW 2.5)</p>
<p>In ESL Ch 7.6 we defined the effective number of parameters (here now referred to as the <em>effective degrees of freedom</em>) for a linear smoother, and used that for the ridge regression. However, the lasso is not a linear smoother (it is nonlinear in the reponses <span class="math inline">\(y_i\)</span>).</p>
<p>The lasso is an adaptive fitting procedure, and if our final model has <span class="math inline">\(k\)</span> covariates that is different from zero, we would not think that the effective degrees of freedom for the lasso is then <span class="math inline">\(k\)</span>. However, it turns out that it is correct to <em>count</em> the number of degrees of freedom by the number of nonzero coefficients.</p>
<hr>
<p>In ESL Ch 7.6 we also defined the degrees of freedom using the covariance generalization: <span class="math display">\[\text{df}(\hat{{\mathbf y}})=\frac{\sum_{i=1}^N \text{Cov}(\hat{y}_i,y_i)}{\sigma_{\varepsilon}^2}\]</span></p>
<p>where the covariance is taken for the response and predicted response, while the covariates are kept fixed (this formula was developed in connection to the in-sample prediction error).</p>
<hr>
<p>It has been shown (HTW refer to this at “somewhat miraculously) that with a fixed penalty parameter <span class="math inline">\(\lambda\)</span> the number of non-zero coefficients <span class="math inline">\(k_{\lambda}\)</span> is an <em>unbiased estimate</em> for the degrees of freedom.</p>
<p>This is explained by considering that the lasso does not only select predictors (selecting predictors will give an inflated degrees of freedom) - but also shrinks the coefficients relative to the LS estimates. These two forces kind of cancel out.</p>
<p>HTW (page 19): a general proof is difficult, but for an orthogonal design using the fact that the lasso estimates are soft-thresholded versions of the univariate regression coefficients for the othogonal design.</p>
</section>
<section id="properties-of-the-lasso-estimator-and-solution" class="level1">
<h1>Properties of the lasso estimator and solution</h1>
<section id="uniqueness" class="level2">
<h2 class="anchored" data-anchor-id="uniqueness">Uniqueness</h2>
<p>(WNvW 6.1)</p>
<p>The lasso estimator is non-unique if <span class="math inline">\(p&gt;N\)</span> and if not full rank, else unique.</p>
<p>The prediction <span class="math inline">\(X \hat{\beta}_{\text lasso}\)</span> is unique.</p>
<hr>
</section>
<section id="sparsity" class="level2">
<h2 class="anchored" data-anchor-id="sparsity">Sparsity</h2>
<p>“Geometric accident”</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./WNvWFig63.jpg" class="img-fluid figure-img" style="width:90.0%"></p>
<p></p><figcaption class="figure-caption">Figure 6.3 from <span class="citation" data-cites="WNvW">Wieringen (<a href="#ref-WNvW" role="doc-biblioref">2020</a>)</span></figcaption><p></p>
</figure>
</div>
</div>
</div>
<hr>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./WNvWFig64.jpg" class="img-fluid figure-img" style="width:90.0%"></p>
<p></p><figcaption class="figure-caption">Figure 6.4 from <span class="citation" data-cites="WNvW">Wieringen (<a href="#ref-WNvW" role="doc-biblioref">2020</a>)</span></figcaption><p></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
<section id="shrinkage" class="level2">
<h2 class="anchored" data-anchor-id="shrinkage">Shrinkage</h2>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./WNvWFig69.jpg" class="img-fluid figure-img" style="width:90.0%"></p>
<p></p><figcaption class="figure-caption">Figure 6.9 from <span class="citation" data-cites="WNvW">Wieringen (<a href="#ref-WNvW" role="doc-biblioref">2020</a>)</span></figcaption><p></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
<section id="moments" class="level2">
<h2 class="anchored" data-anchor-id="moments">Moments</h2>
<p>Not on closed form. Later in W6 look at using bootstrapping for statistical inference.</p>
</section>
<section id="oracle-property" class="level2">
<h2 class="anchored" data-anchor-id="oracle-property">Oracle property</h2>
<p><span class="citation" data-cites="Zou2006">Zou (<a href="#ref-Zou2006" role="doc-biblioref">2006</a>)</span> results to be added.</p>
</section>
<section id="is-really-the-mse-for-lasso-smaller-than-for-ls" class="level2">
<h2 class="anchored" data-anchor-id="is-really-the-mse-for-lasso-smaller-than-for-ls">Is really the MSE for lasso smaller than for LS?</h2>
<p>WNvW and reference within - with figure - to be added</p>
</section>
<section id="what-needs-to-be-improved" class="level2">
<h2 class="anchored" data-anchor-id="what-needs-to-be-improved">What needs to be improved?</h2>
<p>To be added or just in L9 - when we look at friends of the lasso.</p>
</section>
</section>
<section id="summing-up" class="level1">
<h1>Summing up</h1>
<ul>
<li>When is lasso preferred to LS. Only for (an unknown?) range of <span class="math inline">\(\lambda\)</span>-values.</li>
<li>Lasso vs ridge: Neighter ridge or lasso dominates (in MSE) the other in all situations.</li>
</ul>
<p>Take home message from <span class="citation" data-cites="Lasso1996">Tibshirani (<a href="#ref-Lasso1996" role="doc-biblioref">1996</a>)</span> Section 11:</p>
<ul>
<li>“Small number of large effects: subset selection is preferred, the lasso not quite as well and the ridge regression performs poorly”</li>
<li>“Small to moderate number of moderate-sized effects - the lasso does best, follow by ridge regression and then subset selection”</li>
<li>“large number of small effects - ridge regression does best by a good margin, followed by lasso and then subset selection”</li>
</ul>
<!-- # Exercises -->
<!-- None -->
<!-- # Solutions to exercises -->
<!-- Please try yourself first, or take a small peek - and try some more - before fully reading the solutions. Report errors or improvements to <Mette.Langaas@ntnu.no>.  -->
</section>
<section id="resources" class="level1">
<h1>Resources</h1>
<ul>
<li>Videos in statistics learning with Rob Tibshirani and Daniela Witten, made for the Introduction to statistical learning Springer textbook.
<ul>
<li><a href="https://www.youtube.com/watch?v=A5I1G1MfUmA">Lasso</a></li>
<li><a href="https://www.youtube.com/watch?v=xMKVUstjXBE">Selecting tuning parameter</a></li>
</ul></li>
<li>Video from webinar with Trevor Hastie on <a href="http://youtu.be/BU2gjoLPfDc">glmnet from 2019</a></li>
</ul>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-casi" class="csl-entry" role="doc-biblioentry">
Efron, Bradley, and Trevor Hastie. 2016. <em>Computer Age Statistical Inference - Algorithms, Evidence, and Data Science</em>. Cambridge University Press. <a href="https://hastie.su.domains/CASI/">https://hastie.su.domains/CASI/</a>.
</div>
<div id="ref-HTW" class="csl-entry" role="doc-biblioentry">
Hastie, Trevor, Roberg Tibshirani, and Martin Wainwright. 2015. <em>Statistical Learning with Sparsity: The Lasso and Generalizations</em>. CRC Press. <a href="https://hastie.su.domains/StatLearnSparsity/">https://hastie.su.domains/StatLearnSparsity/</a>.
</div>
<div id="ref-ESL" class="csl-entry" role="doc-biblioentry">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Vol. 2. Springer series in statistics New York. <a href="https://hastie.su.domains/ElemStatLearn">hastie.su.domains/ElemStatLearn</a>.
</div>
<div id="ref-Lasso1996" class="csl-entry" role="doc-biblioentry">
Tibshirani, Robert. 1996. <span>“Regression Shrinkage and Selection via the Lasso.”</span> <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 58 (1): 267–88. <a href="http://www.jstor.org/stable/2346178">http://www.jstor.org/stable/2346178</a>.
</div>
<div id="ref-WNvW" class="csl-entry" role="doc-biblioentry">
Wieringen, Wessel N. van. 2020. <span>“Lecture Notes on Ridge Regression.”</span> <a href="https://arxiv.org/pdf/1509.09169.pdf">https://arxiv.org/pdf/1509.09169.pdf</a>.
</div>
<div id="ref-Zou2006" class="csl-entry" role="doc-biblioentry">
Zou, Hui. 2006. <span>“The Adaptive Lasso and Its Oracle Properties.”</span> <em>Journal of the American Statistical Association</em> 101 (476): 1418–29. <a href="https://doi.org/10.1198/016214506000000735">https://doi.org/10.1198/016214506000000735</a>.
</div>
</div>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>