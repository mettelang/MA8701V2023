\contentsline {section}{Ensembles - first act}{2}{section*.2}% 
\contentsline {subsection}{Outline}{3}{section*.3}% 
\contentsline {subsection}{Literature L5}{3}{section*.4}% 
\contentsline {section}{Trees}{3}{section*.5}% 
\contentsline {subsection}{Leo Breiman}{3}{section*.6}% 
\contentsline {subsection}{Main idea}{4}{section*.7}% 
\contentsline {subsubsection}{From regions in predictor space to decision tree}{4}{section*.8}% 
\contentsline {subsection}{Glossary}{4}{section*.9}% 
\contentsline {subsubsection}{Problem: preditor space partition vs tree}{4}{section*.10}% 
\contentsline {section}{Regression tree}{5}{section*.11}% 
\contentsline {subsection}{Fitting a regression tree}{5}{section*.12}% 
\contentsline {subsection}{Recursive binary splitting}{6}{section*.13}% 
\contentsline {subsubsection}{Problem: hands-on splitting}{7}{section*.14}% 
\contentsline {subsection}{Pruning}{9}{section*.15}% 
\contentsline {subsubsection}{Cost complexity pruning}{9}{section*.16}% 
\contentsline {subsubsection}{Pruning in practice}{9}{section*.17}% 
\contentsline {subsection}{R: function \texttt {tree} in package \texttt {tree}}{10}{section*.18}% 
\contentsline {subsection}{R: \texttt {rpart} package}{10}{section*.19}% 
\contentsline {subsection}{Regression example}{10}{section*.20}% 
\contentsline {subsubsection}{Variables}{10}{section*.21}% 
\contentsline {subsubsection}{Data}{11}{section*.22}% 
\contentsline {subsubsection}{Regression tree}{11}{section*.23}% 
\contentsline {subsubsection}{Need to prune?}{13}{section*.24}% 
\contentsline {subsubsection}{Pruning}{14}{section*.25}% 
\contentsline {subsubsection}{Test error for full tree}{15}{section*.26}% 
\contentsline {subsubsection}{Effect of changing training set}{15}{section*.27}% 
\contentsline {section}{Classification tree}{17}{section*.28}% 
\contentsline {subsection}{Fitting a classification tree}{17}{section*.29}% 
\contentsline {subsubsection}{Why not misclassification rate as the split criterion?}{18}{section*.30}% 
\contentsline {subsection}{Classification example}{19}{section*.31}% 
\contentsline {subsubsection}{Full tree performance}{20}{section*.32}% 
\contentsline {subsubsection}{Pruned tree performance}{24}{section*.33}% 
\contentsline {section}{Tree issues}{26}{section*.34}% 
\contentsline {subsection}{Building a regression (classification) tree}{26}{section*.35}% 
\contentsline {subsection}{Missing covariates}{26}{section*.36}% 
\contentsline {subsubsection}{Notation}{27}{section*.37}% 
\contentsline {subsubsection}{Missing completely at random (MCAR)}{27}{section*.38}% 
\contentsline {subsubsection}{Missing at random (MAR)}{27}{section*.39}% 
\contentsline {subsubsection}{Missing not at random (MNAR)}{27}{section*.40}% 
\contentsline {subsection}{General solutions to missing covariates}{28}{section*.41}% 
\contentsline {subsection}{Handling missing covariates in trees}{28}{section*.42}% 
\contentsline {subsubsection}{Example}{29}{section*.43}% 
\contentsline {subsection}{Other issues}{29}{section*.44}% 
\contentsline {subsection}{Questions}{30}{section*.45}% 
\contentsline {subsubsection}{Some answers}{30}{section*.46}% 
\contentsline {subsection}{What is next?}{31}{section*.47}% 
\contentsline {section}{Bagging}{31}{section*.48}% 
\contentsline {subsection}{High variance of trees}{31}{section*.49}% 
\contentsline {subsection}{Independent data sets}{31}{section*.50}% 
\contentsline {subsection}{Bootstrapping}{32}{section*.51}% 
\contentsline {subsection}{Bootstrap samples and trees}{32}{section*.52}% 
\contentsline {subsection}{Bagging regression trees}{32}{section*.53}% 
\contentsline {subsection}{Bagging classification trees}{32}{section*.54}% 
\contentsline {subsubsection}{Prediction by consensus vs probability}{33}{section*.55}% 
\contentsline {subsection}{Choosing \(B\)}{33}{section*.56}% 
\contentsline {subsection}{Pruning trees?}{34}{section*.57}% 
\contentsline {subsection}{Bagging algorithm}{34}{section*.58}% 
\contentsline {subsection}{Wisdom of the crowd}{34}{section*.59}% 
\contentsline {subsection}{Out-of-bag error estimation}{34}{section*.60}% 
\contentsline {subsubsection}{Boston housing}{34}{section*.61}% 
\contentsline {subsubsection}{Pima indians}{35}{section*.62}% 
\contentsline {subsection}{When should we use bagging?}{36}{section*.63}% 
\contentsline {section}{Random forest}{37}{section*.64}% 
\contentsline {subsection}{The effect of correlation on the variance of the mean}{37}{section*.65}% 
\contentsline {subsection}{Core modifications to bagging}{37}{section*.66}% 
\contentsline {subsection}{Random forest algorithm}{38}{section*.67}% 
\contentsline {subsection}{OOB again}{38}{section*.68}% 
\contentsline {subsection}{Variable importance plots}{38}{section*.69}% 
\contentsline {subsubsection}{Variable importance for a single tree}{39}{section*.70}% 
\contentsline {subsubsection}{From single to many trees}{39}{section*.71}% 
\contentsline {subsubsection}{Variable importance based on randomization}{40}{section*.72}% 
\contentsline {subsubsection}{Boston}{40}{section*.73}% 
\contentsline {subsubsection}{Pima indians}{41}{section*.74}% 
\contentsline {subsection}{Forward - boosting next}{44}{section*.75}% 
\contentsline {section}{Conclusions}{44}{section*.76}% 
\contentsline {section}{Exercises}{44}{section*.77}% 
\contentsline {subsection}{Small tasks}{44}{section*.78}% 
\contentsline {subsection}{Prove the formula}{44}{section*.79}% 
\contentsline {section}{References}{44}{section*.80}% 
