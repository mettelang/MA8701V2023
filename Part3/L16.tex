% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={MA8701 Advanced methods in statistical inference and learning},
  pdfauthor={Mette Langaas},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{MA8701 Advanced methods in statistical inference and learning}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Part 3: Ensembles. L16: Hyperparameter tuning}
\author{Mette Langaas}
\date{3/8/23}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[borderline west={3pt}{0pt}{shadecolor}, interior hidden, enhanced, breakable, boxrule=0pt, frame hidden, sharp corners]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}
Course homepage: \url{https://wiki.math.ntnu.no/ma8701/2023v/start}

\hypertarget{before-we-start}{%
\section{Before we start}\label{before-we-start}}

\hypertarget{literature}{%
\subsection{Literature}\label{literature}}

\begin{itemize}
\item
  Hyperparameter tuning with Bayesian Optimization. Frazier (2018): ``A
  tutorial on Bayesian optimization'', https://arxiv.org/abs/1807.02811:
  Sections 1,2,3,4.1, 5: only the section ``Noisy evaluations'', 6,7.
\item
  G. A. Lujan-Moreno, P. R. Howard, O. G. Rojas and D. C. Montgomery
  (2018): Design of experiments and response surface methodology to tune
  machine learning hyperparameters, with a random forest case- study.
  Expert Systems with Applications. 109, 195-205.
\end{itemize}

\hypertarget{choosing-hyperparameters}{%
\section{Choosing hyperparameters}\label{choosing-hyperparameters}}

\begin{itemize}
\tightlist
\item
  What are \emph{hyperparameters}?
\item
  Which hyperparameters have we encountered in the course so far?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\emph{Hyperparameters} are parameters than cannot be directly estimated
from data. This may be model parameters (the \(\lambda\) in lasso) or
parameters that influence the fitting of the model (e.g.~related to some
optimization algorithm).

We have already studied hyperparameter tuning for the lasso, ridge,
elastic net, random forest, and boosting - with the use of
cross-validation of some loss function for a predefined set (grid) of
hyperparameter values.

The use of ensembles like the stacked ensemble (Super Learner) may be
seen as an alternative to hyperparameter tuning.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Overview of advices from Berent Ã….S. Lunde on tuning parameters in
xbgoost (from 2021):

\textbf{Ways to speed-up computation:}

\begin{itemize}
\tightlist
\item
  Higher learning-rate, then tune the number of boosting iterations.
\item
  When tuning, do not use too high k in k-fold CV (for both speed and
  also to avoid high variance), or drop CV and use a validation set.
\item
  Speedups with histogram algorithms of order \(n\) (avoid exact
  enumeration of all splits, nlogn).
\item
  Use sparsity when possible!
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Comments on hyperparameters:}

\begin{itemize}
\tightlist
\item
  Learning rate (eta in xgb): Set as low as computation times allow.
  Typically in between \(0.01\) and \(0.1\) is sufficient.
\item
  Number of trees (boosting iterations): Very much affected by the
  learning rate. Most important parameter to tune, given a learning
  rate. Maximum depth of trees: start low, then increase. High values
  takes significantly longer to train. Think about the problem, and the
  number of possible interaction effects. If max-depth = J, then
  interactions among J-1 features is possible. How much is needed?
\item
  Gamma: Very traditional tree-hyperparameter to tune. Tune.
\item
  Colsampling for tree is usually sufficient.
\item
  subsample: in 0.5-0.9
\item
  Min child weight: Something I usually do not think about. Default=1
  (low) works. For me
\end{itemize}

Every problem is different, and there will exist exceptions to the above
guidelines. \emph{I look forward to a world without hyperparameters.}
(Berent Ã….S. Lunde)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The hyperparameters may be continuous (penalty parameter), discrete
(number of layers in a neural network, applying early stopping or not)
or categorical (choose between different optimizers).

The choice of hyperparameters is important, and will often directly
affect the model complexity, and unwise choices may lead to overfitting.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Hyperparametertuning is performed using a separate validation set or by
cross-validation. Different loss functions or selection criteria may be
used (MSE, ROC-AUC, misclassification rate, \ldots).

The hyperparameter tuning is often referred to as a black-box
optimization because we (usually) only calculate loss function values
(with CV) and do not get to compute gradients.

What may be challenges with hyperparameter optimization?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Some challenges with hyperparameter optimization (Feurer and Hutter,
Ch1):

\begin{itemize}
\tightlist
\item
  expensive evaluation of the model under study (large networks, large
  data sets)
\item
  unclear which of possibly many hyperparameters that need to be
  selected carefully (refer to the discussion for xgboost)
\item
  gradient of selection criterion with respect to the hyperparameters
  not (generally) available, and criterion not convex or smooth in the
  hyperparameters
\item
  and the need for external validation or CV
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

There exist many ways to \emph{group} methods for hyperparameter tuning.
One way to look at this is (Kuhn and Silge, 2021, Ch 12)

\begin{itemize}
\tightlist
\item
  grid search: specify a set of possible values a priori and investigate
  only these values, choose the value where the chosen selection
  criterion is optimal. This is also called ``model free methods''.
\item
  iterative search: start with a set of values, fit/evaluate some
  (surrogate) model (might also be the loss function), and based on this
  choose new values to evaluate next.
\end{itemize}

For grid search also methods for \emph{speeding up calculations} exists
- for example by stopping evaluation at a grid point where the loss is
seen to be high after some CV-folds, for example the method of
\emph{racing} described by Kuhn and Silge, Ch 13.4.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

(Class notes: see example from Kuhn and Silge, 2021, Spacefilling grid
vs global search)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{surrogate-methods}{%
\subsection{Surrogate methods}\label{surrogate-methods}}

We will look at two types of surrogate models: Bayesian regression with
Gaussian processes (in Bayesian optimization) and regression-type models
in response surface methods.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bayesian-optimization}{%
\subsection{Bayesian optimization}\label{bayesian-optimization}}

Bayesian optimization is an iterative method - where we start with
evaluating some loss function at some predefined set of points in the
hyperparameter space. New position in the hyperparameter space are
chosen iteratively.

Two key ingredients:

\begin{itemize}
\tightlist
\item
  a surrogate model (we will only look at Bayesian regression with
  Gaussian processes) to fit to the observed values of the loss function
  in the hyperparameter space
\item
  an \emph{acquisition} function to decide a new point in the
  hyperparameter space to evaluate next
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Underlying idea: given some ``observations'' in the hyperparameter
space, the task is to decide where to place a new point. We should try a
point where:

\begin{itemize}
\tightlist
\item
  we expect a good value and/or
\item
  we have little information so far
\end{itemize}

To do that we need information on both expected value \emph{and}
variance - or preferably the distribution of the loss function for our
problem.

We now look at the multivariate Gaussian distribution and conditional
distribution, a Gaussian process

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{gaussian-processes}{%
\subsubsection{Gaussian processes}\label{gaussian-processes}}

(Eidsvik 2017, page 6-7, note in TMA4265)

A Gaussian process is defined for

\begin{itemize}
\tightlist
\item
  times or locations \(x_i\), \(i=1,\ldots,n\) in \(\Re^d\), where
\item
  \(Y_i=Y(x_i)\) is a random variable at \(x_i\)
\item
  such that \({\boldsymbol Y}=(Y_1,\ldots,Y_n)\) is multivariate
  Gaussian.
\end{itemize}

The process is \emph{first order (mean) stationary} if
\(\text{E}(Y(x))=\mu\) for all \(x\), and this can be extended to depend
on covariates.

The process is \emph{second order stationary} if
\(\text{Var}(Y(x))=\sigma^2\) for all \(x\) and the correlation
\(\text{Corr}(Y(x),Y(x'))\) only depends on differences between \(x\)
and \(x'\).

The multivariate Gaussian distribution is defined by the mean and
covariance alone.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{correlation-functions}{%
\paragraph{Correlation functions}\label{correlation-functions}}

(Eidsvik 2017, page 7, Frazier 2018, Ch 3.1)

Correlation functions are also referred to as \emph{kernels}.

We assume that points at positions close to each other have a stronger
correlation than point far apart.

\textbf{Power exponential or Gaussian kernel}
\[ \text{Corr}(Y(x),Y(x'))=\exp(-\phi_G \Vert x-x' \Vert ^2)\] where the
L2 distance is used and \(\phi_G\) is a parameter that determine the
decay in the correlations.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Matern-type kernel}

\[\text{Corr}(Y(x),Y(xÂ´))=(1+\phi_M \Vert x - x' \Vert)\exp(-\phi_M \Vert x - x' \Vert)\]

now with decay-describing parameter \(\phi_M\).

The parameters of the kernels need to be estimated, see Ch 3.2 of
Frazier 2018 (who use a slightly different parameterization). We will
just assume that these parameters are known.

(Class notes: study Figure 4 and 5 of Eidsvik, 2018.)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{from-correlations-into-covariance-matrix}{%
\paragraph{From correlations into covariance
matrix}\label{from-correlations-into-covariance-matrix}}

For simplicity assume that \(d=1\). The number of positions to consider
is \(n\).

To get from correlation function to a \(n \times n\) covariance matrix
first construct a \(n \times n\) matrix of distances for each pair of
positions, denote this \({\boldsymbol H}\).

For the Matern-type correlation function the covariance matrix can then
be written

\[ \Sigma=\sigma^2 (1+\phi_M {\boldsymbol H}) \otimes \exp(-\phi_M {\boldsymbol H}))\]
where \(\otimes\) is elementwise multiplication.

See Eidsvik (2018, Ch 3.2 and 3.3) for how to build covariance matrices
in an efficient way.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{multivariate-normal-distribution}{%
\subsubsection{Multivariate normal
distribution}\label{multivariate-normal-distribution}}

aka multivariate Gaussian distribution. Known from TMA4265 and TMA4267.

The random vector \(\boldsymbol{Y}_{p\times 1}\) is multivariate normal
\(N_p\) with mean \(\boldsymbol{\mu}\) and (positive definite) covariate
matrix \(\Sigma\). The pdf is:

\[f(\boldsymbol{Y})=\frac{1}{(2\pi)^\frac{p}{2}|\Sigma|^\frac{1}{2}} \exp\{-\frac{1}{2}(\boldsymbol{Y}-\boldsymbol{\mu})^T\Sigma^{-1}(\boldsymbol{Y}-\boldsymbol{\mu})\}\]

Six useful properties of the mvN - we need number 6.

Let \(\boldsymbol{Y}_{(p\times 1)}\) be a random vector from
\(N_p(\boldsymbol{\mu},\Sigma)\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The grapical contours of the mvN are ellipsoids (can be shown using
  spectral decomposition).
\item
  Linear combinations of components of \(\boldsymbol{Y}\) are
  (multivariate) normal (can be easily proven using moment generating
  functions MGF).
\item
  All subsets of the components of \(\boldsymbol{Y}\) are (multivariate)
  normal (special case of the above).
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Zero covariance implies that the corresponding components are
  independently distributed (can be proven using MGF).
\item
  \(\boldsymbol{A}\Sigma\boldsymbol{B}^T=\boldsymbol{0} \Leftrightarrow \boldsymbol{A}\boldsymbol{Y}\)
  and \(\boldsymbol{B}\boldsymbol{Y}\) are independent.
\item
  The conditional distributions of the components are (multivariate)
  normal.
  \[\boldsymbol{Y}_2 \mid (\boldsymbol{Y}_1=\boldsymbol{Y}_1) \sim N_{p2}(\boldsymbol{\mu}_2+\Sigma_{21}\Sigma_{11}^{-1} (\boldsymbol{Y}_1-\boldsymbol{\mu}_1),\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}).\]
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{acquisition-function-expected-improvement}{%
\subsubsection{Acquisition function: Expected
improvement}\label{acquisition-function-expected-improvement}}

(Frazier 2018 page 7)

Thought experiment:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  we have evaluated our function at all possible points \(x\), and must
  return a solution based on what we already have evaluated. If the
  evaluation is noise-less we need to return the point with the largest
  observed value \(f\).
\item
  Correction: We may perform one more evaluation. If we choose \(x\) we
  observe \(f(x)\), and the best point before that was \(f^{*}_n\). The
  improvement at the new observation is then \[ \max(f(x)-f^{*}_n,0)\]
\end{enumerate}

(In class study Figure 1 of Frazier 2018)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  We define the \emph{expected improvement} as
\end{enumerate}

\[ \text{EI}_n(x)=\text{E}_n[\max(f(x)-f^{*}_n,0)]\]

where the expectation is taken at the posterior distribution given that
we have evaluated \(f\) at \(n\) observations \(x_1,\ldots, x_n\), and
the posterior distribution is that \(f\) conditional on
\(x_1,\ldots,x_n,y_1,\ldots,y_n\) is normal with mean \(\mu_n(x)\) and
variance \(\sigma^2_n(x)\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  How to evaluate the expected improvement? Integration by parts gives
\end{enumerate}

\[ \text{EI}_n(x)=\max(\mu_n(x)-f^{*}_n,0)])+\sigma_n(x) \phi(\frac{\mu_n(x)-f^{*}_n}{\sigma_n(x)}) \]
\[-\text{abs}(\mu_n(x)-f^{*}_n) \Phi(\frac{\mu_n(x)-f^{*}_n}{\sigma_n(x)})\]

\(\mu_n(x)-f^{*}_n\) is expected proposed vs previously best

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{4}
\tightlist
\item
  We choose to evaluate the point with the largest expected improvement
\end{enumerate}

\[ x_{n+1}=\text{argmax}\text{EI}_n(x)\]

Is often found using quasi-Newton optimization.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{algorithm-for-bayesian-optimization-of-a-function-f}{%
\subsubsection{\texorpdfstring{Algorithm for Bayesian optimization of a
function
\(f\)}{Algorithm for Bayesian optimization of a function f}}\label{algorithm-for-bayesian-optimization-of-a-function-f}}

(Frazier 2018, page 3, noise-free evaluation)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Place a Gaussian process prior on \(f\).

Observe \(f\) at \(n_0\) points from some experimental design. Set
\(n=n_0\).

\textbf{while} \(n \le N\) do

Update the posterior on f with all available data

Let \(x_n\) be a maximizer of the acquisition function over \(x\),
computed using the current posterior

Observe \(y_n=f(x_n)\)

Increment \(n\)

\textbf{end while}

Return a solution: a point with largest \(f(x)\) or the point with the
largest posterior mean

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

What does the steps mean?

\begin{itemize}
\tightlist
\item
  Gaussian prior: choose (estimate?) mean and correlation function for
  the problem.
\item
  Observe \(n_0\) points: calculate the loss function at each of the
  points (remark: we have noise)
\item
  Update the posterior: calculate the conditional distribution for \(f\)
  for a new point given the observed loss at all previously observed
  points
\item
  Acquisition function: find \(\text{argmax}\text{EI}_n(x)\).
\end{itemize}

(Class notes: Figure 1 of Frazier 2018.)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  For a point \(x\) we model the distribution of \(f(x)\),
\item
  which is normally distributed with mean \(\mu_n(x)\) and variance
  \(\sigma_n^2(x)\). The mean and variance is found from the conditional
  distribution.
\item
  With 95\% credibility interval \(\mu_n(x)\pm 1.95 \sigma_n(x)\).
\item
  The width of the credibility interval at observations is 0.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{extension}{%
\subsection{Extension}\label{extension}}

What is the objection function is not observed noise-less?

Independent normal error term \(\varepsilon\) can be added to the
previously defined \(Y=f(x)\) to make a new \(Y=f(x)+\varepsilon\). This
(only) adds a diagonal term to the covariance matrix, and it is common
to assume that the variance is the same for all \(x\) and treat the
variance as a hyperparameter.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example}{%
\subsubsection{Example}\label{example}}

(Kuhn and Silge, Ch 14, the example is for SVM)

First just grid search to test what is best value for \texttt{mtry}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(Boston, }\AttributeTok{package =} \StringTok{"MASS"}\NormalTok{)}
\CommentTok{\# first using a grid}
\NormalTok{tune\_grid }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}
  \AttributeTok{mtry =}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{:}\DecValTok{13}\NormalTok{))}
\CommentTok{\#  ntree=seq(100,500,length=10)) \# how to also include ntree? primary only mtry, how to define secondary?}
\NormalTok{tune\_control }\OtherTok{\textless{}{-}}\NormalTok{ caret}\SpecialCharTok{::}\FunctionTok{trainControl}\NormalTok{(}
  \AttributeTok{method =} \StringTok{"oob"}\NormalTok{, }\CommentTok{\# cross{-}validation \#eller cv}
  \CommentTok{\#number = 3, \# with n folds }
  \AttributeTok{verboseIter =} \ConstantTok{FALSE}\NormalTok{, }\CommentTok{\# no training log}
  \AttributeTok{allowParallel =} \ConstantTok{FALSE} \CommentTok{\# FALSE for reproducible results }
\NormalTok{)}
\NormalTok{rf\_tune }\OtherTok{\textless{}{-}}\NormalTok{ caret}\SpecialCharTok{::}\FunctionTok{train}\NormalTok{(}
\NormalTok{  medv}\SpecialCharTok{\textasciitilde{}}\NormalTok{crim}\SpecialCharTok{+}\NormalTok{zn}\SpecialCharTok{+}\NormalTok{indus}\SpecialCharTok{+}\NormalTok{chas}\SpecialCharTok{+}\NormalTok{nox}\SpecialCharTok{+}\NormalTok{rm}\SpecialCharTok{+}\NormalTok{age}\SpecialCharTok{+}\NormalTok{dis}\SpecialCharTok{+}\NormalTok{rad}\SpecialCharTok{+}\NormalTok{tax}\SpecialCharTok{+}\NormalTok{ptratio}\SpecialCharTok{+}\NormalTok{black}\SpecialCharTok{+}\NormalTok{lstat, }
  \AttributeTok{data=}\NormalTok{Boston,}
  \AttributeTok{na.action=}\NormalTok{na.roughfix,}
  \AttributeTok{trControl =}\NormalTok{ tune\_control,}
  \AttributeTok{tuneGrid =}\NormalTok{ tune\_grid,}
  \AttributeTok{method =} \StringTok{"rf"}\NormalTok{, }\CommentTok{\# rf is randomForest, checked at \#vhttp://topepo.github.io/caret/train{-}models{-}by{-}tag.html\#Random\_Forest}
  \AttributeTok{verbose =} \ConstantTok{TRUE}
\NormalTok{)}
\NormalTok{tuneplot }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, }\AttributeTok{probs =}\NormalTok{ .}\DecValTok{90}\NormalTok{) \{}
  \FunctionTok{ggplot}\NormalTok{(x) }\SpecialCharTok{+}
    \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\FunctionTok{quantile}\NormalTok{(x}\SpecialCharTok{$}\NormalTok{results}\SpecialCharTok{$}\NormalTok{RMSE, }\AttributeTok{probs =}\NormalTok{ probs), }\FunctionTok{min}\NormalTok{(x}\SpecialCharTok{$}\NormalTok{results}\SpecialCharTok{$}\NormalTok{RMSE))) }\SpecialCharTok{+}
    \FunctionTok{theme\_bw}\NormalTok{()}
\NormalTok{\}}
\FunctionTok{tuneplot}\NormalTok{(rf\_tune)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{L16_files/figure-pdf/unnamed-chunk-2-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf\_tune}\SpecialCharTok{$}\NormalTok{bestTune}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  mtry
7    7
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The R the function \texttt{tune\_bayes} is available in the package
\texttt{tune}, and requires that the analyses is done with a workflow.
Default in the GP is exponential correlation function, but first we try
the Matern.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree\_rec }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(medv}\SpecialCharTok{\textasciitilde{}}\NormalTok{crim}\SpecialCharTok{+}\NormalTok{zn}\SpecialCharTok{+}\NormalTok{indus}\SpecialCharTok{+}\NormalTok{chas}\SpecialCharTok{+}\NormalTok{nox}\SpecialCharTok{+}\NormalTok{rm}\SpecialCharTok{+}\NormalTok{age}\SpecialCharTok{+}\NormalTok{dis}\SpecialCharTok{+}\NormalTok{rad}\SpecialCharTok{+}\NormalTok{tax}\SpecialCharTok{+}\NormalTok{ptratio}\SpecialCharTok{+}\NormalTok{black}\SpecialCharTok{+}\NormalTok{lstat, }\AttributeTok{data =}\NormalTok{ Boston)}

\NormalTok{tune\_spec }\OtherTok{\textless{}{-}} \FunctionTok{rand\_forest}\NormalTok{( }\CommentTok{\# parsnip interface to random forests models}
  \AttributeTok{mode=}\StringTok{"regression"}\NormalTok{,}
  \AttributeTok{mtry =} \FunctionTok{tune}\NormalTok{(),}
  \AttributeTok{trees =} \FunctionTok{tune}\NormalTok{(),}
\CommentTok{\#  min\_n = tune()}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\CommentTok{\#  set\_mode("regression") \%\textgreater{}\%}
\CommentTok{\#  set\_engine("ranger",objective="reg:rmse") \# errors with ranger}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"randomForest"}\NormalTok{) }\CommentTok{\# randomforest ok}

\NormalTok{tune\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(tree\_rec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(tune\_spec)}

\NormalTok{tune\_param }\OtherTok{\textless{}{-}}\NormalTok{ tune\_spec}\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  parameters}\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{update}\NormalTok{(}\AttributeTok{mtry=}\FunctionTok{mtry}\NormalTok{(}\FunctionTok{c}\NormalTok{(1L,13L)),}\AttributeTok{trees=}\FunctionTok{trees}\NormalTok{(}\FunctionTok{c}\NormalTok{(100L,500L)))}

\NormalTok{vfold  }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(Boston, }\AttributeTok{v =} \DecValTok{5}\NormalTok{)}
\CommentTok{\# then trying BO}
\NormalTok{ctrl }\OtherTok{\textless{}{-}} \FunctionTok{control\_bayes}\NormalTok{(}\AttributeTok{verbose =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{bayesres}\OtherTok{\textless{}{-}} \FunctionTok{tune\_bayes}\NormalTok{(tune\_wf,}
    \AttributeTok{resamples =}\NormalTok{ vfold,}
    \CommentTok{\#metrics = rmse,}
    \AttributeTok{corr=}\FunctionTok{list}\NormalTok{(}\AttributeTok{type=}\StringTok{"matern"}\NormalTok{,}\AttributeTok{nu=}\DecValTok{5}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }
    \CommentTok{\#default in corr\_mat(GPfit) is "exponential" power 1.95}
    \AttributeTok{initial =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{param\_info =}\NormalTok{ tune\_param,}
    \AttributeTok{iter =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{objective=}\FunctionTok{exp\_improve}\NormalTok{(),}
    \AttributeTok{control =}\NormalTok{ ctrl}
\NormalTok{  )}
\FunctionTok{dput}\NormalTok{(bayesres,}\StringTok{"bayesres.dd"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 9
    mtry trees .metric .estimator  mean     n std_err .config              .iter
   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                <int>
 1     4   333 rmse    standard    3.26     5   0.439 Preprocessor1_Model~     0
 2     6   423 rmse    standard    3.26     5   0.416 Preprocessor1_Model~     0
 3     4   500 rmse    standard    3.28     5   0.442 Iter4                    4
 4     5   336 rmse    standard    3.29     5   0.446 Iter1                    1
 5     6   347 rmse    standard    3.29     5   0.411 Preprocessor1_Model~     0
 6     6   500 rmse    standard    3.29     5   0.413 Iter6                    6
 7     7   500 rmse    standard    3.30     5   0.411 Iter3                    3
 8     8   399 rmse    standard    3.32     5   0.393 Preprocessor1_Model~     0
 9     9   186 rmse    standard    3.33     5   0.367 Preprocessor1_Model~     0
10     9   477 rmse    standard    3.34     5   0.384 Preprocessor1_Model~     0
\end{verbatim}

\includegraphics{L16_files/figure-pdf/unnamed-chunk-4-1.pdf}

\includegraphics{L16_files/figure-pdf/unnamed-chunk-4-2.pdf}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Here we try the default exponential correlation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bayesres2}\OtherTok{\textless{}{-}} \FunctionTok{tune\_bayes}\NormalTok{(tune\_wf,}
    \AttributeTok{resamples =}\NormalTok{ vfold,}
    \CommentTok{\#metrics = rmse,}
    \CommentTok{\#corr=list(type="matern",nu=5/2), }
    \CommentTok{\#default in corr\_mat(GPfit) is "exponential" power 1.95}
    \AttributeTok{initial =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{param\_info =}\NormalTok{ tune\_param,}
    \AttributeTok{iter =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{objective=}\FunctionTok{exp\_improve}\NormalTok{(),}
    \AttributeTok{control =}\NormalTok{ ctrl}
\NormalTok{  )}
\FunctionTok{dput}\NormalTok{(bayesres2,}\StringTok{"bayesres2.dd"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bayesres2}\OtherTok{=}\FunctionTok{dget}\NormalTok{(}\StringTok{"bayesres2.dd"}\NormalTok{)}
\FunctionTok{show\_best}\NormalTok{(bayesres2,}\AttributeTok{n=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 9
    mtry trees .metric .estimator  mean     n std_err .config              .iter
   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                <int>
 1     5   499 rmse    standard    3.25     5   0.447 Iter6                    6
 2     4   313 rmse    standard    3.29     5   0.452 Iter5                    5
 3     7   445 rmse    standard    3.29     5   0.399 Preprocessor1_Model~     0
 4     5   453 rmse    standard    3.30     5   0.436 Iter8                    8
 5     6   498 rmse    standard    3.31     5   0.429 Iter2                    2
 6     4   500 rmse    standard    3.31     5   0.451 Iter1                    1
 7     7   500 rmse    standard    3.31     5   0.402 Iter9                    9
 8     5   258 rmse    standard    3.32     5   0.416 Preprocessor1_Model~     0
 9     8   496 rmse    standard    3.33     5   0.398 Iter4                    4
10     4   231 rmse    standard    3.33     5   0.433 Iter10                  10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{autoplot}\NormalTok{(bayesres2,}\AttributeTok{type=}\StringTok{"performance"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{L16_files/figure-pdf/unnamed-chunk-6-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{autoplot}\NormalTok{(bayesres2,}\AttributeTok{type=}\StringTok{"parameters"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{L16_files/figure-pdf/unnamed-chunk-6-2.pdf}

}

\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{suggested-software}{%
\subsubsection{Suggested software}\label{suggested-software}}

(Frazier 2018, Ch 6)

\begin{itemize}
\item
  R: DiceOptim (on CRAN)
\item
  R: tune\_bayes in \texttt{tune} (also CRAN)
\item
  Python: Spearmint \url{https://github.com/HIPS/Spearmint}
\item
  Python: GPyOpt \url{https://github.com/SheffieldML/GPyOpt}
\item
  Python: GPFlow (Tensorflow) \url{https://github.com/GPflow/GPflow} and
  GPYTorch (PyTorch) \url{https://github.com/cornellius-gp/gpytorch}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{design-of-experiments-and-response-surface-methodology}{%
\subsection{Design of experiments and response surface
methodology}\label{design-of-experiments-and-response-surface-methodology}}

G. A. Lujan-Moreno, P. R. Howard, O. G. Rojas and D. C. Montgomery
(2018): Design of experiments and response surface methodology to tune
machine learning hyperparameters, with a random forest case- study.
Expert Systems with Applications. 109, 195-205.

See separate slide-deck made by HÃ¥kon Gryvill, Yngvild Hamre and Javier
Aguilar for the article presentation in MA8701 in the spring of 2021.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{references}{%
\section{References}\label{references}}

\begin{itemize}
\item
  M. Feurer and F. Hutter (2019). In F. Hutter et al (eds.) Automated
  Machine Learning. The Springer Series on Challenges in Machine
  Learning.
\item
  Jo Eidsvik (2017): Introduction to Gaussian processes, note to
  TMA4265.
\item
  Peter I. Frazier (2018): A tutorial on Bayesian optimization. arxiv
  \url{https://arxiv.org/abs/1807.02811}
\item
  Max Kuhn and Julia Silge Version 0.0.1.9008 (2021-02-15) Tidy
  modelling with R. \url{https://www.tmwr.org/}
\item
  Roger Gosse, University of Toronto: CSC321 Lecture 21: Bayesian
  Hyperparameter Optimization.
  \url{http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec21.pdf}
\item
  Max Kuhn (2020). caret: Classification and Regression Training. R
  package version 6.0-86. \url{https://CRAN.R-project.org/package=caret}
\end{itemize}



\end{document}
