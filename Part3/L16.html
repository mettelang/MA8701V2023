<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mette Langaas">
<meta name="dcterms.date" content="2023-03-08">

<title>MA8701 Advanced methods in statistical inference and learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="L16_files/libs/clipboard/clipboard.min.js"></script>
<script src="L16_files/libs/quarto-html/quarto.js"></script>
<script src="L16_files/libs/quarto-html/popper.min.js"></script>
<script src="L16_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="L16_files/libs/quarto-html/anchor.min.js"></script>
<link href="L16_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="L16_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="L16_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="L16_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="L16_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#before-we-start" id="toc-before-we-start" class="nav-link active" data-scroll-target="#before-we-start">Before we start</a>
  <ul class="collapse">
  <li><a href="#literature" id="toc-literature" class="nav-link" data-scroll-target="#literature">Literature</a></li>
  </ul></li>
  <li><a href="#choosing-hyperparameters" id="toc-choosing-hyperparameters" class="nav-link" data-scroll-target="#choosing-hyperparameters">Choosing hyperparameters</a>
  <ul class="collapse">
  <li><a href="#surrogate-methods" id="toc-surrogate-methods" class="nav-link" data-scroll-target="#surrogate-methods">Surrogate methods</a></li>
  <li><a href="#bayesian-optimization" id="toc-bayesian-optimization" class="nav-link" data-scroll-target="#bayesian-optimization">Bayesian optimization</a>
  <ul class="collapse">
  <li><a href="#gaussian-processes" id="toc-gaussian-processes" class="nav-link" data-scroll-target="#gaussian-processes">Gaussian processes</a></li>
  <li><a href="#multivariate-normal-distribution" id="toc-multivariate-normal-distribution" class="nav-link" data-scroll-target="#multivariate-normal-distribution">Multivariate normal distribution</a></li>
  <li><a href="#acquisition-function-expected-improvement" id="toc-acquisition-function-expected-improvement" class="nav-link" data-scroll-target="#acquisition-function-expected-improvement">Acquisition function: Expected improvement</a></li>
  <li><a href="#algorithm-for-bayesian-optimization-of-a-function-f" id="toc-algorithm-for-bayesian-optimization-of-a-function-f" class="nav-link" data-scroll-target="#algorithm-for-bayesian-optimization-of-a-function-f">Algorithm for Bayesian optimization of a function <span class="math inline">\(f\)</span></a></li>
  </ul></li>
  <li><a href="#extension" id="toc-extension" class="nav-link" data-scroll-target="#extension">Extension</a>
  <ul class="collapse">
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example</a></li>
  <li><a href="#suggested-software" id="toc-suggested-software" class="nav-link" data-scroll-target="#suggested-software">Suggested software</a></li>
  </ul></li>
  <li><a href="#design-of-experiments-and-response-surface-methodology" id="toc-design-of-experiments-and-response-surface-methodology" class="nav-link" data-scroll-target="#design-of-experiments-and-response-surface-methodology">Design of experiments and response surface methodology</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">MA8701 Advanced methods in statistical inference and learning</h1>
<p class="subtitle lead">Part 3: Ensembles. L16: Hyperparameter tuning</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mette Langaas </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 8, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<div class="cell">

</div>
<p>Course homepage: <a href="https://wiki.math.ntnu.no/ma8701/2023v/start" class="uri">https://wiki.math.ntnu.no/ma8701/2023v/start</a></p>
<section id="before-we-start" class="level1">
<h1>Before we start</h1>
<section id="literature" class="level2">
<h2 class="anchored" data-anchor-id="literature">Literature</h2>
<ul>
<li><p>Hyperparameter tuning with Bayesian Optimization. Frazier (2018): “A tutorial on Bayesian optimization”, https://arxiv.org/abs/1807.02811: Sections 1,2,3,4.1, 5: only the section “Noisy evaluations”, 6,7.</p></li>
<li><p>G. A. Lujan-Moreno, P. R. Howard, O. G. Rojas and D. C. Montgomery (2018): Design of experiments and response surface methodology to tune machine learning hyperparameters, with a random forest case- study. Expert Systems with Applications. 109, 195-205.</p></li>
</ul>
</section>
</section>
<section id="choosing-hyperparameters" class="level1">
<h1>Choosing hyperparameters</h1>
<ul>
<li>What are <em>hyperparameters</em>?</li>
<li>Which hyperparameters have we encountered in the course so far?</li>
</ul>
<hr>
<p><em>Hyperparameters</em> are parameters than cannot be directly estimated from data. This may be model parameters (the <span class="math inline">\(\lambda\)</span> in lasso) or parameters that influence the fitting of the model (e.g.&nbsp;related to some optimization algorithm).</p>
<p>We have already studied hyperparameter tuning for the lasso, ridge, elastic net, random forest, and boosting - with the use of cross-validation of some loss function for a predefined set (grid) of hyperparameter values.</p>
<p>The use of ensembles like the stacked ensemble (Super Learner) may be seen as an alternative to hyperparameter tuning.</p>
<hr>
<p>Overview of advices from Berent Å.S. Lunde on tuning parameters in xbgoost (from 2021):</p>
<p><strong>Ways to speed-up computation:</strong></p>
<ul>
<li>Higher learning-rate, then tune the number of boosting iterations.</li>
<li>When tuning, do not use too high k in k-fold CV (for both speed and also to avoid high variance), or drop CV and use a validation set.</li>
<li>Speedups with histogram algorithms of order <span class="math inline">\(n\)</span> (avoid exact enumeration of all splits, nlogn).</li>
<li>Use sparsity when possible!</li>
</ul>
<hr>
<p><strong>Comments on hyperparameters:</strong></p>
<ul>
<li>Learning rate (eta in xgb): Set as low as computation times allow. Typically in between <span class="math inline">\(0.01\)</span> and <span class="math inline">\(0.1\)</span> is sufficient.</li>
<li>Number of trees (boosting iterations): Very much affected by the learning rate. Most important parameter to tune, given a learning rate. Maximum depth of trees: start low, then increase. High values takes significantly longer to train. Think about the problem, and the number of possible interaction effects. If max-depth = J, then interactions among J-1 features is possible. How much is needed?</li>
<li>Gamma: Very traditional tree-hyperparameter to tune. Tune.</li>
<li>Colsampling for tree is usually sufficient.</li>
<li>subsample: in 0.5-0.9</li>
<li>Min child weight: Something I usually do not think about. Default=1 (low) works. For me</li>
</ul>
<p>Every problem is different, and there will exist exceptions to the above guidelines. <em>I look forward to a world without hyperparameters.</em> (Berent Å.S. Lunde)</p>
<hr>
<p>The hyperparameters may be continuous (penalty parameter), discrete (number of layers in a neural network, applying early stopping or not) or categorical (choose between different optimizers).</p>
<p>The choice of hyperparameters is important, and will often directly affect the model complexity, and unwise choices may lead to overfitting.</p>
<hr>
<p>Hyperparametertuning is performed using a separate validation set or by cross-validation. Different loss functions or selection criteria may be used (MSE, ROC-AUC, misclassification rate, …).</p>
<p>The hyperparameter tuning is often referred to as a black-box optimization because we (usually) only calculate loss function values (with CV) and do not get to compute gradients.</p>
<p>What may be challenges with hyperparameter optimization?</p>
<hr>
<p>Some challenges with hyperparameter optimization (Feurer and Hutter, Ch1):</p>
<ul>
<li>expensive evaluation of the model under study (large networks, large data sets)</li>
<li>unclear which of possibly many hyperparameters that need to be selected carefully (refer to the discussion for xgboost)</li>
<li>gradient of selection criterion with respect to the hyperparameters not (generally) available, and criterion not convex or smooth in the hyperparameters</li>
<li>and the need for external validation or CV</li>
</ul>
<hr>
<p>There exist many ways to <em>group</em> methods for hyperparameter tuning. One way to look at this is (Kuhn and Silge, 2021, Ch 12)</p>
<ul>
<li>grid search: specify a set of possible values a priori and investigate only these values, choose the value where the chosen selection criterion is optimal. This is also called “model free methods”.</li>
<li>iterative search: start with a set of values, fit/evaluate some (surrogate) model (might also be the loss function), and based on this choose new values to evaluate next.</li>
</ul>
<p>For grid search also methods for <em>speeding up calculations</em> exists - for example by stopping evaluation at a grid point where the loss is seen to be high after some CV-folds, for example the method of <em>racing</em> described by Kuhn and Silge, Ch 13.4.</p>
<hr>
<p>(Class notes: see example from Kuhn and Silge, 2021, Spacefilling grid vs global search)</p>
<hr>
<section id="surrogate-methods" class="level2">
<h2 class="anchored" data-anchor-id="surrogate-methods">Surrogate methods</h2>
<p>We will look at two types of surrogate models: Bayesian regression with Gaussian processes (in Bayesian optimization) and regression-type models in response surface methods.</p>
<hr>
</section>
<section id="bayesian-optimization" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-optimization">Bayesian optimization</h2>
<p>Bayesian optimization is an iterative method - where we start with evaluating some loss function at some predefined set of points in the hyperparameter space. New position in the hyperparameter space are chosen iteratively.</p>
<p>Two key ingredients:</p>
<ul>
<li>a surrogate model (we will only look at Bayesian regression with Gaussian processes) to fit to the observed values of the loss function in the hyperparameter space</li>
<li>an <em>acquisition</em> function to decide a new point in the hyperparameter space to evaluate next</li>
</ul>
<hr>
<p>Underlying idea: given some “observations” in the hyperparameter space, the task is to decide where to place a new point. We should try a point where:</p>
<ul>
<li>we expect a good value and/or</li>
<li>we have little information so far</li>
</ul>
<p>To do that we need information on both expected value <em>and</em> variance - or preferably the distribution of the loss function for our problem.</p>
<p>We now look at the multivariate Gaussian distribution and conditional distribution, a Gaussian process</p>
<hr>
<section id="gaussian-processes" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-processes">Gaussian processes</h3>
<p>(Eidsvik 2017, page 6-7, note in TMA4265)</p>
<p>A Gaussian process is defined for</p>
<ul>
<li>times or locations <span class="math inline">\(x_i\)</span>, <span class="math inline">\(i=1,\ldots,n\)</span> in <span class="math inline">\(\Re^d\)</span>, where</li>
<li><span class="math inline">\(Y_i=Y(x_i)\)</span> is a random variable at <span class="math inline">\(x_i\)</span></li>
<li>such that <span class="math inline">\({\boldsymbol Y}=(Y_1,\ldots,Y_n)\)</span> is multivariate Gaussian.</li>
</ul>
<p>The process is <em>first order (mean) stationary</em> if <span class="math inline">\(\text{E}(Y(x))=\mu\)</span> for all <span class="math inline">\(x\)</span>, and this can be extended to depend on covariates.</p>
<p>The process is <em>second order stationary</em> if <span class="math inline">\(\text{Var}(Y(x))=\sigma^2\)</span> for all <span class="math inline">\(x\)</span> and the correlation <span class="math inline">\(\text{Corr}(Y(x),Y(x'))\)</span> only depends on differences between <span class="math inline">\(x\)</span> and <span class="math inline">\(x'\)</span>.</p>
<p>The multivariate Gaussian distribution is defined by the mean and covariance alone.</p>
<hr>
<section id="correlation-functions" class="level4">
<h4 class="anchored" data-anchor-id="correlation-functions">Correlation functions</h4>
<p>(Eidsvik 2017, page 7, Frazier 2018, Ch 3.1)</p>
<p>Correlation functions are also referred to as <em>kernels</em>.</p>
<p>We assume that points at positions close to each other have a stronger correlation than point far apart.</p>
<p><strong>Power exponential or Gaussian kernel</strong> <!-- $$\text{Corr}(Y(x),Y(x'))=\alpha_0 \exp(-\sum_{j=1}^d \alpha_j (x_j-x_j')^2) --> <!-- $$ --> <span class="math display">\[ \text{Corr}(Y(x),Y(x'))=\exp(-\phi_G \Vert x-x' \Vert ^2)\]</span> where the L2 distance is used and <span class="math inline">\(\phi_G\)</span> is a parameter that determine the decay in the correlations.</p>
<!-- (Class notes: study Figure 2 of Frazier 2018 for plots for Gaussian kernel.) -->
<hr>
<p><strong>Matern-type kernel</strong></p>
<!-- $$\text{Corr}(Y(x),Y(x´))=\alpha_0 \frac{2^{1-\nu}}{\Gamma(\nu)} (\sqrt{2\nu}\sum_{j=1}^d \alpha_j (x_j-x_j'))^{\nu} K_{\nu}(\sum_{j=1}^d \alpha_j (x_j-x_j'))  -->
<!-- $$with the modified Bessel function $K_{\nu}$ and additional  -->
<p><span class="math display">\[\text{Corr}(Y(x),Y(x´))=(1+\phi_M \Vert x - x' \Vert)\exp(-\phi_M \Vert x - x' \Vert)\]</span></p>
<p>now with decay-describing parameter <span class="math inline">\(\phi_M\)</span>.</p>
<p>The parameters of the kernels need to be estimated, see Ch 3.2 of Frazier 2018 (who use a slightly different parameterization). We will just assume that these parameters are known.</p>
<p>(Class notes: study Figure 4 and 5 of Eidsvik, 2018.)</p>
<hr>
</section>
<section id="from-correlations-into-covariance-matrix" class="level4">
<h4 class="anchored" data-anchor-id="from-correlations-into-covariance-matrix">From correlations into covariance matrix</h4>
<p>For simplicity assume that <span class="math inline">\(d=1\)</span>. The number of positions to consider is <span class="math inline">\(n\)</span>.</p>
<p>To get from correlation function to a <span class="math inline">\(n \times n\)</span> covariance matrix first construct a <span class="math inline">\(n \times n\)</span> matrix of distances for each pair of positions, denote this <span class="math inline">\({\boldsymbol H}\)</span>.</p>
<p>For the Matern-type correlation function the covariance matrix can then be written</p>
<p><span class="math display">\[ \Sigma=\sigma^2 (1+\phi_M {\boldsymbol H}) \otimes \exp(-\phi_M {\boldsymbol H}))\]</span> where <span class="math inline">\(\otimes\)</span> is elementwise multiplication.</p>
<p>See Eidsvik (2018, Ch 3.2 and 3.3) for how to build covariance matrices in an efficient way.</p>
<hr>
</section>
</section>
<section id="multivariate-normal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="multivariate-normal-distribution">Multivariate normal distribution</h3>
<p>aka multivariate Gaussian distribution. Known from TMA4265 and TMA4267.</p>
<p>The random vector <span class="math inline">\(\boldsymbol{Y}_{p\times 1}\)</span> is multivariate normal <span class="math inline">\(N_p\)</span> with mean <span class="math inline">\(\boldsymbol{\mu}\)</span> and (positive definite) covariate matrix <span class="math inline">\(\Sigma\)</span>. The pdf is:</p>
<p><span class="math display">\[f(\boldsymbol{Y})=\frac{1}{(2\pi)^\frac{p}{2}|\Sigma|^\frac{1}{2}} \exp\{-\frac{1}{2}(\boldsymbol{Y}-\boldsymbol{\mu})^T\Sigma^{-1}(\boldsymbol{Y}-\boldsymbol{\mu})\}\]</span></p>
<p>Six useful properties of the mvN - we need number 6.</p>
<p>Let <span class="math inline">\(\boldsymbol{Y}_{(p\times 1)}\)</span> be a random vector from <span class="math inline">\(N_p(\boldsymbol{\mu},\Sigma)\)</span>.</p>
<ol type="1">
<li>The grapical contours of the mvN are ellipsoids (can be shown using spectral decomposition).</li>
<li>Linear combinations of components of <span class="math inline">\(\boldsymbol{Y}\)</span> are (multivariate) normal (can be easily proven using moment generating functions MGF).</li>
<li>All subsets of the components of <span class="math inline">\(\boldsymbol{Y}\)</span> are (multivariate) normal (special case of the above).</li>
</ol>
<hr>
<ol start="4" type="1">
<li>Zero covariance implies that the corresponding components are independently distributed (can be proven using MGF).</li>
<li><span class="math inline">\(\boldsymbol{A}\Sigma\boldsymbol{B}^T=\boldsymbol{0} \Leftrightarrow \boldsymbol{A}\boldsymbol{Y}\)</span> and <span class="math inline">\(\boldsymbol{B}\boldsymbol{Y}\)</span> are independent.</li>
<li>The conditional distributions of the components are (multivariate) normal. <span class="math display">\[\boldsymbol{Y}_2 \mid (\boldsymbol{Y}_1=\boldsymbol{Y}_1) \sim N_{p2}(\boldsymbol{\mu}_2+\Sigma_{21}\Sigma_{11}^{-1} (\boldsymbol{Y}_1-\boldsymbol{\mu}_1),\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}).\]</span></li>
</ol>
<hr>
</section>
<section id="acquisition-function-expected-improvement" class="level3">
<h3 class="anchored" data-anchor-id="acquisition-function-expected-improvement">Acquisition function: Expected improvement</h3>
<p>(Frazier 2018 page 7)</p>
<p>Thought experiment:</p>
<ol type="1">
<li><p>we have evaluated our function at all possible points <span class="math inline">\(x\)</span>, and must return a solution based on what we already have evaluated. If the evaluation is noise-less we need to return the point with the largest observed value <span class="math inline">\(f\)</span>.</p></li>
<li><p>Correction: We may perform one more evaluation. If we choose <span class="math inline">\(x\)</span> we observe <span class="math inline">\(f(x)\)</span>, and the best point before that was <span class="math inline">\(f^{*}_n\)</span>. The improvement at the new observation is then <span class="math display">\[ \max(f(x)-f^{*}_n,0)\]</span></p></li>
</ol>
<p>(In class study Figure 1 of Frazier 2018)</p>
<hr>
<ol start="3" type="1">
<li>We define the <em>expected improvement</em> as</li>
</ol>
<p><span class="math display">\[ \text{EI}_n(x)=\text{E}_n[\max(f(x)-f^{*}_n,0)]\]</span></p>
<p>where the expectation is taken at the posterior distribution given that we have evaluated <span class="math inline">\(f\)</span> at <span class="math inline">\(n\)</span> observations <span class="math inline">\(x_1,\ldots, x_n\)</span>, and the posterior distribution is that <span class="math inline">\(f\)</span> conditional on <span class="math inline">\(x_1,\ldots,x_n,y_1,\ldots,y_n\)</span> is normal with mean <span class="math inline">\(\mu_n(x)\)</span> and variance <span class="math inline">\(\sigma^2_n(x)\)</span>.</p>
<hr>
<ol start="4" type="1">
<li>How to evaluate the expected improvement? Integration by parts gives</li>
</ol>
<p><span class="math display">\[ \text{EI}_n(x)=\max(\mu_n(x)-f^{*}_n,0)])+\sigma_n(x) \phi(\frac{\mu_n(x)-f^{*}_n}{\sigma_n(x)}) \]</span> <span class="math display">\[-\text{abs}(\mu_n(x)-f^{*}_n) \Phi(\frac{\mu_n(x)-f^{*}_n}{\sigma_n(x)})\]</span></p>
<p><span class="math inline">\(\mu_n(x)-f^{*}_n\)</span> is expected proposed vs previously best</p>
<ol start="5" type="1">
<li>We choose to evaluate the point with the largest expected improvement</li>
</ol>
<p><span class="math display">\[ x_{n+1}=\text{argmax}\text{EI}_n(x)\]</span></p>
<p>Is often found using quasi-Newton optimization.</p>
<hr>
</section>
<section id="algorithm-for-bayesian-optimization-of-a-function-f" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-for-bayesian-optimization-of-a-function-f">Algorithm for Bayesian optimization of a function <span class="math inline">\(f\)</span></h3>
<p>(Frazier 2018, page 3, noise-free evaluation)</p>
<hr>
<p>Place a Gaussian process prior on <span class="math inline">\(f\)</span>.</p>
<p>Observe <span class="math inline">\(f\)</span> at <span class="math inline">\(n_0\)</span> points from some experimental design. Set <span class="math inline">\(n=n_0\)</span>.</p>
<p><strong>while</strong> <span class="math inline">\(n \le N\)</span> do</p>
<p>Update the posterior on f with all available data</p>
<p>Let <span class="math inline">\(x_n\)</span> be a maximizer of the acquisition function over <span class="math inline">\(x\)</span>, computed using the current posterior</p>
<p>Observe <span class="math inline">\(y_n=f(x_n)\)</span></p>
<p>Increment <span class="math inline">\(n\)</span></p>
<p><strong>end while</strong></p>
<p>Return a solution: a point with largest <span class="math inline">\(f(x)\)</span> or the point with the largest posterior mean</p>
<hr>
<p>What does the steps mean?</p>
<ul>
<li>Gaussian prior: choose (estimate?) mean and correlation function for the problem.</li>
<li>Observe <span class="math inline">\(n_0\)</span> points: calculate the loss function at each of the points (remark: we have noise)</li>
<li>Update the posterior: calculate the conditional distribution for <span class="math inline">\(f\)</span> for a new point given the observed loss at all previously observed points</li>
<li>Acquisition function: find <span class="math inline">\(\text{argmax}\text{EI}_n(x)\)</span>.</li>
</ul>
<p>(Class notes: Figure 1 of Frazier 2018.)</p>
<hr>
<ul>
<li>For a point <span class="math inline">\(x\)</span> we model the distribution of <span class="math inline">\(f(x)\)</span>,</li>
<li>which is normally distributed with mean <span class="math inline">\(\mu_n(x)\)</span> and variance <span class="math inline">\(\sigma_n^2(x)\)</span>. The mean and variance is found from the conditional distribution.</li>
<li>With 95% credibility interval <span class="math inline">\(\mu_n(x)\pm 1.95 \sigma_n(x)\)</span>.</li>
<li>The width of the credibility interval at observations is 0.</li>
</ul>
<hr>
</section>
</section>
<section id="extension" class="level2">
<h2 class="anchored" data-anchor-id="extension">Extension</h2>
<p>What is the objection function is not observed noise-less?</p>
<p>Independent normal error term <span class="math inline">\(\varepsilon\)</span> can be added to the previously defined <span class="math inline">\(Y=f(x)\)</span> to make a new <span class="math inline">\(Y=f(x)+\varepsilon\)</span>. This (only) adds a diagonal term to the covariance matrix, and it is common to assume that the variance is the same for all <span class="math inline">\(x\)</span> and treat the variance as a hyperparameter.</p>
<hr>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">Example</h3>
<p>(Kuhn and Silge, Ch 14, the example is for SVM)</p>
<p>First just grid search to test what is best value for <code>mtry</code></p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(Boston, <span class="at">package =</span> <span class="st">"MASS"</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># first using a grid</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>tune_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">mtry =</span> (<span class="dv">1</span><span class="sc">:</span><span class="dv">13</span>))</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">#  ntree=seq(100,500,length=10)) # how to also include ntree? primary only mtry, how to define secondary?</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>tune_control <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">trainControl</span>(</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">"oob"</span>, <span class="co"># cross-validation #eller cv</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">#number = 3, # with n folds </span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">verboseIter =</span> <span class="cn">FALSE</span>, <span class="co"># no training log</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">allowParallel =</span> <span class="cn">FALSE</span> <span class="co"># FALSE for reproducible results </span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>rf_tune <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">train</span>(</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  medv<span class="sc">~</span>crim<span class="sc">+</span>zn<span class="sc">+</span>indus<span class="sc">+</span>chas<span class="sc">+</span>nox<span class="sc">+</span>rm<span class="sc">+</span>age<span class="sc">+</span>dis<span class="sc">+</span>rad<span class="sc">+</span>tax<span class="sc">+</span>ptratio<span class="sc">+</span>black<span class="sc">+</span>lstat, </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">data=</span>Boston,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">na.action=</span>na.roughfix,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">trControl =</span> tune_control,</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuneGrid =</span> tune_grid,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">"rf"</span>, <span class="co"># rf is randomForest, checked at #vhttp://topepo.github.io/caret/train-models-by-tag.html#Random_Forest</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">verbose =</span> <span class="cn">TRUE</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>tuneplot <span class="ot">&lt;-</span> <span class="cf">function</span>(x, <span class="at">probs =</span> .<span class="dv">90</span>) {</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(x) <span class="sc">+</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">coord_cartesian</span>(<span class="at">ylim =</span> <span class="fu">c</span>(<span class="fu">quantile</span>(x<span class="sc">$</span>results<span class="sc">$</span>RMSE, <span class="at">probs =</span> probs), <span class="fu">min</span>(x<span class="sc">$</span>results<span class="sc">$</span>RMSE))) <span class="sc">+</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="fu">tuneplot</span>(rf_tune)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="L16_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>rf_tune<span class="sc">$</span>bestTune</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>  mtry
6    6</code></pre>
</div>
</div>
<hr>
<p>The R the function <code>tune_bayes</code> is available in the package <code>tune</code>, and requires that the analyses is done with a workflow. Default in the GP is exponential correlation function, but first we try the Matern.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>tree_rec <span class="ot">&lt;-</span> <span class="fu">recipe</span>(medv<span class="sc">~</span>crim<span class="sc">+</span>zn<span class="sc">+</span>indus<span class="sc">+</span>chas<span class="sc">+</span>nox<span class="sc">+</span>rm<span class="sc">+</span>age<span class="sc">+</span>dis<span class="sc">+</span>rad<span class="sc">+</span>tax<span class="sc">+</span>ptratio<span class="sc">+</span>black<span class="sc">+</span>lstat, <span class="at">data =</span> Boston)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>tune_spec <span class="ot">&lt;-</span> <span class="fu">rand_forest</span>( <span class="co"># parsnip interface to random forests models</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">mode=</span><span class="st">"regression"</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">mtry =</span> <span class="fu">tune</span>(),</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">trees =</span> <span class="fu">tune</span>(),</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">#  min_n = tune()</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">#  set_mode("regression") %&gt;%</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">#  set_engine("ranger",objective="reg:rmse") # errors with ranger</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"randomForest"</span>) <span class="co"># randomforest ok</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>tune_wf <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_recipe</span>(tree_rec) <span class="sc">%&gt;%</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(tune_spec)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>tune_param <span class="ot">&lt;-</span> tune_spec<span class="sc">%&gt;%</span> </span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>  parameters<span class="sc">%&gt;%</span> </span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">update</span>(<span class="at">mtry=</span><span class="fu">mtry</span>(<span class="fu">c</span>(1L,13L)),<span class="at">trees=</span><span class="fu">trees</span>(<span class="fu">c</span>(100L,500L)))</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>vfold  <span class="ot">&lt;-</span> <span class="fu">vfold_cv</span>(Boston, <span class="at">v =</span> <span class="dv">5</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co"># then trying BO</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">control_bayes</span>(<span class="at">verbose =</span> <span class="cn">TRUE</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>bayesres<span class="ot">&lt;-</span> <span class="fu">tune_bayes</span>(tune_wf,</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">resamples =</span> vfold,</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="co">#metrics = rmse,</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">corr=</span><span class="fu">list</span>(<span class="at">type=</span><span class="st">"matern"</span>,<span class="at">nu=</span><span class="dv">5</span><span class="sc">/</span><span class="dv">2</span>), </span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">#default in corr_mat(GPfit) is "exponential" power 1.95</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">initial =</span> <span class="dv">10</span>,</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    <span class="at">param_info =</span> tune_param,</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    <span class="at">iter =</span> <span class="dv">10</span>,</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    <span class="at">objective=</span><span class="fu">exp_improve</span>(),</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    <span class="at">control =</span> ctrl</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="fu">dput</span>(bayesres,<span class="st">"bayesres.dd"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 10 × 9
    mtry trees .metric .estimator  mean     n std_err .config              .iter
   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                &lt;int&gt;
 1     4   333 rmse    standard    3.26     5   0.439 Preprocessor1_Model…     0
 2     6   423 rmse    standard    3.26     5   0.416 Preprocessor1_Model…     0
 3     4   500 rmse    standard    3.28     5   0.442 Iter4                    4
 4     5   336 rmse    standard    3.29     5   0.446 Iter1                    1
 5     6   347 rmse    standard    3.29     5   0.411 Preprocessor1_Model…     0
 6     6   500 rmse    standard    3.29     5   0.413 Iter6                    6
 7     7   500 rmse    standard    3.30     5   0.411 Iter3                    3
 8     8   399 rmse    standard    3.32     5   0.393 Preprocessor1_Model…     0
 9     9   186 rmse    standard    3.33     5   0.367 Preprocessor1_Model…     0
10     9   477 rmse    standard    3.34     5   0.384 Preprocessor1_Model…     0</code></pre>
</div>
<div class="cell-output-display">
<p><img src="L16_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
<div class="cell-output-display">
<p><img src="L16_files/figure-html/unnamed-chunk-4-2.png" class="img-fluid" width="672"></p>
</div>
</div>
<hr>
<p>Here we try the default exponential correlation.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>bayesres2<span class="ot">&lt;-</span> <span class="fu">tune_bayes</span>(tune_wf,</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">resamples =</span> vfold,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">#metrics = rmse,</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">#corr=list(type="matern",nu=5/2), </span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">#default in corr_mat(GPfit) is "exponential" power 1.95</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">initial =</span> <span class="dv">10</span>,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">param_info =</span> tune_param,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">iter =</span> <span class="dv">10</span>,</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">objective=</span><span class="fu">exp_improve</span>(),</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">control =</span> ctrl</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="fu">dput</span>(bayesres2,<span class="st">"bayesres2.dd"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>bayesres2<span class="ot">=</span><span class="fu">dget</span>(<span class="st">"bayesres2.dd"</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">show_best</span>(bayesres2,<span class="at">n=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 10 × 9
    mtry trees .metric .estimator  mean     n std_err .config              .iter
   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                &lt;int&gt;
 1     5   499 rmse    standard    3.25     5   0.447 Iter6                    6
 2     4   313 rmse    standard    3.29     5   0.452 Iter5                    5
 3     7   445 rmse    standard    3.29     5   0.399 Preprocessor1_Model…     0
 4     5   453 rmse    standard    3.30     5   0.436 Iter8                    8
 5     6   498 rmse    standard    3.31     5   0.429 Iter2                    2
 6     4   500 rmse    standard    3.31     5   0.451 Iter1                    1
 7     7   500 rmse    standard    3.31     5   0.402 Iter9                    9
 8     5   258 rmse    standard    3.32     5   0.416 Preprocessor1_Model…     0
 9     8   496 rmse    standard    3.33     5   0.398 Iter4                    4
10     4   231 rmse    standard    3.33     5   0.433 Iter10                  10</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(bayesres2,<span class="at">type=</span><span class="st">"performance"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="L16_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(bayesres2,<span class="at">type=</span><span class="st">"parameters"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="L16_files/figure-html/unnamed-chunk-6-2.png" class="img-fluid" width="672"></p>
</div>
</div>
<hr>
</section>
<section id="suggested-software" class="level3">
<h3 class="anchored" data-anchor-id="suggested-software">Suggested software</h3>
<p>(Frazier 2018, Ch 6)</p>
<ul>
<li><p>R: DiceOptim (on CRAN)</p></li>
<li><p>R: tune_bayes in <code>tune</code> (also CRAN)</p></li>
<li><p>Python: Spearmint <a href="https://github.com/HIPS/Spearmint" class="uri">https://github.com/HIPS/Spearmint</a></p></li>
<li><p>Python: GPyOpt <a href="https://github.com/SheffieldML/GPyOpt" class="uri">https://github.com/SheffieldML/GPyOpt</a></p></li>
<li><p>Python: GPFlow (Tensorflow) <a href="https://github.com/GPflow/GPflow" class="uri">https://github.com/GPflow/GPflow</a> and GPYTorch (PyTorch) <a href="https://github.com/cornellius-gp/gpytorch" class="uri">https://github.com/cornellius-gp/gpytorch</a></p></li>
</ul>
<hr>
</section>
</section>
<section id="design-of-experiments-and-response-surface-methodology" class="level2">
<h2 class="anchored" data-anchor-id="design-of-experiments-and-response-surface-methodology">Design of experiments and response surface methodology</h2>
<p>G. A. Lujan-Moreno, P. R. Howard, O. G. Rojas and D. C. Montgomery (2018): Design of experiments and response surface methodology to tune machine learning hyperparameters, with a random forest case- study. Expert Systems with Applications. 109, 195-205.</p>
<p>See separate slide-deck made by Håkon Gryvill, Yngvild Hamre and Javier Aguilar for the article presentation in MA8701 in the spring of 2021.</p>
<hr>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ul>
<li><p>M. Feurer and F. Hutter (2019). In F. Hutter et al (eds.) Automated Machine Learning. The Springer Series on Challenges in Machine Learning.</p></li>
<li><p>Jo Eidsvik (2017): Introduction to Gaussian processes, note to TMA4265.</p></li>
<li><p>Peter I. Frazier (2018): A tutorial on Bayesian optimization. arxiv <a href="https://arxiv.org/abs/1807.02811" class="uri">https://arxiv.org/abs/1807.02811</a></p></li>
<li><p>Max Kuhn and Julia Silge Version 0.0.1.9008 (2021-02-15) Tidy modelling with R. <a href="https://www.tmwr.org/" class="uri">https://www.tmwr.org/</a></p></li>
<li><p>Roger Gosse, University of Toronto: CSC321 Lecture 21: Bayesian Hyperparameter Optimization. <a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec21.pdf" class="uri">http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec21.pdf</a></p></li>
<li><p>Max Kuhn (2020). caret: Classification and Regression Training. R package version 6.0-86. <a href="https://CRAN.R-project.org/package=caret" class="uri">https://CRAN.R-project.org/package=caret</a></p></li>
</ul>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>