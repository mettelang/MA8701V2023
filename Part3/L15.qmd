---
title: "MA8701 Advanced methods in statistical inference and learning"
author: "Mette Langaas"
date: "`r format(Sys.time(), '%d %B, %Y')`"
subtitle: 'Part 3: Ensembles. L15: Stacked ensembles'
bibliography: ../Part1/references.bib
nocite: |
  @casi, @ESL, @ISL
format: 
  html: 
    toc: true
    code-fold: true
    toc-location: left
    toc-depth: 3
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
    keep-tex: true
  beamer:
    incremental: false
    aspectratio: 43
    navigation: frame
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| eval: true
#| echo: false
#| warning: false
suppressPackageStartupMessages(library(knitr))
knitr::opts_chunk$set(echo = FALSE, message=FALSE,warning = FALSE, error = FALSE)
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(rpart))
suppressPackageStartupMessages(library(rpart.plot))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(DAAG))
suppressPackageStartupMessages(library(tree))
suppressPackageStartupMessages(library(MASS))
suppressPackageStartupMessages(library(GGally))
suppressPackageStartupMessages(library(corrplot))
suppressPackageStartupMessages(library(caret)) #for confusion matrices
suppressPackageStartupMessages(library(pROC)) #for ROC curves
suppressPackageStartupMessages(library(SuperLearner))
suppressPackageStartupMessages(library(tidymodels))
#suppressPackageStartupMessages(library(SuperLearner))
```

Course homepage: <https://wiki.math.ntnu.no/ma8701/2023v/start>

# Before we start

## Literature

-   Erin Le Dell (2015): [Scalable Ensemble Learning and Computationally
    Efficient Variance Estimation. PhD Thesis, University of California,
    Berkeley.](https://escholarship.org/uc/item/3kb142r2) or <https://github.com/ledell/phd-thesis>

## Supporting literature

-   Mark J. van der Laan, Eric C. Polley, Alan E. Hubbard (2007): Super
    Learner, Statistical Applications in Genetics and Molecular Biology,
    6, 1, 25

-   Eric C. Polley, Sherri Rose, Mark J. van der Laan (2011): Super
    Learning. Chapter 3 of M. J. van der Laan and S. Rose. Targeted
    Learning, Springer.

------------------------------------------------------------------------

# Ensembles - overview

(ELS Ch 16.1)

With ensembles we want to build *one prediction model* which combines
the strength of *a collection of models*.

These models may be simple base models - or more elaborate models.

We have studied bagging - where we take a simple average of the
prediction from many models (or majority vote), and the base models can
be trees - or other type of models.

Random forest is a version of bagging, with trees made to be different
(decorrelated).

------------------------------------------------------------------------

We have studied boosting, where the models are trained on sequentially
different data - from residuals or gradients of loss functions - and the
ensemble members cast weighted votes. We have in particular looked into
the xgboost variant of boosting, and also evaluated possible parameters
to tune to optimize performance.

In L7 we also look into an ensemble built of elaborate models with the
Super Learner and study some possible methods for tuning
hyperparameters.

# Super Learner

Why do we want to study the Super Learner?

The Super Learner or *generalized stacking*  or "stacked ensemble" is an algorithm that
combines

-   multiple, (typically) diverse prediction methods (learning
    algorithms) called *base learners* into a
-   a *metalearner* - which can be seen as a *single* method.

------------------------------------------------------------------------

## Development:

-   1992: stacking introduce for neural nets by Wolpert
-   1996: adapted to regression problems by Breiman - but only for one
    type of methods at once (cart with different number of terminal
    nodes, glms with subset selection, ridge regression with different
    ridge penalty parameters)
-   2006: proven to have asymptotic theoretical oracle property by van
    der Laan, Polley and Hubbard.

------------------------------------------------------------------------

## Ingredients:

-   *Training data* (level-zero data) $O_i=(X_i,Y_i)$ of $N$ i.i.d
    observations.
    
-   A total of $L$ *base learning algorithms* $\Psi^l$ for
    $l=1,\ldots,L$, each from some algorithmic class and each with a
    specific set of model parameters.
    
-   A *metalearner* ${\bf \Phi}$ is used to find an *optimal
    combination* of the $L$ base learners.

------------------------------------------------------------------------

## Algorithm

**Step 1: Produce level-one data** ${\bf Z}$

a)  Divide the training data ${\bf X}$ randomly into $V$ roughly-equally
    sized validation folds ${\bf X}_{(1)},\ldots,{\bf X}_{(V)}$. $V$ is
    often 5 or 10. (The responses ${\bf Y}$ are also needed.)

b)  For each base learner $\Psi^l$ perform $V$-fold cross-validation to
    produce prediction.

This gives the level-one data set ${\bf Z}$ consisting prediction of all
the level-zero data - that is a matrix with $N$ rows and $L$ columns.

**What could the base learners be?**

------------------------------------------------------------------------

"Any" method that produces a prediction - "all" types of problems.

-   linear regression
-   lasso
-   cart
-   random forest with mtry=value 1
-   random forest with mtry=value 2
-   xgboost with hyperparameter set 1
-   xgboost with hyperparameter set 2
-   neural net with hyperparameter set 1

------------------------------------------------------------------------

**Step 2: Fit the metalearner**

a)  The starting point is the level-one prediction data ${\bf Z}$
    together with the responses $(Y_1,\ldots ,Y_N)$.
b)  The metalearner is used to estimate the weights given to each base
    learner: $\hat{\eta_i}=\alpha_1 z_{1i}+ \cdots + \alpha_L z_{Li}$.

**What could the metalearner be?**

------------------------------------------------------------------------

* the mean (bagging)
* constructed by minimizing the 
  + squared loss (ordinary least squares) or
  +  non-negative least squares
* ridge or lasso regression
* constructed by minimizing 1-ROC-AUC

------------------------------------------------------------------------

(Class notes: Study Figure 3.2 from Polley et al)

------------------------------------------------------------------------

**Step 3: Re-estimate base learners and combine into superlearner on
full training data**

a)  Fit each of the $L$ base learners to the full training set.
b)  The *ensemble fit* consists the $L$ base learner fits together with
    the metalearner fit.

**Step 4: Using the ensemble for prediction**

For a new observaton ${\bf x}^*$

a)  Use each of the $L$ base learners to produce a prediction
    ${\bf z}^*$, and
b)  feed this to the metalearner-fit to produce the final prediction
    $y^*$.

------------------------------------------------------------------------

## The metalearning

Some observations

-   The term *discrete super learner* is used if the base learner with
    the lowest risk (i.e. CV-error) is selected.
-   Since the predictions from multiple base learners may be highly
    correlated - the chosen method should perform well in that case
    (i.e. ridge and lasso).
-   When minimizing the squared loss it has been found that adding a
    non-negativity constraint $\alpha_l\le 0$ works well,
-   and also the additivity constraint $\sum_{l=1}^L \alpha_l=1$ - the
    ensemble is a *convex combination* of the base learners.
-   Non-linear optimization methods may be employed for the metalearner
    if no existing algorithm is available
-   Historically a regularized linear model has "mostly" been used
-   For classification the logistic response function can be used on the
    linear combination of base learners (Figure 3.2 Polley).

------------------------------------------------------------------------

## Examples

### Simulations examples

(Class notes: Study Figure 3.3 and Table 3.2 from Polley et al)

### Real data

(Class notes: Study Figure 3.4 and Table 3.3 from Polley et al. RE=MSE
relative to the linear model OLS.)

------------------------------------------------------------------------

## Theoretical result
(Le Dell 2015, page 6)

-   Oracle selector: the estimator among all possible weighted
    combinations of the base prediction function that minimizes the risk
    under the *true data generating distribution*.
    
    
-   The *oracle result* was established for the Super Learner by van der
    Laan et al (2006).
    
    
-   If the *true prediction function* cannot be represented by a
    combination of the base learners (available), then "optimal" will be
    the closest linear combination that would be optimal if the true
    data-generating function was known.
    
    
-   The oracle result require an *uniformly bounded loss function*.
    Using the convex restriction (sum alphas =1) implies that if each
    based learner is bounded so is the convex combination. In practice:
    truncation of the predicted values to the range of the outcome in
    the training set is sufficient to allow for unbounded loss functions


------------------------------------------------------------------------

## Uncertainty in the ensemble

(Class notes: Study "Road map" 2 from Polley et al)

-   Add an outer (external) cross validation loop (where the super
    learner loop is inside). Suggestion: use 20-fold, especially when
    small sample size.
    
-   Overfitting? Check if the super learner does as well or better than
    any of the base learners in the ensemble.
    
-   Results using *influence functions* for estimation of the variance
    for the Super Learner are based on asymptotic variances in the use
    of $V$-fold cross-validation (see Ch 5.3 of Le Dell, 2015)

------------------------------------------------------------------------

## Other issues

-   Many different implementations available, and much work on parallell
    processing and speed and memory efficient execution.
    
-   Super Learner implicitly can handle hyperparameter tuning by
    including the same base learner with different model parameter sets
    in the ensemble.
    
-   Speed and memory improvements for large data sets involves
    subsampling, and the R `subsemble` package is one solution, the H2O
    Ensemble project another.

------------------------------------------------------------------------

# R example from Superlearner package

Code is copied from [Guide to SuperLearner](https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html) and the presentation
follows this guide. The data used is the Boston housing dataset from
`MASS`, but with the median value of a house dichotomized into a
classification problem.

Observe that only 150 of the 560 observations is used (to speed up
things, but of cause that gives less accurate results).

```{r,echo=TRUE}
data(Boston, package = "MASS")
#colSums(is.na(Boston)) # no missing values
outcome = Boston$medv
# Create a dataframe to contain our explanatory variables.
data = subset(Boston, select = -medv)
#Set a seed for reproducibility in this random sampling.
set.seed(1)
# Reduce to a dataset of 150 observations to speed up model fitting.
train_obs = sample(nrow(data), 150)
# X is our training sample.
x_train = data[train_obs, ]
# Create a holdout set for evaluating model performance.
# Note: cross-validation is even better than a single holdout sample.
x_holdout = data[-train_obs, ]
# Create a binary outcome variable: towns in which median home value is > 22,000.
outcome_bin = as.numeric(outcome > 22)
y_train = outcome_bin[train_obs]
y_holdout = outcome_bin[-train_obs]
table(y_train, useNA = "ifany")
```

Then checking out the possible functions and how they differ from their
"original versions".

```{r,echo=TRUE}
listWrappers()
# how does SL.glm differ from glm? obsWeight added to easy use the traning fold in the CV and returns a prediction for new observarions
SL.glm
# min and not 1sd used, again obsWeights, make sure model matrix correctly specified
SL.glmnet
```

The fitting lasso to check what is being done. The default metalearner
is "method.NNLS" (both for regression and two-class classification -
probably then for linear predictor NNLS?).

```{r,echo=TRUE}
set.seed(1)
sl_lasso=SuperLearner(Y=y_train, X=x_train,family=binomial(),SL.library="SL.glmnet")
sl_lasso
#str(sl_lasso)
sl_lasso$cvRisk
```

Now use lasso and randomforest, and also add the average of ys just as
the benchmark.

```{r,echo=TRUE}
set.seed(1)
sl=SuperLearner(Y=y_train, X=x_train,family=binomial(),SL.library=c("SL.mean","SL.glmnet","SL.randomForest"))
sl
sl$times$everything
```

Our ensemble give weight 0.13 to lasso and 0.86 to the random forest.
(The guide used a different implementation of the random forest called
ranger, and got 0.02 and 0.98.)

Predict on the part of the dataset not used for the training.

```{r,echo=TRUE}
pred=predict(sl,x_holdout=x_holdout,onlySL=TRUE)
str(pred)
summary(pred$pred)
summary(pred$library.predict)
```

Add now an external cross-validation loop - only using the training
data. Here the default $V=10$ is used for the inner loop, and we set the
value for the outer loop (here $V=3$ for speed).

```{r,echo=TRUE}
system.time({cv_sl=CV.SuperLearner(Y=y_train, X=x_train,V=10,family=binomial(),SL.library=c("SL.mean","SL.glmnet","SL.randomForest"))})
summary(cv_sl)
```

See the guide for more information on running multiple versions of one
base learner, and parallellisation.

# R example from H2O-package

# References

::: {#refs}
:::

